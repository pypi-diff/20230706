# Comparing `tmp/autonomi_nos-0.0.5-py3-none-any.whl.zip` & `tmp/autonomi_nos-0.0.6a1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,61 +1,66 @@
-Zip file size: 58718 bytes, number of entries: 59
--rw-rw-r--  2.0 unx       50 b- defN 23-Jun-03 22:19 nos/__init__.py
--rw-rw-r--  2.0 unx      561 b- defN 23-Jun-03 22:19 nos/constants.py
--rw-rw-r--  2.0 unx       93 b- defN 23-May-01 21:40 nos/exceptions.py
--rw-rw-r--  2.0 unx      940 b- defN 23-May-01 21:40 nos/logging.py
--rw-rw-r--  2.0 unx     2778 b- defN 23-Jun-03 22:19 nos/protoc.py
--rw-rw-r--  2.0 unx       22 b- defN 23-Jun-13 01:28 nos/version.py
--rw-rw-r--  2.0 unx      110 b- defN 23-Apr-23 00:49 nos/cli/benchmark.py
--rw-rw-r--  2.0 unx      446 b- defN 23-Jun-03 22:19 nos/cli/cli.py
--rw-rw-r--  2.0 unx     1983 b- defN 23-Jun-03 22:19 nos/cli/docker.py
--rw-rw-r--  2.0 unx     1294 b- defN 23-Apr-23 00:49 nos/cli/hub.py
--rw-rw-r--  2.0 unx     7543 b- defN 23-Jun-12 21:09 nos/cli/predict.py
--rw-rw-r--  2.0 unx     5962 b- defN 23-Jun-03 22:19 nos/cli/serve_http.py
--rw-rw-r--  2.0 unx     3469 b- defN 23-Jun-03 22:19 nos/cli/system.py
--rw-rw-r--  2.0 unx      425 b- defN 23-May-05 01:05 nos/cli/utils.py
--rw-rw-r--  2.0 unx      396 b- defN 23-Jun-03 22:19 nos/client/__init__.py
--rw-rw-r--  2.0 unx       92 b- defN 23-May-25 21:01 nos/client/exceptions.py
--rw-rw-r--  2.0 unx    12309 b- defN 23-Jun-03 22:19 nos/client/grpc.py
--rw-rw-r--  2.0 unx      275 b- defN 23-Jun-03 22:19 nos/common/__init__.py
--rw-rw-r--  2.0 unx      204 b- defN 23-Jun-03 22:19 nos/common/cloudpickle.py
--rw-rw-r--  2.0 unx     9360 b- defN 23-Jun-03 22:19 nos/common/spec.py
--rw-rw-r--  2.0 unx      518 b- defN 23-Jun-12 21:09 nos/common/tasks.py
--rw-rw-r--  2.0 unx     5725 b- defN 23-Jun-03 22:19 nos/common/types.py
--rw-rw-r--  2.0 unx       50 b- defN 23-Jun-12 21:09 nos/common/io/__init__.py
--rw-rw-r--  2.0 unx     1048 b- defN 23-Jun-12 21:09 nos/common/io/video/base.py
--rw-rw-r--  2.0 unx     4272 b- defN 23-Jun-12 21:09 nos/common/io/video/opencv.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jun-03 22:19 nos/executors/__init__.py
--rw-rw-r--  2.0 unx     7401 b- defN 23-Jun-03 22:19 nos/executors/ray.py
--rw-rw-r--  2.0 unx     3508 b- defN 23-Jun-12 21:09 nos/hub/__init__.py
--rw-rw-r--  2.0 unx     2195 b- defN 23-Jun-12 21:09 nos/hub/config.py
--rw-rw-r--  2.0 unx      316 b- defN 23-Jun-12 21:09 nos/models/__init__.py
--rw-rw-r--  2.0 unx     3605 b- defN 23-Jun-03 22:19 nos/models/clip.py
--rw-rw-r--  2.0 unx     3285 b- defN 23-Jun-13 00:09 nos/models/faster_rcnn.py
--rw-rw-r--  2.0 unx     1507 b- defN 23-Jun-13 01:05 nos/models/sam.py
--rw-rw-r--  2.0 unx     3150 b- defN 23-Jun-03 22:19 nos/models/stable_diffusion.py
--rw-rw-r--  2.0 unx     6956 b- defN 23-Jun-13 00:09 nos/models/yolox.py
--rw-rw-r--  2.0 unx       63 b- defN 23-Jun-03 22:19 nos/models/openmmlab/__init__.py
--rw-rw-r--  2.0 unx       66 b- defN 23-May-13 02:32 nos/models/openmmlab/mmdetection/detection_tensorrt_dynamic-320x320-1344x1344.py
--rw-rw-r--  2.0 unx     3066 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/mmdetection.py
--rw-rw-r--  2.0 unx      370 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/configs/_base_/default_runtime.py
--rw-rw-r--  2.0 unx     3187 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_detection.py
--rw-rw-r--  2.0 unx     1765 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_instance.py
--rw-rw-r--  2.0 unx     3828 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/configs/_base_/models/faster-rcnn_r50_fpn.py
--rw-rw-r--  2.0 unx      304 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/configs/_base_/schedules/schedule_1x.py
--rw-rw-r--  2.0 unx     5340 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/configs/efficientdet/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py
--rw-rw-r--  2.0 unx      177 b- defN 23-Jun-03 22:19 nos/models/openmmlab/mmdetection/configs/faster-rcnn/faster-rcnn_r50_fpn_1x_coco.py
--rw-rw-r--  2.0 unx     2437 b- defN 23-Jun-03 22:19 nos/proto/nos_service.proto
--rw-rw-r--  2.0 unx        0 b- defN 23-Jun-03 22:19 nos/server/__init__.py
--rw-rw-r--  2.0 unx     6327 b- defN 23-Jun-03 22:19 nos/server/docker.py
--rw-rw-r--  2.0 unx     5502 b- defN 23-Jun-12 21:09 nos/server/runtime.py
--rw-rw-r--  2.0 unx    13616 b- defN 23-Jun-12 21:09 nos/server/service.py
--rw-rw-r--  2.0 unx      373 b- defN 23-Apr-23 00:49 nos/test/benchmark.py
--rw-rw-r--  2.0 unx     4244 b- defN 23-Jun-12 21:09 nos/test/conftest.py
--rw-rw-r--  2.0 unx     1888 b- defN 23-Jun-03 22:19 nos/test/utils.py
--rw-rw-r--  2.0 unx     1068 b- defN 23-Jun-13 02:05 autonomi_nos-0.0.5.dist-info/LICENSE
--rw-rw-r--  2.0 unx     6299 b- defN 23-Jun-13 02:05 autonomi_nos-0.0.5.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jun-13 02:05 autonomi_nos-0.0.5.dist-info/WHEEL
--rw-rw-r--  2.0 unx       86 b- defN 23-Jun-13 02:05 autonomi_nos-0.0.5.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        4 b- defN 23-Jun-13 02:05 autonomi_nos-0.0.5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     5063 b- defN 23-Jun-13 02:05 autonomi_nos-0.0.5.dist-info/RECORD
-59 files, 157013 bytes uncompressed, 50626 bytes compressed:  67.8%
+Zip file size: 76103 bytes, number of entries: 64
+-rw-rw-r--  2.0 unx      164 b- defN 23-Jul-06 04:43 nos/__init__.py
+-rw-rw-r--  2.0 unx      561 b- defN 23-Jul-06 04:43 nos/constants.py
+-rw-rw-r--  2.0 unx       93 b- defN 23-Jul-06 04:43 nos/exceptions.py
+-rw-rw-r--  2.0 unx      940 b- defN 23-Jul-06 04:43 nos/logging.py
+-rw-rw-r--  2.0 unx     2778 b- defN 23-Jul-06 04:43 nos/protoc.py
+-rw-rw-r--  2.0 unx       24 b- defN 23-Jul-06 04:43 nos/version.py
+-rw-rw-r--  2.0 unx      369 b- defN 23-Jul-06 04:43 nos/cli/cli.py
+-rw-rw-r--  2.0 unx     4230 b- defN 23-Jul-06 04:43 nos/cli/docker.py
+-rw-rw-r--  2.0 unx     1294 b- defN 23-Jul-06 04:43 nos/cli/hub.py
+-rw-rw-r--  2.0 unx     7543 b- defN 23-Jul-06 04:43 nos/cli/predict.py
+-rw-rw-r--  2.0 unx     5962 b- defN 23-Jul-06 04:43 nos/cli/serve_http.py
+-rw-rw-r--  2.0 unx     1713 b- defN 23-Jul-06 04:43 nos/cli/system.py
+-rw-rw-r--  2.0 unx      425 b- defN 23-Jul-06 04:43 nos/cli/utils.py
+-rw-rw-r--  2.0 unx      396 b- defN 23-Jul-06 04:43 nos/client/__init__.py
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-06 04:43 nos/client/exceptions.py
+-rw-rw-r--  2.0 unx    12378 b- defN 23-Jul-06 04:43 nos/client/grpc.py
+-rw-rw-r--  2.0 unx     1285 b- defN 23-Jul-06 04:43 nos/common/__init__.py
+-rw-rw-r--  2.0 unx      204 b- defN 23-Jul-06 04:43 nos/common/cloudpickle.py
+-rw-rw-r--  2.0 unx     2712 b- defN 23-Jul-06 04:43 nos/common/profiler.py
+-rw-rw-r--  2.0 unx     9446 b- defN 23-Jul-06 04:43 nos/common/spec.py
+-rw-rw-r--  2.0 unx     6076 b- defN 23-Jul-06 04:43 nos/common/system.py
+-rw-rw-r--  2.0 unx      518 b- defN 23-Jul-06 04:43 nos/common/tasks.py
+-rw-rw-r--  2.0 unx     5984 b- defN 23-Jul-06 04:43 nos/common/types.py
+-rw-rw-r--  2.0 unx      942 b- defN 23-Jul-06 04:43 nos/common/io/__init__.py
+-rw-rw-r--  2.0 unx     1050 b- defN 23-Jul-06 04:43 nos/common/io/video/base.py
+-rw-rw-r--  2.0 unx     8424 b- defN 23-Jul-06 04:43 nos/common/io/video/opencv.py
+-rw-rw-r--  2.0 unx     5832 b- defN 23-Jul-06 04:43 nos/compilers/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-06 04:43 nos/compilers/trt/__init__.py
+-rw-rw-r--  2.0 unx     2739 b- defN 23-Jul-06 04:43 nos/compilers/trt/ops/group_norm.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-06 04:43 nos/executors/__init__.py
+-rw-rw-r--  2.0 unx     7544 b- defN 23-Jul-06 04:43 nos/executors/ray.py
+-rw-rw-r--  2.0 unx     3531 b- defN 23-Jul-06 04:43 nos/hub/__init__.py
+-rw-rw-r--  2.0 unx     2195 b- defN 23-Jul-06 04:43 nos/hub/config.py
+-rw-rw-r--  2.0 unx       59 b- defN 23-Jul-06 04:43 nos/managers/__init__.py
+-rw-rw-r--  2.0 unx     7911 b- defN 23-Jul-06 04:43 nos/managers/model.py
+-rw-rw-r--  2.0 unx      301 b- defN 23-Jul-06 04:43 nos/models/__init__.py
+-rw-rw-r--  2.0 unx     9931 b- defN 23-Jul-06 04:43 nos/models/clip.py
+-rw-rw-r--  2.0 unx     3285 b- defN 23-Jul-06 04:43 nos/models/faster_rcnn.py
+-rw-rw-r--  2.0 unx     1507 b- defN 23-Jul-06 04:43 nos/models/sam.py
+-rw-rw-r--  2.0 unx    16095 b- defN 23-Jul-06 04:43 nos/models/stable_diffusion.py
+-rw-rw-r--  2.0 unx    10128 b- defN 23-Jul-06 04:43 nos/models/yolox.py
+-rw-rw-r--  2.0 unx       63 b- defN 23-Jul-06 04:43 nos/models/openmmlab/__init__.py
+-rw-rw-r--  2.0 unx     3066 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/mmdetection.py
+-rw-rw-r--  2.0 unx      370 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/default_runtime.py
+-rw-rw-r--  2.0 unx     3187 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_detection.py
+-rw-rw-r--  2.0 unx     1765 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_instance.py
+-rw-rw-r--  2.0 unx     3828 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/models/faster-rcnn_r50_fpn.py
+-rw-rw-r--  2.0 unx      304 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/_base_/schedules/schedule_1x.py
+-rw-rw-r--  2.0 unx     5340 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/efficientdet/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py
+-rw-rw-r--  2.0 unx      177 b- defN 23-Jul-06 04:43 nos/models/openmmlab/mmdetection/configs/faster-rcnn/faster-rcnn_r50_fpn_1x_coco.py
+-rw-rw-r--  2.0 unx     2437 b- defN 23-Jul-06 04:43 nos/proto/nos_service.proto
+-rw-rw-r--  2.0 unx     6193 b- defN 23-Jul-06 04:43 nos/server/__init__.py
+-rw-rw-r--  2.0 unx     6556 b- defN 23-Jul-06 04:43 nos/server/docker.py
+-rw-rw-r--  2.0 unx     6363 b- defN 23-Jul-06 04:43 nos/server/runtime.py
+-rw-rw-r--  2.0 unx     7326 b- defN 23-Jul-06 04:43 nos/server/service.py
+-rw-rw-r--  2.0 unx      373 b- defN 23-Jul-06 04:43 nos/test/benchmark.py
+-rw-rw-r--  2.0 unx     4252 b- defN 23-Jul-06 04:43 nos/test/conftest.py
+-rw-rw-r--  2.0 unx     2626 b- defN 23-Jul-06 04:43 nos/test/utils.py
+-rw-rw-r--  2.0 unx     1068 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     6804 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       86 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        4 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5441 b- defN 23-Jul-06 05:58 autonomi_nos-0.0.6a1.dist-info/RECORD
+64 files, 214382 bytes uncompressed, 67455 bytes compressed:  68.5%
```

## zipnote {}

```diff
@@ -12,17 +12,14 @@
 
 Filename: nos/protoc.py
 Comment: 
 
 Filename: nos/version.py
 Comment: 
 
-Filename: nos/cli/benchmark.py
-Comment: 
-
 Filename: nos/cli/cli.py
 Comment: 
 
 Filename: nos/cli/docker.py
 Comment: 
 
 Filename: nos/cli/hub.py
@@ -51,17 +48,23 @@
 
 Filename: nos/common/__init__.py
 Comment: 
 
 Filename: nos/common/cloudpickle.py
 Comment: 
 
+Filename: nos/common/profiler.py
+Comment: 
+
 Filename: nos/common/spec.py
 Comment: 
 
+Filename: nos/common/system.py
+Comment: 
+
 Filename: nos/common/tasks.py
 Comment: 
 
 Filename: nos/common/types.py
 Comment: 
 
 Filename: nos/common/io/__init__.py
@@ -69,26 +72,41 @@
 
 Filename: nos/common/io/video/base.py
 Comment: 
 
 Filename: nos/common/io/video/opencv.py
 Comment: 
 
+Filename: nos/compilers/__init__.py
+Comment: 
+
+Filename: nos/compilers/trt/__init__.py
+Comment: 
+
+Filename: nos/compilers/trt/ops/group_norm.py
+Comment: 
+
 Filename: nos/executors/__init__.py
 Comment: 
 
 Filename: nos/executors/ray.py
 Comment: 
 
 Filename: nos/hub/__init__.py
 Comment: 
 
 Filename: nos/hub/config.py
 Comment: 
 
+Filename: nos/managers/__init__.py
+Comment: 
+
+Filename: nos/managers/model.py
+Comment: 
+
 Filename: nos/models/__init__.py
 Comment: 
 
 Filename: nos/models/clip.py
 Comment: 
 
 Filename: nos/models/faster_rcnn.py
@@ -102,17 +120,14 @@
 
 Filename: nos/models/yolox.py
 Comment: 
 
 Filename: nos/models/openmmlab/__init__.py
 Comment: 
 
-Filename: nos/models/openmmlab/mmdetection/detection_tensorrt_dynamic-320x320-1344x1344.py
-Comment: 
-
 Filename: nos/models/openmmlab/mmdetection/mmdetection.py
 Comment: 
 
 Filename: nos/models/openmmlab/mmdetection/configs/_base_/default_runtime.py
 Comment: 
 
 Filename: nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_detection.py
@@ -153,26 +168,26 @@
 
 Filename: nos/test/conftest.py
 Comment: 
 
 Filename: nos/test/utils.py
 Comment: 
 
-Filename: autonomi_nos-0.0.5.dist-info/LICENSE
+Filename: autonomi_nos-0.0.6a1.dist-info/LICENSE
 Comment: 
 
-Filename: autonomi_nos-0.0.5.dist-info/METADATA
+Filename: autonomi_nos-0.0.6a1.dist-info/METADATA
 Comment: 
 
-Filename: autonomi_nos-0.0.5.dist-info/WHEEL
+Filename: autonomi_nos-0.0.6a1.dist-info/WHEEL
 Comment: 
 
-Filename: autonomi_nos-0.0.5.dist-info/entry_points.txt
+Filename: autonomi_nos-0.0.6a1.dist-info/entry_points.txt
 Comment: 
 
-Filename: autonomi_nos-0.0.5.dist-info/top_level.txt
+Filename: autonomi_nos-0.0.6a1.dist-info/top_level.txt
 Comment: 
 
-Filename: autonomi_nos-0.0.5.dist-info/RECORD
+Filename: autonomi_nos-0.0.6a1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nos/__init__.py

```diff
@@ -1 +1,5 @@
+import docker
 from nos.version import __version__  # noqa: F401
+
+from .client import InferenceClient  # noqa: F401
+from .server import init, shutdown  # noqa: F401
```

## nos/version.py

```diff
@@ -1 +1 @@
-__version__ = "0.0.5"
+__version__ = "0.0.6a1"
```

## nos/cli/cli.py

```diff
@@ -1,19 +1,17 @@
 import typer
 
-from nos.cli.benchmark import benchmark_cli
 from nos.cli.docker import docker_cli
 from nos.cli.hub import hub_cli
 from nos.cli.predict import predict_cli
 from nos.cli.system import system_cli
 
 
 app_cli = typer.Typer(no_args_is_help=True)
 app_cli.add_typer(hub_cli)
 app_cli.add_typer(system_cli)
-app_cli.add_typer(benchmark_cli)
 app_cli.add_typer(docker_cli)
 app_cli.add_typer(predict_cli)
 
 
 if __name__ == "__main__":
     app_cli()
```

## nos/cli/docker.py

```diff
@@ -1,60 +1,118 @@
 import rich.console
 import rich.status
 import typer
 
+import docker
 from nos.server.runtime import InferenceServiceRuntime
 
 
 docker_cli = typer.Typer(name="docker", help="NOS Docker CLI.", no_args_is_help=True)
 console = rich.console.Console()
 
 
-@docker_cli.command("start", help="Start NOS inference engine.")
+def get_running_inference_service_runtime_container(id: str = None) -> docker.models.containers.Container:
+    """Get running container."""
+    filters = {"status": "running"}
+    if id is not None:
+        filters["id"] = id
+    containers = InferenceServiceRuntime.list(filters=filters)
+    if not len(containers):
+        console.print("[bold red] ✗ No inference server running.[/bold red]")
+        return None
+    if len(containers) > 1 and id is None:
+        console.print("[bold red] ✗ Multiple inference servers running, provide an id.[/bold red]")
+        return None
+    (container,) = containers
+    return container
+
+
+@docker_cli.command("list", help="List running NOS inference servers.")
+def _docker_list():
+    """List running docker inference servers.
+
+    Usage:
+        $ nos docker list
+    """
+    containers = InferenceServiceRuntime.list()
+    if len(containers) == 0:
+        console.print("[bold red] ✗ No inference server running.[/bold red]")
+        return
+    console.print("[bold green] ✓ Inference servers running:[/bold green]")
+    for container in containers:
+        console.print(
+            f"    - id={container.id[:12]} name={container.name: <40} status={container.status}  image={container.image}"
+        )
+
+
+@docker_cli.command("start", help="Start NOS inference server.")
 def _docker_start(
-    gpu: bool = typer.Option(False, "--gpu", help="Start the container with GPU support."),
-    shm_size: str = typer.Option("4g", "--shm-size", help="Size of /dev/shm."),
+    runtime: str = typer.Option("cpu", "--runtime", help="The runtime to use (cpu or gpu)."),
 ):
-    """Start the NOS inference engine.
+    """Start the NOS inference server.
 
     Usage:
         $ nos docker start
     """
-    with rich.status.Status("[bold green] Starting inference client ...[/bold green]") as status:
-        client = InferenceServiceRuntime()
-        if client.ready():
+    if runtime not in InferenceServiceRuntime.configs:
+        raise ValueError(
+            f"Invalid inference runtime: {runtime}, available: {list(InferenceServiceRuntime.configs.keys())}"
+        )
+
+    containers = InferenceServiceRuntime.list()
+    if len(containers) > 0:
+        container = containers[0]
+        console.print(
+            f"[bold yellow] ✗ Inference server already running (name={container.name}, id={container.id[:12]}).[/bold yellow]"
+        )
+        return
+    with rich.status.Status("[bold green] Starting inference server ...[/bold green]") as status:
+        runtime = InferenceServiceRuntime(runtime=runtime)
+        if runtime.get_container_status() == "running":
             status.stop()
-            id = client.id()
-            console.print(
-                f"[bold green] ✓ Inference client already running (id={id[:12] if id else None}).[/bold green]"
-            )
+            id = runtime.get_container_id()
+            console.print(f"[bold green] ✓ Inference server already running (id={id[:12]}).[/bold green]")
             return
-        client.start(detach=True, gpu=gpu, shm_size=shm_size)
-        id = client.id()
-    console.print(f"[bold green] ✓ Inference client started (id={id[:12] if id else None}). [/bold green]")
+        runtime.start()
+        id = runtime.get_container_id()
+    console.print(f"[bold green] ✓ Inference server started (id={id[:12]}). [/bold green]")
 
 
 @docker_cli.command("stop", help="Stop NOS inference engine.")
-def _docker_stop():
-    """Stop the docker inference engine.
+def _docker_stop(
+    id: str = typer.Option(None, "--id", help="The id of the inference server container."),
+):
+    """Stop the docker inference servers.
 
     Usage:
         $ nos docker stop
+        $ nos docker stop --id <id>
     """
-    with rich.status.Status("[bold green] Stopping inference client ...[/bold green]"):
-        client = InferenceServiceRuntime()
-        client.stop()
-    console.print("[bold green] ✓ Inference client stopped.[/bold green]")
+    container = get_running_inference_service_runtime_container(id=id)
+    if container is None:
+        return
+    id = container.id
+    with rich.status.Status("[bold green] Stopping inference server ...[/bold green]"):
+        try:
+            container.remove(force=True)
+        except Exception:
+            console.print("[bold red] ✗ Failed to stop inference server.[/bold red]")
+            return
+    console.print(f"[bold green] ✓ Inference server stopped (id={id[:12]}).[/bold green]")
 
 
-@docker_cli.command("logs", help="Get NOS inference engine logs.")
-def _docker_logs():
-    """Get the docker logs of the inference engine.
+@docker_cli.command("logs", help="Get NOS inference server logs.")
+def _docker_logs(
+    id: str = typer.Option(None, "--id", help="The id of the inference server container."),
+):
+    """Get the docker logs of the inference server.
 
     Usage:
-        $ nos docker logs
+        $ nos docker logs <id>
     """
-    client = InferenceServiceRuntime()
-    id = client.id()
-    with rich.status.Status(f"[bold green] Fetching client logs (id={id[:12] if id else None}) ...[/bold green]"):
-        logs = client.get_logs()
-    print(logs)
+    container = get_running_inference_service_runtime_container(id=id)
+    if container is None:
+        return
+    id = container.id
+    console.print(f"[bold green] Fetching server logs (id={id[:12]}) ...[/bold green]")
+    for line in container.logs(stream=True):
+        print(line.decode("utf-8").strip())
```

## nos/cli/system.py

```diff
@@ -1,96 +1,43 @@
-import platform
-import subprocess
-from typing import Optional
-
-import psutil
-import torch
 import typer
 from rich.console import Console
+from rich.json import JSON
 from rich.panel import Panel
 
 from docker.errors import APIError
+from nos.common.system import get_system_info, has_gpu
 from nos.logging import logger
 from nos.server.docker import DockerRuntime
 
 
 system_cli = typer.Typer(name="system", help="NOS System CLI.", no_args_is_help=True)
 console = Console()
 
 
-def sh(command: str) -> None:
-    """Execute shell command, returning stdout."""
-    try:
-        output = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
-        return output.stdout.strip()
-    except subprocess.CalledProcessError:
-        logger.error(f"Failed to execute command: {command}")
-        return None
-
-
-def get_docker_version() -> Optional[str]:
-    """Get docker version, if installed."""
-    return sh("docker --version")
-
-
-def get_nvidia_smi() -> Optional[str]:
-    """Get nvidia-smi details, if installed."""
-    return sh(
-        "nvidia-smi --query-gpu=name,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,memory.total --format=csv"
-    )
-
-
 @system_cli.command("info")
 def _system_info() -> None:
     """Get system information (including CPU, GPU, RAM, etc.)"""
 
     # Get system info
-    sys_info = f"""
-    System: {platform.system()}
-    Release: {platform.release()}
-    Version: {platform.version()}
-    Machine: {platform.machine()}
-    Architecture: {platform.architecture()}
-    Processor Type: {platform.processor()}
-    Python Implementation: {platform.python_implementation()}
-    CPU Count: {psutil.cpu_count()}
-    """
-    console.print(Panel(sys_info, title="System"))
+    console.print(Panel(JSON.from_data(get_system_info(docker=True, gpu=True)), title="System"))
 
-    # Get docker version
-    console.print(Panel(f"{get_docker_version()}", title="Docker"))
-
-    # Get nvidia-smi details
-    console.print(Panel(f"{get_nvidia_smi()}", title="nvidia-smi"))
-
-    # Check if GPU is available (via torch)
-    torch_gpu_info = ""
-    try:
-        torch_gpu_info = (
-            f"""GPU Count: {torch.cuda.device_count()}\n"""
-            f"""CUDA Version: {torch.version.cuda}\n"""
-            f"""CUDNN Version: {torch.backends.cudnn.version()}\n"""
-        )
-        if torch.cuda.is_available():
-            for i in range(torch.cuda.device_count()):
-                torch_gpu_info += "\n" if i > 0 else ""
-                torch_gpu_info += f"""[{i}] Device Properties: ({torch.cuda.get_device_properties(i)})"""
-    except ModuleNotFoundError:
-        logger.error("Failed to fetch torch versions")
-        torch_gpu_info = "GPU: None"
-    console.print(Panel(torch_gpu_info, title="Torch"))
+    if not has_gpu():
+        console.print("No GPU detected, exiting early.")
+        return
+    else:
+        console.print("GPU detected, fetching nvidia-smi information within docker.")
 
     # Check if GPU is available
     CUDA_RUNTIME_IMAGE = "nvidia/cuda:11.8.0-base-ubuntu22.04"
     nvidia_docker_gpu_info = ""
 
-    executor = DockerRuntime()
+    runtime = DockerRuntime()
     try:
         # Get the output of nvidia-smi running in the container
-        container = executor.start(
+        container = runtime.start(
             image=CUDA_RUNTIME_IMAGE, name="nos-server-gpu-test", command="nvidia-smi", detach=True, gpu=True
         )
         for i, log in enumerate(container.logs(stream=True)):
             nvidia_docker_gpu_info += "\n" if i > 0 else ""
             nvidia_docker_gpu_info += f"{log.strip().decode()}"
         container.stop()
     except (APIError, ModuleNotFoundError, Exception) as exc:
```

## nos/client/grpc.py

```diff
@@ -179,15 +179,14 @@
         Returns:
             List[ModelSpec]: List of ModelInfo (name, task).
         Raises:
             NosClientException: If the server fails to respond to the request.
         """
         try:
             response: nos_service_pb2.ModelListResponse = self.stub.ListModels(empty_pb2.Empty())
-            logger.debug(response.models)
             return [ModelSpec(name=minfo.name, task=TaskType(minfo.task)) for minfo in response.models]
         except grpc.RpcError as e:
             raise NosClientException(f"Failed to list models ({e})")
 
     def GetModelInfo(self, spec: ModelSpec) -> ModelSpec:
         """Get the relevant model information from the model name.
 
@@ -204,27 +203,27 @@
                 )
             )
             model_spec: ModelSpec = loads(response.response_bytes)
             return model_spec
         except grpc.RpcError as e:
             raise NosClientException(f"Failed to get model info ({e})")
 
-    @lru_cache(maxsize=32)  # noqa: B019
+    @lru_cache(maxsize=8)  # noqa: B019
     def Module(self, task: TaskType, model_name: str) -> "InferenceModule":
         """Instantiate a model module.
 
         Args:
             task (TaskType): Task used for prediction.
             model_name (str): Name of the model to init.
         Returns:
             InferenceModule: Inference module.
         """
         return InferenceModule(task, model_name, self)
 
-    @lru_cache(maxsize=32)  # noqa: B019
+    @lru_cache(maxsize=8)  # noqa: B019
     def ModuleFromSpec(self, spec: ModelSpec) -> "InferenceModule":
         """Instantiate a model module from a model spec.
 
         Args:
             spec (ModelSpec): Model specification.
         Returns:
             InferenceModule: Inference module.
@@ -324,9 +323,13 @@
             inputs=inputs,
         )
         try:
 
             response = self.stub.Run(request)
             response = loads(response.response_bytes)
             return response
-        except grpc.RpcError as e:
-            raise NosClientException(f"Failed to run model {self.model_name} ({e})")
+        except Exception as e:
+            import traceback
+
+            raise NosClientException(
+                f"""Failed to run model {self.model_name} ({e})""" f"""\nTraceback\n""" f"""{traceback.format_exc()}"""
+            )
```

## nos/common/__init__.py

```diff
@@ -1,6 +1,43 @@
+import time
 from typing import Any
 
+from tqdm import tqdm as _tqdm
+
 from .cloudpickle import dumps, loads
-from .spec import FunctionSignature, ModelSpec, ObjectTypeInfo  # noqa: F401
-from .tasks import TaskType  # noqa: F401
-from .types import Batch, EmbeddingSpec, ImageSpec, ImageT, TensorSpec, TensorT  # noqa: F401
+from .spec import FunctionSignature, ModelSpec, ObjectTypeInfo
+from .tasks import TaskType
+from .types import Batch, EmbeddingSpec, ImageSpec, ImageT, TensorSpec, TensorT
+
+
+def tqdm(iterable: Any = None, *args, **kwargs) -> Any:
+    """Wrapper around tqdm that allows for a duration to be
+    specified instead of an iterable
+
+    Args:
+        iterable (Any, optional): Iterable to wrap. Defaults to None.
+    Returns:
+        tqdm.tqdm: Wrapped tqdm iterable.
+    """
+    if iterable is not None:
+        return _tqdm(iterable, *args, **kwargs)
+
+    # Get duration in milliseconds
+    try:
+        duration_s = kwargs.pop("duration")
+        duration_ms = duration_s * 1_000
+    except KeyError:
+        raise KeyError("`duration` must be specified when no iterable is provided")
+
+    # Yield progress bar for the specified duration
+    def iterable():
+        idx = 0
+        st_ms = time.perf_counter() * 1_000
+        while True:
+            now_ms = time.perf_counter() * 1_000
+            elapsed_ms = now_ms - st_ms
+            if elapsed_ms >= duration_ms:
+                return
+            yield idx
+            idx += 1
+
+    return _tqdm(iterable(), *args, **kwargs)
```

## nos/common/spec.py

```diff
@@ -7,14 +7,17 @@
 
 from nos.common.cloudpickle import dumps, loads
 from nos.common.tasks import TaskType
 from nos.common.types import Batch, EmbeddingSpec, ImageSpec, ImageT, TensorSpec, TensorT  # noqa: F401
 from nos.protoc import import_module
 
 
+# TOFIX (spillai): Remove Any type, and explicitly define input/output types.
+FunctionSignatureType = Union[Type[int], Type[str], Type[float], Any]
+
 nos_service_pb2 = import_module("nos_service_pb2")
 
 
 class ObjectTypeInfo:
     """Function signature information.
 
     Parameters:
@@ -109,18 +112,17 @@
 
 @dataclass
 class FunctionSignature:
     """Function signature that fully describes the remote-model to be executed
     including `inputs`, `outputs`, `func_or_cls` to be executed,
     initialization `args`/`kwargs`."""
 
-    # TOFIX (spillai): Remove Any type, and explicitly define input/output types.
-    inputs: Dict[str, Union[Type[int], Type[str], Type[float], Any]]
+    inputs: Dict[str, FunctionSignatureType]
     """Mapping of input names to dtypes."""
-    outputs: Dict[str, Union[Type[int], Type[str], Type[float], Any]]
+    outputs: Dict[str, FunctionSignatureType]
     """Mapping of output names to dtypes."""
 
     """The remaining private fields are used to instantiate a model and execute it."""
     func_or_cls: Optional[Callable] = None
     """Class instance."""
     init_args: Tuple[Any, ...] = field(default_factory=tuple)
     """Arguments to initialize the model instance."""
@@ -129,29 +131,30 @@
     method_name: str = None
     """Class method name. (e.g. forward, __call__ etc)"""
 
     def __repr__(self) -> str:
         """Return the function signature representation."""
         return f"FunctionSignature(inputs={self.inputs}, outputs={self.outputs}, func_or_cls={self.func_or_cls}, init_args={self.init_args}, init_kwargs={self.init_kwargs}, method_name={self.method_name})"
 
-    def _validate_inputs(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
-        """Validate the input dict against the defined signature and decode it."""
-        if len(set(self.inputs.keys()).symmetric_difference(set(inputs.keys()))) > 0:
-            raise ValueError(f"Invalid inputs, provided={set(inputs.keys())}, expected={set(self.inputs.keys())}.")
+    @staticmethod
+    def validate(inputs: Dict[str, Any], sig: Dict[str, FunctionSignatureType]) -> Dict[str, Any]:
+        """Validate the input dict against the defined signature (input or output)."""
+        if len(set(sig.keys()).symmetric_difference(set(inputs.keys()))) > 0:
+            raise ValueError(f"Invalid inputs, provided={set(inputs.keys())}, expected={set(sig.keys())}.")
         # TODO (spillai): Validate input types and shapes.
         return inputs
 
     def _encode_inputs(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         """Encode inputs based on defined signature."""
-        inputs = self._validate_inputs(inputs)
+        inputs = FunctionSignature.validate(inputs, self.inputs)
         return {k: dumps(v) for k, v in inputs.items()}
 
     def _decode_inputs(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         """Decode inputs based on defined signature."""
-        inputs = self._validate_inputs(inputs)
+        inputs = FunctionSignature.validate(inputs, self.inputs)
         return {k: loads(v) for k, v in inputs.items()}
 
     def get_inputs_spec(self) -> Dict[str, Union[ObjectTypeInfo, List[ObjectTypeInfo]]]:
         """Return the full input function signature specification.
 
         For example, for CLIP's text embedding model, the inputs/output spec is:
             ```
```

## nos/common/types.py

```diff
@@ -48,14 +48,22 @@
     def validate_dtype(cls, dtype: str):
         """Validate the dtype."""
         if dtype and not hasattr(np, dtype):
             raise ValidationError(f"Invalid dtype [dtype={dtype}].")
         else:
             return dtype
 
+    @property
+    def nbytes(self) -> Optional[int]:
+        """Return the number of bytes required to store the tensor."""
+        try:
+            return np.prod(self.shape) * np.dtype(self.dtype).itemsize
+        except TypeError:
+            return None
+
 
 @dataclass(frozen=True)
 class ImageSpec(TensorSpec):
     """Image tensor specification with dimensions (H, W, C)."""
 
     @validator("shape")
     def validate_shape(cls, shape: Tuple[Optional[int], ...]):
```

## nos/common/io/__init__.py

```diff
@@ -1 +1,29 @@
-from .video.opencv import VideoFile  # noqa: F401
+from typing import List, Union
+
+import numpy as np
+from PIL import Image
+
+from .video.opencv import VideoReader, VideoWriter  # noqa: F401
+
+
+def prepare_images(images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]) -> List[np.ndarray]:
+    """Prepare images for inference.
+
+    Args:
+        images: A single image or a list of images (PIL.Image or np.ndarray)
+
+    Returns:
+        images: A list of images (np.ndarray)
+    """
+    if isinstance(images, np.ndarray):
+        images = [images]
+    elif isinstance(images, Image.Image):
+        images = [np.asarray(images.convert("RGB"))]
+    elif isinstance(images, list):
+        if isinstance(images[0], Image.Image):
+            images = [np.asarray(image.convert("RGB")) for image in images]
+        elif isinstance(images[0], np.ndarray):
+            pass
+        else:
+            raise ValueError(f"Invalid type for images: {type(images[0])}")
+    return images
```

## nos/common/io/video/base.py

```diff
@@ -2,15 +2,15 @@
 from abc import ABC, abstractmethod
 from typing import Iterator, Optional, TypeVar
 
 
 T = TypeVar("T")
 
 
-class BaseVideoFile(ABC):
+class BaseVideoReader(ABC):
     def __repr__(self) -> str:
         return f"{self.__class__.__name__} [fn={self.filename}]"
 
     @abstractmethod
     def __len__(self) -> int:
         raise NotImplementedError()
```

## nos/common/io/video/opencv.py

```diff
@@ -1,20 +1,21 @@
 from pathlib import Path
 from typing import Callable, Iterator, List, Optional, Union
 
 import cv2
 import numpy as np
 
-from nos.common.io.video.base import BaseVideoFile
+from nos.common.io.video.base import BaseVideoReader
+from nos.logging import logger
 
 
 T = np.ndarray
 
 
-class VideoFile(BaseVideoFile):
+class VideoReader(BaseVideoReader):
     """Video Reader with OpenCV backend."""
 
     def __init__(self, filename: Union[str, Path], transform: Optional[Callable] = None, bridge: str = "numpy"):
         """Initialize video reader.
 
         Args:
             filename (Union[str, Path]): The path to the video file.
@@ -125,7 +126,119 @@
         if idx < 0 or idx >= len(self):
             raise IndexError(f"Invalid index {idx}")
         self._video.set(cv2.CAP_PROP_POS_FRAMES, idx)
 
     def reset(self) -> None:
         """Reset the video to the beginning."""
         self.seek(0)
+
+
+class VideoWriter:
+    """Video Writer with OpenCV backend."""
+
+    def __init__(self, filename: Union[str, Path], fps: int = 30):
+        """Initialize video writer.
+
+        Args:
+            filename (Union[str, Path]): The path to the video file.
+            fps (int, optional): The frames per second. Defaults to 30.
+        """
+        self.filename = Path(str(filename))
+        if self.filename.exists():
+            raise FileExistsError(f"{self.filename} already exists")
+        self.fps = fps
+        self.writer = None
+
+    def write(self, img: np.ndarray) -> None:
+        """Write the given image to the video file.
+
+        Args:
+            img (np.ndarray): The image to write.
+        Returns:
+            None
+        """
+        # Create video writer if it does not exist.
+        if self.writer is None:
+            H, W = img.shape[:2]
+            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
+            self.writer = cv2.VideoWriter(str(self.filename), fourcc, self.fps, (W, H), img.ndim == 3)
+            logger.debug(
+                f"""{self.__class__.__name__} :: Create video writer """
+                f"""[filename={self.filename}, W={W}, H={H}]"""
+            )
+        # Write image to video writer in BGR format.
+        self.writer.write(img[..., ::-1])
+
+    def close(self, reencode: bool = True):
+        """Close the video writer, re-encoding the video if needed."""
+        if self.writer is None:
+            return
+        logger.debug(f"Closing video writer [filename={self.filename}].")
+        self.writer.release()
+        self.writer = None
+        # Re-encode video on releasing for better compatibility and
+        # compression ratios.
+        if reencode:
+            try:
+                VideoWriter.reencode_video(self.filename)
+            except Exception as e:
+                logger.warning(f"Failed to re-encode video, skipping [filename={self.filename}]: {e}")
+        logger.debug(f"Closed video writer [filename={self.filename}].")
+
+    def __del__(self):
+        """Close the video writer cleanly."""
+        self.close()
+
+    def __enter__(self):
+        """Enter context manager."""
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        """Exit context manager."""
+        self.close()
+
+    @staticmethod
+    def reencode_video(filename: Union[str, Path]) -> str:
+        """Re-encode video to ensure compatibility with OpenCV.
+
+        Args:
+            filename (Union[str, Path]): The path to the video file.
+
+        Raises:
+            FileNotFoundError: If the video file does not exist.
+
+        Returns:
+            str: The path to the re-encoded video file.
+        """
+        import uuid
+        from pathlib import Path
+        from subprocess import call
+
+        input_path = Path(str(filename))
+        if not input_path.exists():
+            raise FileNotFoundError(f"{input_path} does not exist")
+        output_path = input_path.parent / f"{str(uuid.uuid4().hex)}.mp4"
+        try:
+            logger.debug(f"re-encode video [filename={str(input_path)}, output={str(output_path)}]")
+            cmd = (
+                f"""ffmpeg -loglevel error -vsync 0 """
+                f"""-i '{str(input_path)}' -c:v libx264 """
+                f"""-pix_fmt yuv420p """
+                f"""-vsync 0 -an {str(output_path)}"""
+            )
+            call([cmd], shell=True)
+        except Exception:
+            logger.error(f"re-encode video failed [filename={str(input_path)}]")
+            logger.error(f"[cmd={cmd}]")
+            raise
+
+        if not output_path.exists():
+            raise RuntimeError(f"Failed to re-encode video [filename={str(input_path)}]")
+        logger.debug(
+            f"""re-encode video overwriting [input={str(input_path)} """
+            f"""({input_path.stat().st_size / 1024 ** 2:.2f} MB), """
+            f"""output={str(output_path)} """
+            f"""({output_path.stat().st_size / 1024 ** 2:.2f} MB)"""
+        )
+        logger.debug(f"re-encode video overwriting [filename={str(input_path)}]")
+        output_path.rename(input_path)
+        return str(input_path)
```

## nos/executors/ray.py

```diff
@@ -20,14 +20,15 @@
 from nos.logging import LOGGING_LEVEL
 
 
 logger = logging.getLogger(__name__)
 
 NOS_RAY_NS = os.getenv("NOS_RAY_NS", "nos-dev")
 NOS_RAY_RUNTIME_ENV = os.getenv("NOS_RAY_ENV", None)
+NOS_RAY_OBJECT_STORE_MEMORY = int(os.getenv("NOS_RAY_OBJECT_STORE_MEMORY", 2 * 1024 * 1024 * 1024))  # 2GB
 
 
 @dataclass
 class RayRuntimeSpec:
     namespace: str = NOS_RAY_NS
     """Namespace for Ray runtime."""
     runtime_env: str = NOS_RAY_RUNTIME_ENV
@@ -90,16 +91,16 @@
                 with console.status(
                     "[bold green] InferenceExecutor :: Connecting to backend ... [/bold green]"
                 ) as status:
                     logger.debug(f"Connecting to executor: namespace={self.spec.namespace}")
                     ray.init(
                         address="auto",
                         namespace=self.spec.namespace,
-                        runtime_env=self.spec.runtime_env,
                         ignore_reinit_error=True,
+                        include_dashboard=False,
                         configure_logging=True,
                         logging_level=logging.ERROR,
                         log_to_driver=level <= logging.ERROR,
                     )
                     status.stop()
                     console.print("[bold green] ✓ InferenceExecutor :: Connected to backend. [/bold green]")
                     logger.debug(
@@ -111,20 +112,20 @@
                 # start it in a background subprocess.
                 if attempt > 0:
                     logger.error(
                         f"Failed to connect to InferenceExecutor.\n"
                         f"{exc}\n"
                         f"Retrying {attempt}/{max_attempts} after {retry_interval}s..."
                     )
+                    time.sleep(retry_interval)
                 else:
                     logger.debug("No executor found, starting a new one")
-                self.start()
+                    self.start()
                 attempt += 1
-
-                time.sleep(retry_interval)
+                continue
         logger.error(f"Failed to connect to InferenceExecutor: namespace={self.spec.namespace}.")
         return False
 
     def start(self) -> None:
         """Force-start a local instance of Ray head."""
         level = getattr(logging, LOGGING_LEVEL)
 
@@ -135,15 +136,15 @@
         ) as status:
             try:
                 logger.debug(f"Starting executor: namespace={self.spec.namespace}")
                 ray.init(
                     _node_name="nos-executor",
                     address="local",
                     namespace=self.spec.namespace,
-                    runtime_env=self.spec.runtime_env,
+                    object_store_memory=NOS_RAY_OBJECT_STORE_MEMORY,
                     ignore_reinit_error=False,
                     include_dashboard=False,
                     configure_logging=True,
                     logging_level=logging.ERROR,
                     log_to_driver=level <= logging.ERROR,
                 )
                 logger.debug(f"Started executor: namespace={self.spec.namespace} (time={time.time() - st:.2f}s)")
```

## nos/hub/__init__.py

```diff
@@ -18,14 +18,16 @@
         """Get the singleton instance.
 
         Returns:
             Hub: Singleton instance.
         """
         if cls._instance is None:
             cls._instance = cls()
+            # Register models / Populate the registry
+            import nos.models  # noqa: F401, E402
         return cls._instance
 
     @classmethod
     def list(cls, private: bool = False) -> List[ModelSpec]:
         """List models in the registry.
 
         Args:
@@ -95,10 +97,7 @@
 
 
 # Alias methods
 list = Hub.list
 load = Hub.load
 register = Hub.register
 load_spec = Hub.load_spec
-
-# Register models / Populate the registry
-import nos.models  # noqa: F401, E402
```

## nos/models/__init__.py

```diff
@@ -1,10 +1,8 @@
-import torch
-
 from nos import hub
 
 from .clip import CLIP  # noqa: F401
 from .faster_rcnn import FasterRCNN  # noqa: F401
 from .openmmlab.mmdetection.mmdetection import MMDetection  # noqa: F401
 from .sam import SAM
-from .stable_diffusion import StableDiffusion2  # noqa: F401
+from .stable_diffusion import StableDiffusion  # noqa: F401
 from .yolox import YOLOX  # noqa: F401
```

## nos/models/clip.py

```diff
@@ -1,23 +1,34 @@
 from dataclasses import dataclass
+from pathlib import Path
 from typing import List, Union
 
+import cv2
 import numpy as np
 import torch
 from PIL import Image
 
 from nos import hub
 from nos.common import EmbeddingSpec, TaskType
+from nos.common.io import prepare_images
 from nos.common.types import Batch, ImageT, TensorT
+from nos.compilers import compile
+from nos.constants import NOS_MODELS_DIR
 from nos.hub import HuggingFaceHubConfig
+from nos.logging import logger
 
 
 @dataclass(frozen=True)
 class CLIPConfig(HuggingFaceHubConfig):
     D: int = 512
+    """Dimension of the embedding."""
+    height: int = 224
+    """Height of the input image."""
+    width: int = 224
+    """Width of the input image."""
 
 
 class CLIP:
     """CLIP model for image and text encoding."""
 
     configs = {
         "openai/clip": CLIPConfig(
@@ -47,25 +58,30 @@
 
         try:
             self.cfg = CLIP.configs[model_name]
         except KeyError:
             raise ValueError(f"Invalid model_name: {model_name}, available models: {CLIP.configs.keys()}")
         model_name = self.cfg.model_name
         self.device = "cuda" if torch.cuda.is_available() else "cpu"
-        self.model = CLIPModel.from_pretrained(model_name).to(self.device)
+        # TOFIX (spillai): Regression with fp16
+        # https://github.com/autonomi-ai/nos/issues/198
+        # torch_dtype = torch.float16 if self.device == "cuda" else torch.float32
+        torch_dtype = torch.float32
+        self.model = CLIPModel.from_pretrained(model_name, torch_dtype=torch_dtype, torchscript=True).to(self.device)
         self.model.eval()
         self.tokenizer = CLIPTokenizer.from_pretrained(model_name)
         self.processor = CLIPProcessor.from_pretrained(model_name)
 
     def encode_image(self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]) -> np.ndarray:
         """Encode image into an embedding."""
+        images = prepare_images(images)
         with torch.inference_mode():
-            if isinstance(images, (np.ndarray, Image.Image)):
-                images = [images]
             inputs = self.processor(images=images, return_tensors="pt").to(self.device)
+            if self.device != "cuda" and self.model.dtype == torch.float16:
+                inputs = {k: v.half() for k, v in inputs.items()}
             image_features = self.model.get_image_features(**inputs)
             return image_features.cpu().numpy()
 
     def encode_text(self, texts: Union[str, List[str]]) -> np.ndarray:
         """Encode text into an embedding."""
         with torch.inference_mode():
             if isinstance(texts, str):
@@ -75,14 +91,135 @@
                 padding=True,
                 return_tensors="pt",
             ).to(self.device)
             text_features = self.model.get_text_features(**inputs)
             return text_features.cpu().numpy()
 
 
+class CLIPTensorRT(CLIP):
+    """TensorRT accelerated for CLIP with Torch TensorRT."""
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+        self._model_dir = Path(NOS_MODELS_DIR, f"cache/{self.cfg.model_name}")
+        self._model_dir.mkdir(parents=True, exist_ok=True)
+        self._patched = {}
+        self._patched_shape = None
+
+    @staticmethod
+    def _get_model_id(name: str, shape: torch.Size, dtype: torch.dtype) -> str:
+        """Get model id from model name, shape and dtype."""
+        replacements = {"/": "-", " ": "-"}
+        for k, v in replacements.items():
+            name = name.replace(k, v)
+        shape = list(map(int, shape))
+        shape_str = "x".join([str(s) for s in shape])
+        precision_str = str(dtype).split(".")[-1]
+        return f"{name}_{shape_str}_{precision_str}"
+
+    def _compile_vision_model(
+        self, inputs: List[torch.Tensor], precision: torch.dtype = torch.float32
+    ) -> torch.nn.Module:
+        """Vision model compilation flow."""
+        import torch.nn as nn
+        import torch_tensorrt.fx.converter_registry as registry
+        from torch_tensorrt.fx.tracer.acc_tracer import acc_ops
+        from transformers.modeling_outputs import BaseModelOutputWithPooling
+
+        assert isinstance(inputs, list), f"inputs must be a list, got {type(inputs)}"
+        assert len(inputs) == 1, f"inputs must be a list of length 1, got {len(inputs)}"
+        logger.debug(f"Compiling {self.cfg.model_name} (vision_model) with precision: {precision}")
+
+        class _CLIPVisionTransformer(nn.Module):
+            """Wrapper for the vision model with patched outputs.
+
+            We need this since the compilation removes the output types and
+            requires us to patch the outputs manually with the correct types
+            so that they can be used identically to the original model.
+            """
+
+            def __init__(self, model):
+                super().__init__()
+                self.model = model
+
+            def forward(self, *args, **kwargs):
+                return BaseModelOutputWithPooling(self.model(*args, **kwargs))
+
+        # TODO (spillai): The compilation for CLIP requires acc_ops_expand_tensor to be disabled
+        # to avoid the assertion thrown by the TRT backend. This is a temporary workaround
+        # until the issue is fixed upstream.
+        logger.debug("Disabling acc_ops.expand_tensor for CLIP compilation")
+        expand_op = None
+        if acc_ops.expand in registry.CONVERTERS.keys():
+            expand_op = registry.CONVERTERS.pop(acc_ops.expand)
+            logger.debug(f"Disabled {acc_ops.expand} from registry.CONVERTERS")
+
+        # Check if we have a cached model
+        model_id = CLIPTensorRT._get_model_id(f"{self.cfg.model_name}", inputs[0].shape, precision)
+        filename = f"{self._model_dir}/{model_id}.torchtrt.pt"
+        if Path(filename).exists():
+            logger.debug(f"Found cached {model_id}: {filename}")
+            trt_model = torch.load(filename)
+            self.model.vision_model = _CLIPVisionTransformer(trt_model)
+            return
+
+        # Compile the model backbone
+        # Note (spillai): Currently we hard-code the iamge size to 1x3x224x224
+        B, H, W = 1, self.cfg.height, self.cfg.width
+        args = {
+            "pixel_values": torch.randn((B, 3, H, W), dtype=precision, device="cuda"),
+            "output_attentions": self.model.config.output_attentions,
+            "output_hidden_states": self.model.config.output_hidden_states,
+            "return_dict": True,
+        }
+        try:
+            trt_model = compile(self.model.vision_model, args, concrete_args=None, precision=precision, slug=model_id)
+            logger.debug(f"Saving compiled {model_id} model to {filename}")
+            torch.save(trt_model, filename)
+            self.model.vision_model = _CLIPVisionTransformer(trt_model)
+            logger.debug(f"Patched {model_id} model")
+        except Exception as e:
+            import traceback
+
+            logger.error(f"Failed to compile {model_id} model\n{traceback.format_exc()}")
+            raise e
+
+        # Restore acc_ops.expand_tensor
+        if expand_op is not None:
+            logger.debug("Restoring acc_ops.expand_tensor for CLIP compilation")
+            registry.CONVERTERS[acc_ops.expand] = expand_op
+            logger.debug(f"Restored {acc_ops.expand} to registry.CONVERTERS")
+
+    def encode_image(self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]) -> np.ndarray:
+        """Encode image into an embedding."""
+        images = prepare_images(images)
+
+        # Resize images if necessary
+        if self._patched_shape is None:
+            H, W = self.cfg.height, self.cfg.width
+        else:
+            _, H, W = self._patched_shape
+        images = [cv2.resize(image, (W, H)) if image.shape[-2:] != (H, W) else image for image in images]
+
+        if not len(self._patched):
+            # Note (spillai): Force compilation with resized images
+            inputs = [torch.tensor(images)]
+            self._compile_vision_model(inputs, precision=self.model.dtype)
+            self._patched["vision_model"] = True
+            self._patched_shape = (len(images), H, W)
+
+        with torch.inference_mode():
+            inputs = self.processor(images=images, return_tensors="pt").to(self.device)
+            if self.device != "cuda" and self.model.dtype == torch.float16:
+                inputs = {k: v.half() for k, v in inputs.items()}
+            image_features = self.model.get_image_features(**inputs)
+            return image_features.cpu().numpy()
+
+
 # Register all CLIP models (for both tasks img2vec and txt2vec)
 for model_name in CLIP.configs:
     cfg = CLIP.configs[model_name]
     hub.register(
         model_name,
         TaskType.TEXT_EMBEDDING,
         CLIP,
```

## nos/models/stable_diffusion.py

```diff
@@ -1,91 +1,405 @@
-from typing import List, Union
+import time
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Callable, List, Union
 
 import torch
 from PIL import Image
+from transformers.utils.fx import HFTracer, symbolic_trace
 
 from nos import hub
 from nos.common import TaskType
 from nos.common.types import Batch, ImageSpec, ImageT
+from nos.constants import NOS_MODELS_DIR
+from nos.hub import HuggingFaceHubConfig
+from nos.logging import logger
 
 
-class StableDiffusion2:
+def get_model_id(name: str, shape: torch.Size, dtype: torch.dtype) -> str:
+    """Get model id from model name, shape and dtype."""
+    replacements = {"/": "-", " ": "-"}
+    for k, v in replacements.items():
+        name = name.replace(k, v)
+    shape = list(map(int, shape))
+    shape_str = "x".join([str(s) for s in shape])
+    precision_str = str(dtype).split(".")[-1]
+    return f"{name}_{shape_str}_{precision_str}"
+
+
+@dataclass(frozen=True)
+class StableDiffusionConfig(HuggingFaceHubConfig):
+    """Configuration for StableDiffusion model."""
+
+    pass
+
+
+class StableDiffusion:
     """StableDiffusion model for text to image generation."""
 
+    configs = {
+        "CompVis/stable-diffusion-v1-4": StableDiffusionConfig(
+            model_name="CompVis/stable-diffusion-v1-4",
+        ),
+        "runwayml/stable-diffusion-v1-5": StableDiffusionConfig(
+            model_name="runwayml/stable-diffusion-v1-5",
+        ),
+        "stabilityai/stable-diffusion-2": StableDiffusionConfig(
+            model_name="stabilityai/stable-diffusion-2",
+        ),
+        "stabilityai/stable-diffusion-2-1": StableDiffusionConfig(
+            model_name="stabilityai/stable-diffusion-2-1",
+        ),
+    }
+
     def __init__(
         self,
         model_name: str = "stabilityai/stable-diffusion-2",
         scheduler: str = "ddim",
         dtype: torch.dtype = torch.float16,
     ):
         from diffusers import (
             DDIMScheduler,
+            DiffusionPipeline,
             EulerDiscreteScheduler,
-            StableDiffusionPipeline,
         )
 
+        try:
+            self.cfg = StableDiffusion.configs[model_name]
+        except KeyError:
+            raise ValueError(f"Invalid model_name: {model_name}, available models: {StableDiffusion.configs.keys()}")
+
         # Only run on CUDA if available
         if torch.cuda.is_available():
             self.device = "cuda"
             self.dtype = dtype
+            self.revision = "fp16"
+            # TODO (spillai): Investigate if this has any effect on performance
+            # tf32 vs fp32: https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/279
             torch.backends.cuda.matmul.allow_tf32 = True
         else:
-            raise RuntimeError("No CUDA device available")
+            self.device = "cpu"
+            self.dtype = torch.float32
+            self.revision = None
 
         if scheduler == "ddim":
-            self.scheduler = DDIMScheduler.from_pretrained(model_name, subfolder="scheduler")
+            self.scheduler = DDIMScheduler.from_pretrained(self.cfg.model_name, subfolder="scheduler")
         elif scheduler == "euler-discrete":
-            self.scheduler = EulerDiscreteScheduler.from_pretrained(model_name, subfolder="scheduler")
+            self.scheduler = EulerDiscreteScheduler.from_pretrained(self.cfg.model_name, subfolder="scheduler")
         else:
             raise ValueError(f"Unknown scheduler: {scheduler}, choose from: ['ddim', 'euler-discrete']")
-        self.pipe = StableDiffusionPipeline.from_pretrained(
-            model_name,
+        self.pipe = DiffusionPipeline.from_pretrained(
+            self.cfg.model_name,
             scheduler=self.scheduler,
             torch_dtype=self.dtype,
-            revision="fp16",
+            revision=self.revision,
         )
         self.pipe = self.pipe.to(self.device)
-        self.pipe.enable_attention_slicing()
 
         # TODO (spillai): Pending xformers integration
+        # self.pipe.enable_attention_slicing()
         # self.pipe.enable_xformers_memory_efficient_attention()
 
     def __call__(
         self,
         prompts: Union[str, List[str]],
         num_images: int = 1,
         num_inference_steps: int = 50,
         guidance_scale: float = 7.5,
         height: int = None,
         width: int = None,
     ) -> List[Image.Image]:
         """Generate images from text prompt."""
         if isinstance(prompts, str):
             prompts = [prompts]
+        return self.pipe(
+            prompts * num_images,
+            num_inference_steps=num_inference_steps,
+            guidance_scale=guidance_scale,
+            height=height,
+            width=width,
+        ).images
+
+
+@dataclass(frozen=True)
+class StableDiffusionTensorRTCompilationConfig:
+    """Compilation configuration for StableDiffusion model."""
+
+    precision: torch.dtype = torch.float16
+    """Precision to use for TensorRT."""
+    width: int = 512
+    """Width of the image."""
+    height: int = 512
+    """Height of the image."""
+
+
+class StableDiffusionTensorRT(StableDiffusion):
+    """TensorRT accelerated StableDiffusion with Torch TensorRT."""
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+        self.verbose = kwargs.get("verbose", False)
+        self.model_dir = Path(NOS_MODELS_DIR, f"cache/{self.cfg.model_name}")
+        self.model_dir.mkdir(parents=True, exist_ok=True)
+        self._compilation_cfg = None
+        self._patched = {}
+
+    def get_inputs_vae(self, width: int = 512, height: int = 512, batch_size: int = 1):
+        """Get inputs for VAE."""
+        # x = torch.rand(batch_size, 3, height // 8, width // 8, dtype=self.pipe.vae.dtype, device=self.pipe.vae.device)
+        z = torch.rand(
+            2 * batch_size, 4, height // 8, width // 8, dtype=self.pipe.vae.dtype, device=self.pipe.vae.device
+        )
+        return [z]
+
+    def get_inputs_text_encoder(self):
+        """Get inputs for text encoder."""
+        model = self.pipe.text_encoder
+        args = model.dummy_inputs
+        inputs = list(args.values())
+        return [x.to(self.pipe.text_encoder.device) for x in inputs]
+
+    def _trace_text_encoder(
+        self, model: torch.nn.Module, dtype: torch.dtype = torch.float32
+    ) -> torch.fx.graph_module.GraphModule:
+        """Trace the text encoder of the model."""
+        model = self.pipe.text_encoder
+        args = model.dummy_inputs
+        return symbolic_trace(model, input_names=list(args.keys()), disable_check=False, tracer_cls=HFTracer)
+
+    def _trace_vae(
+        self, model: torch.nn.Module, dtype: torch.dtype = torch.float32
+    ) -> torch.fx.graph_module.GraphModule:
+        """Trace the VAE of the model."""
+        import torch.nn as nn
+
+        # Note: For SDv2, we need to only trace `vae.decode` since that is method that is
+        # called within SDv2's forward pass. If we try to trace the full VAE directly,
+        # it returns a `GraphModule` that cannot access the `decode` method. We workaround
+        # this by wrapping the decode method below in a `_AutoEncoderKLDecoder` class that
+        # calls the `vae.decode` method and only trace the relevant codepaths for inference.
+        class _AutoEncoderKLDecoder(nn.Module):
+            """Wrapper for VAE to only trace the `decode` method."""
+
+            def __init__(self, model):
+                super().__init__()
+                self.model = model
+
+            def forward(self, z: torch.FloatTensor, return_dict: bool = False):
+                return self.model.decode(z, return_dict=return_dict)
+
+        model = _AutoEncoderKLDecoder(self.pipe.vae)
+        (z,) = self.get_inputs_vae()
+        args = {"z": z}
+        concrete_args = {
+            "return_dict": False,
+        }
+
+        logger.debug("Tracing VAE with HFTracer")
+        tracer = HFTracer()
+        with torch.inference_mode():
+            traced_graph = tracer.trace(model, concrete_args=concrete_args, dummy_inputs=args)
+            traced_model = torch.fx.GraphModule(model, traced_graph)
+        if self.verbose:
+            traced_graph.print_tabular()
+
+        traced_model.config = self.pipe.vae.config
+        # The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus
+        # _generate_dummy_input, where the model class is needed.
+        traced_model.class_for_deserialization = model.__class__
+        traced_model.device = self.pipe.vae.device
+        return traced_model
+
+    def _trace_unet(self, dtype: torch.dtype = torch.float32) -> torch.fx.graph_module.GraphModule:
+        raise NotImplementedError("TODO: Implement UNet tracing")
+
+    def _compile(
+        self,
+        trace_fn: Callable,
+        model: torch.nn.Module,
+        inputs: List[torch.Tensor],
+        precision: torch.dtype = torch.float32,
+        slug: str = "model",
+    ) -> torch.nn.Module:
+        """Compile the text encoder / VAE
+
+        Models are written to disk in the following format:
+            f"{model_dir}/{model_id}.torchtrt.pt"
+            ~/.nos/models/cache/openai-clip-vit-base-32--vae-HxW-fp32.torchtrt.pt
+        """
+        import torch_tensorrt
+        from torch_tensorrt.fx.utils import LowerPrecision
+
+        model_id = get_model_id(f"{self.cfg.model_name}--{slug}", inputs[0].shape, precision)
+        filename = f"{self.model_dir}/{model_id}.torchtrt.pt"
+        if Path(filename).exists():
+            logger.debug(f"Found cached {slug}: {filename}")
+            trt_model = torch.load(filename)
+            return trt_model
+
+        # TODO (spillai): Check if we have traced the right inputs here
+        try:
+            traced_model = trace_fn(model, precision)
+        except Exception as e:
+            logger.error(f"Failed to trace: {e}, skipping compilation")
+            return None
+
         with torch.inference_mode():
-            with torch.autocast("cuda"):
-                images = self.pipe(
-                    prompts * num_images,
-                    num_inference_steps=num_inference_steps,
-                    guidance_scale=guidance_scale,
-                    height=height,
-                    width=width,
-                ).images
-                return images
+            st = time.time()
+            logger.debug(f"Compiling {slug} with TensorRT")
+            try:
+                GB_bytes = 1024 * 1024 * 1024
+                trt_model = torch_tensorrt.fx.compile(
+                    traced_model,
+                    inputs,
+                    min_acc_module_size=5,
+                    max_batch_size=2048,
+                    max_workspace_size=16 * GB_bytes,
+                    explicit_batch_dimension=True,
+                    lower_precision=LowerPrecision.FP32 if precision == torch.float32 else LowerPrecision.FP16,
+                    verbose_log=True,
+                    timing_cache_prefix="",
+                    save_timing_cache=False,
+                    cuda_graph_batch_size=-1,
+                    dynamic_batch=False,
+                    is_aten=False,
+                    use_experimental_fx_rt=False,
+                    # correctness_atol=0.1,
+                    # correctness_rtol=0.1
+                )
+            except Exception as e:
+                logger.error(f"Failed to compile {slug}: {e}, skipping compilation")
+                return None
+            logger.debug(f"Compiled {slug} in {time.time() - st:.2f}s")
+
+        logger.debug(f"Saving compiled {slug} model to {filename}")
+        torch.save(trt_model, filename)
+        return trt_model
+
+    def _compile_vae(self, inputs: List[torch.Tensor], precision: torch.dtype = torch.float32) -> bool:
+        """Compile the VAE model."""
+        return self._compile(self._trace_vae, self.pipe.vae, inputs, precision, slug="vae")
+
+    def _compile_text_encoder(self, inputs: List[torch.Tensor], precision: torch.dtype = torch.float32) -> bool:
+        """Compile the text encoder model."""
+        return self._compile(self._trace_text_encoder, self.pipe.text_encoder, inputs, precision, slug="text-encoder")
+
+    def _compile_unet(self, inputs: List[torch.Tensor], precision: torch.dtype = torch.float32) -> str:
+        """Compile the UNet model."""
+        try:
+            logger.info("Compiling UNet model")
+            torch._dynamo.config.verbose = False
+            torch._dynamo.config.suppress_errors = True
+            torch._dynamo.reset()
+            with torch.no_grad():
+                torch.cuda.empty_cache()
+            self.pipe.unet.to(memory_format=torch.channels_last)
+            self.pipe.unet = torch.compile(self.pipe.unet, mode="reduce-overhead", fullgraph=True)
+        except RuntimeError as e:
+            logger.error(f"Failed to compile UNet: {e}")
+
+    def __compile__(
+        self, inputs: List[torch.Tensor], precision: torch.dtype = torch.float32, width: int = 512, height: int = 512
+    ) -> str:
+        """Model compilation flow."""
+        st = time.time()
+        logger.debug("Compiling and patching model")
+        try:
+            logger.info("Compiling VAE model")
+            vae_inputs = self.get_inputs_vae(width=width, height=height)
+            trt_model = self._compile_vae(vae_inputs, precision)
+            if trt_model is None:
+                raise RuntimeError("Failed to compile VAE")
+            logger.debug("Patching VAE model")
+            self.pipe.vae.decode = trt_model
+            self._patched["vae"] = True
+            logger.debug("Done patching VAE model")
+        except Exception as e:
+            logger.error(f"Failed to compile VAE: {e}")
+            self._patched["vae"] = False
+
+        try:
+            raise NotImplementedError("TODO: Implement text-encoder compilation")
+            logger.info("Compiling text encoder model")
+            text_inputs = self.get_inputs_text_encoder()
+            trt_model = self._compile_text_encoder(text_inputs, precision)
+            if trt_model is None:
+                raise RuntimeError("Failed to compile text encoder")
+            logger.debug("Patching text encoder model")
+            self.pipe.text_encoder = trt_model
+            self._patched["text_encoder"] = True
+            logger.debug("Done patching text encoder model")
+        except Exception as e:
+            logger.error(f"Failed to compile text encoder: {e}")
+            self._patched["text_encoder"] = False
+
+        try:
+            raise NotImplementedError("TODO: Implement UNet compilation")
+            logger.info("Compiling UNet model")
+            opt_model = self._compile_unet(inputs, precision)
+            if opt_model is None:
+                raise RuntimeError("Failed to compile UNet")
+            logger.debug("Patching UNet model")
+            self.pipe.unet = opt_model
+            self._patched["unet"] = True
+            logger.debug("Done patching UNet model")
+        except Exception as e:
+            logger.error(f"Failed to compile UNet: {e}")
+            self._patched["unet"] = False
+
+        # TODO (spillai): Investigate why we need to do this here.
+        try:
+            self.pipe._execution_device = self.device
+        except Exception:
+            logger.warning("Failed to set execution device")
+        logger.debug(f"Done compiling models in {time.time() - st:.2f}s")
+
+    def __call__(
+        self,
+        prompts: Union[str, List[str]],
+        num_images: int = 1,
+        num_inference_steps: int = 50,
+        guidance_scale: float = 7.5,
+        height: int = 512,
+        width: int = 512,
+    ) -> List[Image.Image]:
+        """Generate images from text prompt."""
+
+        if isinstance(prompts, str):
+            prompts = [prompts]
+
+        # TODO (spillai): Capture all of this in StableDiffusionTensorRTConfig?
+        if not len(self._patched):
+            self._compilation_cfg = StableDiffusionTensorRTCompilationConfig(
+                precision=self.pipe.vae.dtype,
+                width=width,
+                height=height,
+            )
+            self.__compile__(prompts, precision=self.pipe.unet.dtype, width=width, height=height)
+
+        return self.pipe(
+            prompts * num_images,
+            num_inference_steps=num_inference_steps,
+            guidance_scale=guidance_scale,
+            height=height,
+            width=width,
+        ).images
 
 
 # Register the model with the hub
 # TODO (spillai): Ideally, we should do this via a decorator
 # @hub.register("stabilityai/stable-diffusion-2")
 # def stable_diffusion_2_ddim_fp16():
 #     return StableDiffusion2("stabilityai/stable-diffusion-2", scheduler="ddim", dtype=torch.float16)
 #
-hub.register(
-    "stabilityai/stable-diffusion-2",
-    TaskType.IMAGE_GENERATION,
-    StableDiffusion2,
-    init_args=("stabilityai/stable-diffusion-2",),
-    init_kwargs={"scheduler": "ddim", "dtype": torch.float16},
-    method_name="__call__",
-    inputs={"prompts": Batch[str], "num_images": int, "height": int, "width": int},
-    outputs={"images": Batch[ImageT[Image.Image, ImageSpec(shape=(None, None, 3), dtype="uint8")]]},
-)
+for model_name in StableDiffusion.configs.keys():
+    hub.register(
+        model_name,
+        TaskType.IMAGE_GENERATION,
+        StableDiffusion,
+        init_args=(model_name,),
+        init_kwargs={"scheduler": "ddim", "dtype": torch.float16},
+        method_name="__call__",
+        inputs={"prompts": Batch[str], "num_images": int, "height": int, "width": int},
+        outputs={"images": Batch[ImageT[Image.Image, ImageSpec(shape=(None, None, 3), dtype="uint8")]]},
+    )
```

## nos/models/yolox.py

```diff
@@ -1,20 +1,25 @@
 from dataclasses import dataclass
-from typing import List, Union
+from pathlib import Path
+from typing import Dict, List, Union
 
 import numpy as np
 import torch
 import torchvision.transforms.functional as F
 from PIL import Image
 from torchvision import ops
 
 from nos import hub
 from nos.common import ImageSpec, TaskType, TensorSpec
+from nos.common.io import prepare_images
 from nos.common.types import Batch, ImageT, TensorT
+from nos.compilers import compile
+from nos.constants import NOS_MODELS_DIR
 from nos.hub import TorchHubConfig
+from nos.logging import logger
 
 
 @dataclass(frozen=True)
 class YOLOXConfig(TorchHubConfig):
     confidence_threshold: float = 0.3
     """Confidence threshold for object detection."""
     nms_threshold: float = 0.3
@@ -41,15 +46,15 @@
     box_corner = prediction.new(prediction.shape)
     box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2
     box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2
     box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2
     box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2
     prediction[:, :, :4] = box_corner[:, :, :4]
 
-    output = [None for _ in range(len(prediction))]
+    output = [np.empty(shape=(0, 7), dtype=np.float32) for _ in range(len(prediction))]
     for i, image_pred in enumerate(prediction):
 
         # If none are remaining => process next image
         if not image_pred.size(0):
             continue
 
         # Get score and class with highest confidence
@@ -75,38 +80,21 @@
                 detections[:, 4] * detections[:, 5],
                 detections[:, 6],
                 nms_threshold,
             )
 
         # Filter detections based on NMS
         detections = detections[nms_out_index]
-        if output[i] is None:
-            output[i] = detections
-        else:
-            output[i] = torch.cat((output[i], detections))
+        output[i] = detections.cpu().numpy()
     return output
 
 
 class YOLOX:
-    """Various object detection models from Torch Hub.
-
-
-    YOLOX:
-        https://github.com/Megvii-BaseDetection/YOLOX/tree/main#benchmark
-
-        Standard models:
-            YOLOX-S
-            YOLOX-M
-            YOLOX-L
-            YOLOX-X
-
-        Light models:
-            YOLOX-Tiny
-            YOLOX-Nano
-
+    """YOLOX Object Detection
+    https://github.com/Megvii-BaseDetection/YOLOX/tree/main#benchmark
     """
 
     configs = {
         "yolox/small": YOLOXConfig(
             repo="Megvii-BaseDetection/YOLOX",
             model_name="yolox_s",
         ),
@@ -137,47 +125,106 @@
             self.cfg = YOLOX.configs[model_name]
         except KeyError:
             raise ValueError(f"Invalid model_name: {model_name}, available models: {YOLOX.configs.keys()}")
         self.device = "cuda" if torch.cuda.is_available() else "cpu"
         self.model = torch.hub.load(self.cfg.repo, self.cfg.model_name).to(self.device)
         self.model.eval()
 
-    def __call__(self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]) -> np.ndarray:
+    def __call__(
+        self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]
+    ) -> Dict[str, np.ndarray]:
         """Predict bounding boxes for images."""
+        images = prepare_images(images)
         with torch.inference_mode():
-            if isinstance(images, np.ndarray):
-                images = [images]
-            elif isinstance(images, Image.Image):
-                images = [np.asarray(images)]
-            elif isinstance(images, list):
-                if isinstance(images[0], Image.Image):
-                    images = [np.asarray(image) for image in images]
-                elif isinstance(images[0], np.ndarray):
-                    pass
-                else:
-                    raise ValueError(f"Invalid type for images: {type(images[0])}")
-
             images = (
                 torch.stack([F.to_tensor(image) for image in images]) * 255
             )  # yolox expects non-normalized 0-255 tensor
             images = images.to(self.device)
             predictions = self.model(images)
             predictions = postprocess(
                 predictions,
                 conf_threshold=self.cfg.confidence_threshold,
                 nms_threshold=self.cfg.nms_threshold,
                 class_agnostic=self.cfg.class_agnostic,
             )
             return {
-                "bboxes": [p[:, :4].cpu().numpy() for p in predictions],
-                "scores": [(p[:, 4] * p[:, 5]).cpu().numpy() for p in predictions],  # obj_conf * class_conf
-                "labels": [p[:, 6].cpu().numpy().astype(np.int32) for p in predictions],
+                "bboxes": [p[:, :4] for p in predictions],
+                "scores": [(p[:, 4] * p[:, 5]) for p in predictions],  # obj_conf * class_conf
+                "labels": [p[:, 6].astype(np.int32) for p in predictions],
             }
 
 
+class YOLOXTensorRT(YOLOX):
+    """TensorRT accelerated for YOLOX with Torch TensorRT."""
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+        self.verbose = kwargs.get("verbose", False)
+        self._model_dir = Path(NOS_MODELS_DIR, f"cache/{self.cfg.model_name}")
+        self._model_dir.mkdir(parents=True, exist_ok=True)
+        self._patched = False
+        self._patched_shape = None
+
+    @staticmethod
+    def _get_model_id(name: str, shape: torch.Size, dtype: torch.dtype) -> str:
+        """Get model id from model name, shape and dtype."""
+        replacements = {"/": "-", " ": "-"}
+        for k, v in replacements.items():
+            name = name.replace(k, v)
+        shape = list(map(int, shape))
+        shape_str = "x".join([str(s) for s in shape])
+        precision_str = str(dtype).split(".")[-1]
+        return f"{name}_{shape_str}_{precision_str}"
+
+    def __compile__(self, inputs: List[torch.Tensor], precision: torch.dtype = torch.float32) -> torch.nn.Module:
+        """Model compilation flow."""
+        assert isinstance(inputs, list), f"inputs must be a list, got {type(inputs)}"
+        assert len(inputs) == 1, f"inputs must be a list of length 1, got {len(inputs)}"
+        keys = {"input"}
+        args = dict(zip(keys, inputs))
+
+        # Check if we have a cached model
+        slug = "backbone"
+        model_id = YOLOXTensorRT._get_model_id(f"{self.cfg.model_name}--{slug}", inputs[0].shape, precision)
+        filename = f"{self._model_dir}/{model_id}.torchtrt.pt"
+        if Path(filename).exists():
+            logger.debug(f"Found cached {model_id}: {filename}")
+            trt_model = torch.load(filename)
+            self.model.backbone = trt_model
+            return
+
+        # Compile the model backbone
+        try:
+            trt_model = compile(self.model.backbone, args, concrete_args=None, precision=precision, slug=model_id)
+            logger.debug(f"Saving compiled {model_id} model to {filename}")
+            torch.save(trt_model, filename)
+            self.model.backbone = trt_model
+            logger.debug(f"Patched {model_id} model")
+        except Exception as e:
+            logger.error(f"Failed to compile {model_id} model: {e}")
+
+    def __call__(
+        self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]
+    ) -> Dict[str, np.ndarray]:
+        """Predict bounding boxes for images."""
+        images = prepare_images(images)
+        B = len(images)
+        H, W = images[0].shape[:2]
+        if not self._patched:
+            assert H is not None and W is not None, "Must provide image size for first call to __call__"
+            inputs = [torch.rand(B, 3, H, W).to(self.device)]
+            self.__compile__(inputs, precision=torch.float32)
+            self._patched = True  # we set this to patched even if the compilation fails
+            self._patched_shape = (B, H, W)
+        if (B, H, W) != self._patched_shape:
+            raise ValueError(f"Image size changed from {self._patched_shape} to {(B, H, W)}")
+        return super().__call__(images)
+
+
 for model_name in YOLOX.configs:
     hub.register(
         model_name,
         TaskType.OBJECT_DETECTION_2D,
         YOLOX,
         init_args=(model_name,),
         method_name="__call__",
@@ -189,7 +236,24 @@
         },
         outputs={
             "bboxes": Batch[TensorT[np.ndarray, TensorSpec(shape=(None, 4), dtype="float32")]],
             "scores": Batch[TensorT[np.ndarray, TensorSpec(shape=(None), dtype="float32")]],
             "labels": Batch[TensorT[np.ndarray, TensorSpec(shape=(None), dtype="int32")]],
         },
     )
+
+# for model_name in YOLOXTensorRT.configs:
+#     hub.register(
+#         model_name,
+#         TaskType.OBJECT_DETECTION_2D,
+#         YOLOXTensorRT,
+#         init_args=(model_name,),
+#         method_name="__call__",
+#         inputs={
+#             "images": Batch[ImageT[Image.Image, ImageSpec(shape=(480, 640, 3), dtype="uint8")], 1],
+#         },
+#         outputs={
+#             "bboxes": Batch[TensorT[np.ndarray, TensorSpec(shape=(None, 4), dtype="float32")]],
+#             "scores": Batch[TensorT[np.ndarray, TensorSpec(shape=(None), dtype="float32")]],
+#             "labels": Batch[TensorT[np.ndarray, TensorSpec(shape=(None), dtype="int32")]],
+#         },
+#     )
```

## nos/server/__init__.py

```diff
@@ -0,0 +1,388 @@
+00000000: 696d 706f 7274 2064 6f63 6b65 720a 6672  import docker.fr
+00000010: 6f6d 206e 6f73 2e63 6f6e 7374 616e 7473  om nos.constants
+00000020: 2069 6d70 6f72 7420 4445 4641 554c 545f   import DEFAULT_
+00000030: 4752 5043 5f50 4f52 540a 0a0a 6465 6620  GRPC_PORT...def 
+00000040: 696e 6974 280a 2020 2020 7275 6e74 696d  init(.    runtim
+00000050: 653a 2073 7472 203d 2022 6175 746f 222c  e: str = "auto",
+00000060: 0a20 2020 2070 6f72 743a 2069 6e74 203d  .    port: int =
+00000070: 2044 4546 4155 4c54 5f47 5250 435f 504f   DEFAULT_GRPC_PO
+00000080: 5254 2c0a 2020 2020 7574 696c 697a 6174  RT,.    utilizat
+00000090: 696f 6e3a 2066 6c6f 6174 203d 2030 2e38  ion: float = 0.8
+000000a0: 2c0a 2020 2020 7075 6c6c 3a20 626f 6f6c  ,.    pull: bool
+000000b0: 203d 2054 7275 652c 0a29 202d 3e20 646f   = True,.) -> do
+000000c0: 636b 6572 2e6d 6f64 656c 732e 636f 6e74  cker.models.cont
+000000d0: 6169 6e65 7273 2e43 6f6e 7461 696e 6572  ainers.Container
+000000e0: 3a0a 2020 2020 2222 2249 6e69 7469 616c  :.    """Initial
+000000f0: 697a 6520 7468 6520 696e 6665 7265 6e63  ize the inferenc
+00000100: 6520 7365 7276 6572 2e0a 0a20 2020 2041  e server...    A
+00000110: 7267 733a 0a20 2020 2020 2020 2072 756e  rgs:.        run
+00000120: 7469 6d65 2028 7374 722c 206f 7074 696f  time (str, optio
+00000130: 6e61 6c29 3a20 5468 6520 7275 6e74 696d  nal): The runtim
+00000140: 6520 746f 2075 7365 2028 692e 652e 2022  e to use (i.e. "
+00000150: 6370 7522 2c20 2267 7075 2229 2e20 4465  cpu", "gpu"). De
+00000160: 6661 756c 7473 2074 6f20 2261 7574 6f22  faults to "auto"
+00000170: 2e0a 2020 2020 2020 2020 2020 2020 496e  ..            In
+00000180: 2022 6175 746f 2220 6d6f 6465 2c20 7468   "auto" mode, th
+00000190: 6520 7275 6e74 696d 6520 7769 6c6c 2062  e runtime will b
+000001a0: 6520 6175 746f 6d61 7469 6361 6c6c 7920  e automatically 
+000001b0: 6465 7465 6374 6564 2e0a 2020 2020 2020  detected..      
+000001c0: 2020 706f 7274 2028 696e 742c 206f 7074    port (int, opt
+000001d0: 696f 6e61 6c29 3a20 5468 6520 706f 7274  ional): The port
+000001e0: 2074 6f20 7573 6520 666f 7220 7468 6520   to use for the 
+000001f0: 696e 6665 7265 6e63 6520 7365 7276 6572  inference server
+00000200: 2e20 4465 6661 756c 7473 2074 6f20 4445  . Defaults to DE
+00000210: 4641 554c 545f 4752 5043 5f50 4f52 542e  FAULT_GRPC_PORT.
+00000220: 0a20 2020 2020 2020 2075 7469 6c69 7a61  .        utiliza
+00000230: 7469 6f6e 2028 666c 6f61 742c 206f 7074  tion (float, opt
+00000240: 696f 6e61 6c29 3a20 5468 6520 7461 7267  ional): The targ
+00000250: 6574 2063 7075 2f6d 656d 6f72 7920 7574  et cpu/memory ut
+00000260: 696c 697a 6174 696f 6e20 6f66 2069 6e66  ilization of inf
+00000270: 6572 656e 6365 2073 6572 7665 722e 2044  erence server. D
+00000280: 6566 6175 6c74 7320 746f 2030 2e38 2e0a  efaults to 0.8..
+00000290: 2020 2020 2020 2020 7075 6c6c 2028 626f          pull (bo
+000002a0: 6f6c 2c20 6f70 7469 6f6e 616c 293a 2050  ol, optional): P
+000002b0: 756c 6c20 7468 6520 646f 636b 6572 2069  ull the docker i
+000002c0: 6d61 6765 2062 6566 6f72 6520 7374 6172  mage before star
+000002d0: 7469 6e67 2074 6865 2069 6e66 6572 656e  ting the inferen
+000002e0: 6365 2073 6572 7665 722e 2044 6566 6175  ce server. Defau
+000002f0: 6c74 7320 746f 2054 7275 652e 0a20 2020  lts to True..   
+00000300: 2022 2222 0a20 2020 2069 6d70 6f72 7420   """.    import 
+00000310: 6d61 7468 0a0a 2020 2020 696d 706f 7274  math..    import
+00000320: 2070 7375 7469 6c0a 0a20 2020 2066 726f   psutil..    fro
+00000330: 6d20 6e6f 732e 636f 6d6d 6f6e 2e73 7973  m nos.common.sys
+00000340: 7465 6d20 696d 706f 7274 2067 6574 5f73  tem import get_s
+00000350: 7973 7465 6d5f 696e 666f 2c20 6861 735f  ystem_info, has_
+00000360: 6770 750a 2020 2020 6672 6f6d 206e 6f73  gpu.    from nos
+00000370: 2e6c 6f67 6769 6e67 2069 6d70 6f72 7420  .logging import 
+00000380: 6c6f 6767 6572 0a20 2020 2066 726f 6d20  logger.    from 
+00000390: 6e6f 732e 7365 7276 6572 2e72 756e 7469  nos.server.runti
+000003a0: 6d65 2069 6d70 6f72 7420 496e 6665 7265  me import Infere
+000003b0: 6e63 6553 6572 7669 6365 5275 6e74 696d  nceServiceRuntim
+000003c0: 650a 0a20 2020 205f 4d49 4e5f 4e55 4d5f  e..    _MIN_NUM_
+000003d0: 4350 5553 203d 2034 0a20 2020 205f 4d49  CPUS = 4.    _MI
+000003e0: 4e5f 4d45 4d5f 4742 203d 2036 0a20 2020  N_MEM_GB = 6.   
+000003f0: 205f 4d49 4e5f 5348 4d45 4d5f 4742 203d   _MIN_SHMEM_GB =
+00000400: 2034 0a0a 2020 2020 6465 6620 5f63 6865   4..    def _che
+00000410: 636b 5f73 7973 7465 6d5f 7265 7175 6972  ck_system_requir
+00000420: 656d 656e 7473 2829 3a0a 2020 2020 2020  ements():.      
+00000430: 2020 2320 466f 7220 6e6f 772c 2077 6520    # For now, we 
+00000440: 7265 7175 6972 6520 6174 206c 6561 7374  require at least
+00000450: 2034 2070 6879 7369 6361 6c20 4350 5520   4 physical CPU 
+00000460: 636f 7265 7320 616e 6420 3620 4742 206f  cores and 6 GB o
+00000470: 6620 6672 6565 206d 656d 6f72 790a 2020  f free memory.  
+00000480: 2020 2020 2020 7379 7369 6e66 6f20 3d20        sysinfo = 
+00000490: 6765 745f 7379 7374 656d 5f69 6e66 6f28  get_system_info(
+000004a0: 290a 2020 2020 2020 2020 6e75 6d5f 6370  ).        num_cp
+000004b0: 7573 203d 2073 7973 696e 666f 5b22 6370  us = sysinfo["cp
+000004c0: 7522 5d5b 2263 6f72 6573 225d 5b22 7068  u"]["cores"]["ph
+000004d0: 7973 6963 616c 225d 0a20 2020 2020 2020  ysical"].       
+000004e0: 206d 656d 5f67 6220 3d20 7379 7369 6e66   mem_gb = sysinf
+000004f0: 6f5b 226d 656d 6f72 7922 5d5b 2261 7661  o["memory"]["ava
+00000500: 696c 6162 6c65 225d 202f 2031 3032 342a  ilable"] / 1024*
+00000510: 2a33 0a0a 2020 2020 2020 2020 6966 206e  *3..        if n
+00000520: 756d 5f63 7075 7320 3c3d 205f 4d49 4e5f  um_cpus <= _MIN_
+00000530: 4e55 4d5f 4350 5553 3a0a 2020 2020 2020  NUM_CPUS:.      
+00000540: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+00000550: 6545 7272 6f72 280a 2020 2020 2020 2020  eError(.        
+00000560: 2020 2020 2020 2020 6622 496e 7375 6666          f"Insuff
+00000570: 6963 6965 6e74 206e 756d 6265 7220 6f66  icient number of
+00000580: 2070 6879 7369 6361 6c20 4350 5520 636f   physical CPU co
+00000590: 7265 7320 287b 6e75 6d5f 6370 7573 7d20  res ({num_cpus} 
+000005a0: 636f 7265 7329 2c20 6174 206c 6561 7374  cores), at least
+000005b0: 2034 2063 6f72 6573 2072 6571 7569 7265   4 cores require
+000005c0: 642e 220a 2020 2020 2020 2020 2020 2020  d.".            
+000005d0: 290a 2020 2020 2020 2020 6966 206d 656d  ).        if mem
+000005e0: 5f67 6220 3c3d 205f 4d49 4e5f 4d45 4d5f  _gb <= _MIN_MEM_
+000005f0: 4742 3a0a 2020 2020 2020 2020 2020 2020  GB:.            
+00000600: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+00000610: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00000620: 2020 6622 496e 7375 6666 6963 6965 6e74    f"Insufficient
+00000630: 2061 7661 696c 6162 6c65 2073 7973 7465   available syste
+00000640: 6d20 6d65 6d6f 7279 2028 7b6d 656d 5f67  m memory ({mem_g
+00000650: 623a 2e31 7d47 4229 2c20 6174 206c 6561  b:.1}GB), at lea
+00000660: 7374 2036 2047 4220 6f66 2066 7265 6520  st 6 GB of free 
+00000670: 6d65 6d6f 7279 2072 6571 7569 7265 642e  memory required.
+00000680: 220a 2020 2020 2020 2020 2020 2020 290a  ".            ).
+00000690: 0a20 2020 2023 2043 6865 636b 2073 7973  .    # Check sys
+000006a0: 7465 6d20 7265 7175 6972 656d 656e 7473  tem requirements
+000006b0: 0a20 2020 205f 6368 6563 6b5f 7379 7374  .    _check_syst
+000006c0: 656d 5f72 6571 7569 7265 6d65 6e74 7328  em_requirements(
+000006d0: 290a 0a20 2020 2023 2043 6865 636b 2069  )..    # Check i
+000006e0: 6620 696e 6665 7265 6e63 6520 7365 7276  f inference serv
+000006f0: 6572 2069 7320 616c 7265 6164 7920 7275  er is already ru
+00000700: 6e6e 696e 670a 2020 2020 636f 6e74 6169  nning.    contai
+00000710: 6e65 7273 203d 2049 6e66 6572 656e 6365  ners = Inference
+00000720: 5365 7276 6963 6552 756e 7469 6d65 2e6c  ServiceRuntime.l
+00000730: 6973 7428 290a 2020 2020 6966 206c 656e  ist().    if len
+00000740: 2863 6f6e 7461 696e 6572 7329 203e 2030  (containers) > 0
+00000750: 3a0a 2020 2020 2020 2020 6173 7365 7274  :.        assert
+00000760: 2028 0a20 2020 2020 2020 2020 2020 206c   (.            l
+00000770: 656e 2863 6f6e 7461 696e 6572 7329 203d  en(containers) =
+00000780: 3d20 310a 2020 2020 2020 2020 292c 2022  = 1.        ), "
+00000790: 4d75 6c74 6970 6c65 2069 6e66 6572 656e  Multiple inferen
+000007a0: 6365 2073 6572 7665 7273 2072 756e 6e69  ce servers runni
+000007b0: 6e67 2c20 706c 6561 7365 206d 616e 7561  ng, please manua
+000007c0: 6c6c 7920 7374 6f70 2061 6c6c 2063 6f6e  lly stop all con
+000007d0: 7461 696e 6572 7320 6265 666f 7265 2070  tainers before p
+000007e0: 726f 6365 6564 696e 672e 220a 2020 2020  roceeding.".    
+000007f0: 2020 2020 2863 6f6e 7461 696e 6572 2c29      (container,)
+00000800: 203d 2063 6f6e 7461 696e 6572 730a 2020   = containers.  
+00000810: 2020 2020 2020 6c6f 6767 6572 2e77 6172        logger.war
+00000820: 6e69 6e67 2866 2249 6e66 6572 656e 6365  ning(f"Inference
+00000830: 2073 6572 7665 7220 616c 7265 6164 7920   server already 
+00000840: 7275 6e6e 696e 6720 286e 616d 653d 7b63  running (name={c
+00000850: 6f6e 7461 696e 6572 2e6e 616d 657d 2c20  ontainer.name}, 
+00000860: 6964 3d7b 636f 6e74 6169 6e65 722e 6964  id={container.id
+00000870: 5b3a 3132 5d7d 292e 2229 0a20 2020 2020  [:12]}).").     
+00000880: 2020 2072 6574 7572 6e20 636f 6e74 6169     return contai
+00000890: 6e65 720a 0a20 2020 2023 2044 6574 6572  ner..    # Deter
+000008a0: 6d69 6e65 2072 756e 7469 6d65 2066 726f  mine runtime fro
+000008b0: 6d20 7379 7374 656d 0a20 2020 2069 6620  m system.    if 
+000008c0: 7275 6e74 696d 6520 3d3d 2022 6175 746f  runtime == "auto
+000008d0: 223a 0a20 2020 2020 2020 2072 756e 7469  ":.        runti
+000008e0: 6d65 203d 2022 6770 7522 2069 6620 6861  me = "gpu" if ha
+000008f0: 735f 6770 7528 2920 656c 7365 2022 6370  s_gpu() else "cp
+00000900: 7522 0a20 2020 2065 6c73 653a 0a20 2020  u".    else:.   
+00000910: 2020 2020 2069 6620 7275 6e74 696d 6520       if runtime 
+00000920: 6e6f 7420 696e 2049 6e66 6572 656e 6365  not in Inference
+00000930: 5365 7276 6963 6552 756e 7469 6d65 2e63  ServiceRuntime.c
+00000940: 6f6e 6669 6773 3a0a 2020 2020 2020 2020  onfigs:.        
+00000950: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00000960: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
+00000970: 2020 2020 2020 6622 496e 7661 6c69 6420        f"Invalid 
+00000980: 696e 6665 7265 6e63 6520 7365 7276 6963  inference servic
+00000990: 6520 7275 6e74 696d 653a 207b 7275 6e74  e runtime: {runt
+000009a0: 696d 657d 2c20 6176 6169 6c61 626c 653a  ime}, available:
+000009b0: 207b 6c69 7374 2849 6e66 6572 656e 6365   {list(Inference
+000009c0: 5365 7276 6963 6552 756e 7469 6d65 2e63  ServiceRuntime.c
+000009d0: 6f6e 6669 6773 2e6b 6579 7328 2929 7d22  onfigs.keys())}"
+000009e0: 0a20 2020 2020 2020 2020 2020 2029 0a0a  .            )..
+000009f0: 2020 2020 2320 5075 6c6c 2064 6f63 6b65      # Pull docke
+00000a00: 7220 696d 6167 6520 2869 6620 6e65 6365  r image (if nece
+00000a10: 7373 6172 7929 0a20 2020 2069 6620 7075  ssary).    if pu
+00000a20: 6c6c 3a0a 2020 2020 2020 2020 5f70 756c  ll:.        _pul
+00000a30: 6c5f 696d 6167 6528 496e 6665 7265 6e63  l_image(Inferenc
+00000a40: 6553 6572 7669 6365 5275 6e74 696d 652e  eServiceRuntime.
+00000a50: 636f 6e66 6967 735b 7275 6e74 696d 655d  configs[runtime]
+00000a60: 2e69 6d61 6765 290a 0a20 2020 2023 2053  .image)..    # S
+00000a70: 7461 7274 2069 6e66 6572 656e 6365 2073  tart inference s
+00000a80: 6572 7665 720a 2020 2020 7275 6e74 696d  erver.    runtim
+00000a90: 6520 3d20 496e 6665 7265 6e63 6553 6572  e = InferenceSer
+00000aa0: 7669 6365 5275 6e74 696d 6528 7275 6e74  viceRuntime(runt
+00000ab0: 696d 653d 7275 6e74 696d 6529 0a20 2020  ime=runtime).   
+00000ac0: 206c 6f67 6765 722e 696e 666f 2866 2253   logger.info(f"S
+00000ad0: 7461 7274 696e 6720 696e 6665 7265 6e63  tarting inferenc
+00000ae0: 6520 7365 7276 6963 653a 205b 6e61 6d65  e service: [name
+00000af0: 3d7b 7275 6e74 696d 652e 6366 672e 6e61  ={runtime.cfg.na
+00000b00: 6d65 7d2c 2072 756e 7469 6d65 3d7b 7275  me}, runtime={ru
+00000b10: 6e74 696d 657d 5d22 290a 0a20 2020 2023  ntime}]")..    #
+00000b20: 2044 6574 6572 6d69 6e65 206e 756d 6265   Determine numbe
+00000b30: 7220 6f66 2063 7075 732c 2073 7973 7465  r of cpus, syste
+00000b40: 6d20 6d65 6d6f 7279 2062 6566 6f72 6520  m memory before 
+00000b50: 7374 6172 7469 6e67 2063 6f6e 7461 696e  starting contain
+00000b60: 6572 0a20 2020 206e 756d 5f63 7075 7320  er.    num_cpus 
+00000b70: 3d20 6d61 7828 5f4d 494e 5f4e 554d 5f43  = max(_MIN_NUM_C
+00000b80: 5055 532c 2075 7469 6c69 7a61 7469 6f6e  PUS, utilization
+00000b90: 202a 2070 7375 7469 6c2e 6370 755f 636f   * psutil.cpu_co
+00000ba0: 756e 7428 6c6f 6769 6361 6c3d 4661 6c73  unt(logical=Fals
+00000bb0: 6529 290a 2020 2020 6d65 6d5f 6c69 6d69  e)).    mem_limi
+00000bc0: 7420 3d20 6d61 7828 5f4d 494e 5f4d 454d  t = max(_MIN_MEM
+00000bd0: 5f47 422c 2075 7469 6c69 7a61 7469 6f6e  _GB, utilization
+00000be0: 202a 206d 6174 682e 666c 6f6f 7228 7073   * math.floor(ps
+00000bf0: 7574 696c 2e76 6972 7475 616c 5f6d 656d  util.virtual_mem
+00000c00: 6f72 7928 292e 6176 6169 6c61 626c 6520  ory().available 
+00000c10: 2f20 3130 3234 2a2a 3329 290a 2020 2020  / 1024**3)).    
+00000c20: 6c6f 6767 6572 2e64 6562 7567 2866 2253  logger.debug(f"S
+00000c30: 7461 7274 696e 6720 696e 6665 7265 6e63  tarting inferenc
+00000c40: 6520 636f 6e74 6169 6e65 723a 205b 6e75  e container: [nu
+00000c50: 6d5f 6370 7573 3d7b 6e75 6d5f 6370 7573  m_cpus={num_cpus
+00000c60: 7d2c 206d 656d 5f6c 696d 6974 3d7b 6d65  }, mem_limit={me
+00000c70: 6d5f 6c69 6d69 747d 675d 2229 0a0a 2020  m_limit}g]")..  
+00000c80: 2020 2320 5374 6172 7420 636f 6e74 6169    # Start contai
+00000c90: 6e65 720a 2020 2020 636f 6e74 6169 6e65  ner.    containe
+00000ca0: 7220 3d20 7275 6e74 696d 652e 7374 6172  r = runtime.star
+00000cb0: 7428 0a20 2020 2020 2020 206e 616e 6f5f  t(.        nano_
+00000cc0: 6370 7573 3d69 6e74 286e 756d 5f63 7075  cpus=int(num_cpu
+00000cd0: 7320 2a20 3165 3929 2c0a 2020 2020 2020  s * 1e9),.      
+00000ce0: 2020 6d65 6d5f 6c69 6d69 743d 6622 7b6d    mem_limit=f"{m
+00000cf0: 656d 5f6c 696d 6974 7d67 222c 0a20 2020  em_limit}g",.   
+00000d00: 2020 2020 2073 686d 5f73 697a 653d 6622       shm_size=f"
+00000d10: 7b5f 4d49 4e5f 5348 4d45 4d5f 4742 7d67  {_MIN_SHMEM_GB}g
+00000d20: 222c 0a20 2020 2020 2020 2070 6f72 7473  ",.        ports
+00000d30: 3d7b 6622 7b44 4546 4155 4c54 5f47 5250  ={f"{DEFAULT_GRP
+00000d40: 435f 504f 5254 7d2f 7463 7022 3a20 706f  C_PORT}/tcp": po
+00000d50: 7274 7d2c 0a20 2020 2029 0a20 2020 206c  rt},.    ).    l
+00000d60: 6f67 6765 722e 696e 666f 2866 2249 6e66  ogger.info(f"Inf
+00000d70: 6572 656e 6365 2073 6572 7669 6365 2073  erence service s
+00000d80: 7461 7274 6564 3a20 5b6e 616d 653d 7b72  tarted: [name={r
+00000d90: 756e 7469 6d65 2e63 6667 2e6e 616d 657d  untime.cfg.name}
+00000da0: 2c20 7275 6e74 696d 653d 7b72 756e 7469  , runtime={runti
+00000db0: 6d65 7d2c 2069 643d 7b63 6f6e 7461 696e  me}, id={contain
+00000dc0: 6572 2e69 645b 3a31 325d 7d5d 2229 0a20  er.id[:12]}]"). 
+00000dd0: 2020 2072 6574 7572 6e20 636f 6e74 6169     return contai
+00000de0: 6e65 720a 0a0a 6465 6620 7368 7574 646f  ner...def shutdo
+00000df0: 776e 2829 202d 3e20 646f 636b 6572 2e6d  wn() -> docker.m
+00000e00: 6f64 656c 732e 636f 6e74 6169 6e65 7273  odels.containers
+00000e10: 2e43 6f6e 7461 696e 6572 3a0a 2020 2020  .Container:.    
+00000e20: 2222 2253 6875 7464 6f77 6e20 7468 6520  """Shutdown the 
+00000e30: 696e 6665 7265 6e63 6520 7365 7276 6572  inference server
+00000e40: 2e22 2222 0a20 2020 2066 726f 6d20 6e6f  .""".    from no
+00000e50: 732e 6c6f 6767 696e 6720 696d 706f 7274  s.logging import
+00000e60: 206c 6f67 6765 720a 2020 2020 6672 6f6d   logger.    from
+00000e70: 206e 6f73 2e73 6572 7665 722e 7275 6e74   nos.server.runt
+00000e80: 696d 6520 696d 706f 7274 2049 6e66 6572  ime import Infer
+00000e90: 656e 6365 5365 7276 6963 6552 756e 7469  enceServiceRunti
+00000ea0: 6d65 0a0a 2020 2020 2320 4368 6563 6b20  me..    # Check 
+00000eb0: 6966 2069 6e66 6572 656e 6365 2073 6572  if inference ser
+00000ec0: 7665 7220 6973 2061 6c72 6561 6479 2072  ver is already r
+00000ed0: 756e 6e69 6e67 0a20 2020 2063 6f6e 7461  unning.    conta
+00000ee0: 696e 6572 7320 3d20 496e 6665 7265 6e63  iners = Inferenc
+00000ef0: 6553 6572 7669 6365 5275 6e74 696d 652e  eServiceRuntime.
+00000f00: 6c69 7374 2829 0a20 2020 2069 6620 6e6f  list().    if no
+00000f10: 7420 6c65 6e28 636f 6e74 6169 6e65 7273  t len(containers
+00000f20: 293a 0a20 2020 2020 2020 2072 6169 7365  ):.        raise
+00000f30: 2052 756e 7469 6d65 4572 726f 7228 2249   RuntimeError("I
+00000f40: 6e66 6572 656e 6365 2073 6572 7665 7220  nference server 
+00000f50: 6e6f 7420 7275 6e6e 696e 672c 206e 6f74  not running, not
+00000f60: 6869 6e67 2074 6f20 7368 7574 646f 776e  hing to shutdown
+00000f70: 2e22 290a 2020 2020 6966 206c 656e 2863  .").    if len(c
+00000f80: 6f6e 7461 696e 6572 7329 203e 2031 3a0a  ontainers) > 1:.
+00000f90: 2020 2020 2020 2020 7261 6973 6520 5275          raise Ru
+00000fa0: 6e74 696d 6545 7272 6f72 2822 4d75 6c74  ntimeError("Mult
+00000fb0: 6970 6c65 2069 6e66 6572 656e 6365 2073  iple inference s
+00000fc0: 6572 7665 7273 2072 756e 6e69 6e67 2c20  ervers running, 
+00000fd0: 706c 6561 7365 206d 616e 7561 6c6c 7920  please manually 
+00000fe0: 7374 6f70 2061 6c6c 2063 6f6e 7461 696e  stop all contain
+00000ff0: 6572 732e 2229 0a20 2020 2023 2053 6875  ers.").    # Shu
+00001000: 7464 6f77 6e20 696e 6665 7265 6e63 6520  tdown inference 
+00001010: 7365 7276 6572 0a20 2020 2028 636f 6e74  server.    (cont
+00001020: 6169 6e65 722c 2920 3d20 636f 6e74 6169  ainer,) = contai
+00001030: 6e65 7273 0a20 2020 206c 6f67 6765 722e  ners.    logger.
+00001040: 696e 666f 2866 2253 746f 7070 696e 6720  info(f"Stopping 
+00001050: 696e 6665 7265 6e63 6520 7365 7276 6963  inference servic
+00001060: 653a 205b 6e61 6d65 3d7b 636f 6e74 6169  e: [name={contai
+00001070: 6e65 722e 6e61 6d65 7d2c 2069 643d 7b63  ner.name}, id={c
+00001080: 6f6e 7461 696e 6572 2e69 645b 3a31 325d  ontainer.id[:12]
+00001090: 7d5d 2229 0a20 2020 2074 7279 3a0a 2020  }]").    try:.  
+000010a0: 2020 2020 2020 636f 6e74 6169 6e65 722e        container.
+000010b0: 7265 6d6f 7665 2866 6f72 6365 3d54 7275  remove(force=Tru
+000010c0: 6529 0a20 2020 2065 7863 6570 7420 4578  e).    except Ex
+000010d0: 6365 7074 696f 6e20 6173 2065 3a0a 2020  ception as e:.  
+000010e0: 2020 2020 2020 7261 6973 6520 5275 6e74        raise Runt
+000010f0: 696d 6545 7272 6f72 2866 2246 6169 6c65  imeError(f"Faile
+00001100: 6420 746f 2073 6875 7464 6f77 6e20 696e  d to shutdown in
+00001110: 6665 7265 6e63 6520 7365 7276 6572 3a20  ference server: 
+00001120: 7b65 7d22 290a 2020 2020 6c6f 6767 6572  {e}").    logger
+00001130: 2e69 6e66 6f28 6622 496e 6665 7265 6e63  .info(f"Inferenc
+00001140: 6520 7365 7276 6963 6520 7374 6f70 7065  e service stoppe
+00001150: 643a 205b 6e61 6d65 3d7b 636f 6e74 6169  d: [name={contai
+00001160: 6e65 722e 6e61 6d65 7d2c 2069 643d 7b63  ner.name}, id={c
+00001170: 6f6e 7461 696e 6572 2e69 645b 3a31 325d  ontainer.id[:12]
+00001180: 7d5d 2229 0a20 2020 2072 6574 7572 6e20  }]").    return 
+00001190: 636f 6e74 6169 6e65 720a 0a0a 6465 6620  container...def 
+000011a0: 5f70 756c 6c5f 696d 6167 6528 696d 6167  _pull_image(imag
+000011b0: 653a 2073 7472 2c20 7175 6965 743a 2062  e: str, quiet: b
+000011c0: 6f6f 6c20 3d20 4661 6c73 652c 2070 6c61  ool = False, pla
+000011d0: 7466 6f72 6d3a 2073 7472 203d 204e 6f6e  tform: str = Non
+000011e0: 6529 202d 3e20 7374 723a 0a20 2020 2022  e) -> str:.    "
+000011f0: 2222 5075 6c6c 2074 6865 206c 6174 6573  ""Pull the lates
+00001200: 7420 696e 6665 7265 6e63 6520 7365 7276  t inference serv
+00001210: 6572 2069 6d61 6765 2e22 2222 0a20 2020  er image.""".   
+00001220: 2069 6d70 6f72 7420 7375 6270 726f 6365   import subproce
+00001230: 7373 0a20 2020 2066 726f 6d20 636f 6c6c  ss.    from coll
+00001240: 6563 7469 6f6e 7320 696d 706f 7274 2064  ections import d
+00001250: 6571 7565 0a0a 2020 2020 696d 706f 7274  eque..    import
+00001260: 2072 6963 682e 7374 6174 7573 0a0a 2020   rich.status..  
+00001270: 2020 696d 706f 7274 2064 6f63 6b65 722e    import docker.
+00001280: 6572 726f 7273 0a20 2020 2066 726f 6d20  errors.    from 
+00001290: 6e6f 732e 6c6f 6767 696e 6720 696d 706f  nos.logging impo
+000012a0: 7274 206c 6f67 6765 720a 2020 2020 6672  rt logger.    fr
+000012b0: 6f6d 206e 6f73 2e73 6572 7665 722e 7275  om nos.server.ru
+000012c0: 6e74 696d 6520 696d 706f 7274 2044 6f63  ntime import Doc
+000012d0: 6b65 7252 756e 7469 6d65 0a0a 2020 2020  kerRuntime..    
+000012e0: 7472 793a 0a20 2020 2020 2020 2044 6f63  try:.        Doc
+000012f0: 6b65 7252 756e 7469 6d65 2e67 6574 2829  kerRuntime.get()
+00001300: 2e5f 636c 6965 6e74 2e69 6d61 6765 732e  ._client.images.
+00001310: 6765 7428 696d 6167 6529 0a20 2020 2020  get(image).     
+00001320: 2020 206c 6f67 6765 722e 696e 666f 2866     logger.info(f
+00001330: 2246 6f75 6e64 2075 702d 746f 2d64 6174  "Found up-to-dat
+00001340: 6520 7365 7276 6572 2069 6d61 6765 3a20  e server image: 
+00001350: 7b69 6d61 6765 7d22 290a 2020 2020 6578  {image}").    ex
+00001360: 6365 7074 2064 6f63 6b65 722e 6572 726f  cept docker.erro
+00001370: 7273 2e49 6d61 6765 4e6f 7446 6f75 6e64  rs.ImageNotFound
+00001380: 3a0a 2020 2020 2020 2020 7472 793a 0a20  :.        try:. 
+00001390: 2020 2020 2020 2020 2020 206c 6f67 6765             logge
+000013a0: 722e 696e 666f 2866 2250 756c 6c69 6e67  r.info(f"Pulling
+000013b0: 206e 6577 2073 6572 7665 7220 696d 6167   new server imag
+000013c0: 653a 207b 696d 6167 657d 2028 7468 6973  e: {image} (this
+000013d0: 206d 6179 2074 616b 6520 6120 7768 696c   may take a whil
+000013e0: 6529 2e22 290a 2020 2020 2020 2020 2020  e).").          
+000013f0: 2020 2320 7573 6520 7375 6270 726f 6365    # use subproce
+00001400: 7373 2074 6f20 7075 6c6c 2069 6d61 6765  ss to pull image
+00001410: 2061 6e64 2073 7472 6561 6d20 6f75 7470   and stream outp
+00001420: 7574 0a20 2020 2020 2020 2020 2020 2070  ut.            p
+00001430: 726f 6320 3d20 7375 6270 726f 6365 7373  roc = subprocess
+00001440: 2e50 6f70 656e 280a 2020 2020 2020 2020  .Popen(.        
+00001450: 2020 2020 2020 2020 6622 646f 636b 6572          f"docker
+00001460: 2070 756c 6c20 7b69 6d61 6765 7d22 2c20   pull {image}", 
+00001470: 7374 646f 7574 3d73 7562 7072 6f63 6573  stdout=subproces
+00001480: 732e 5049 5045 2c20 7374 6465 7272 3d73  s.PIPE, stderr=s
+00001490: 7562 7072 6f63 6573 732e 5354 444f 5554  ubprocess.STDOUT
+000014a0: 2c20 7368 656c 6c3d 5472 7565 0a20 2020  , shell=True.   
+000014b0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+000014c0: 2020 2020 2020 2073 7461 7475 735f 7120         status_q 
+000014d0: 3d20 6465 7175 6528 6d61 786c 656e 3d32  = deque(maxlen=2
+000014e0: 3529 0a20 2020 2020 2020 2020 2020 2073  5).            s
+000014f0: 7461 7475 735f 7374 7220 3d20 2250 756c  tatus_str = "Pul
+00001500: 6c69 6e67 206e 6577 2073 6572 7665 7220  ling new server 
+00001510: 696d 6167 653a 207b 696d 6167 657d 2028  image: {image} (
+00001520: 7468 6973 206d 6179 2074 616b 6520 6120  this may take a 
+00001530: 7768 696c 6529 2e22 0a20 2020 2020 2020  while).".       
+00001540: 2020 2020 2077 6974 6820 7269 6368 2e73       with rich.s
+00001550: 7461 7475 732e 5374 6174 7573 2873 7461  tatus.Status(sta
+00001560: 7475 735f 7374 7229 2061 7320 7374 6174  tus_str) as stat
+00001570: 7573 3a0a 2020 2020 2020 2020 2020 2020  us:.            
+00001580: 2020 2020 7768 696c 6520 7072 6f63 2e73      while proc.s
+00001590: 7464 6f75 742e 7265 6164 6162 6c65 2829  tdout.readable()
+000015a0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+000015b0: 2020 2020 2020 6c69 6e65 203d 2070 726f        line = pro
+000015c0: 632e 7374 646f 7574 2e72 6561 646c 696e  c.stdout.readlin
+000015d0: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
+000015e0: 2020 2020 2020 2020 6966 206e 6f74 206c          if not l
+000015f0: 696e 653a 0a20 2020 2020 2020 2020 2020  ine:.           
+00001600: 2020 2020 2020 2020 2020 2020 2062 7265               bre
+00001610: 616b 0a20 2020 2020 2020 2020 2020 2020  ak.             
+00001620: 2020 2020 2020 2073 7461 7475 735f 712e         status_q.
+00001630: 6170 7065 6e64 286c 696e 652e 6465 636f  append(line.deco
+00001640: 6465 2822 7574 662d 3822 292e 7374 7269  de("utf-8").stri
+00001650: 7028 2929 0a20 2020 2020 2020 2020 2020  p()).           
+00001660: 2020 2020 2020 2020 2073 7461 7475 732e           status.
+00001670: 7570 6461 7465 2822 5b62 6f6c 6420 7965  update("[bold ye
+00001680: 6c6c 6f77 5d22 202b 2066 227b 7374 6174  llow]" + f"{stat
+00001690: 7573 5f73 7472 7d5c 6e5c 7422 202b 2022  us_str}\n\t" + "
+000016a0: 5c6e 5c74 222e 6a6f 696e 2873 7461 7475  \n\t".join(statu
+000016b0: 735f 7129 202b 2022 5b2f 626f 6c64 2079  s_q) + "[/bold y
+000016c0: 656c 6c6f 775d 2229 0a20 2020 2020 2020  ellow]").       
+000016d0: 2020 2020 2070 726f 632e 7761 6974 2829       proc.wait()
+000016e0: 0a20 2020 2020 2020 2020 2020 206c 6f67  .            log
+000016f0: 6765 722e 696e 666f 2866 2250 756c 6c65  ger.info(f"Pulle
+00001700: 6420 6e65 7720 7365 7276 6572 2069 6d61  d new server ima
+00001710: 6765 3a20 7b69 6d61 6765 7d22 290a 2020  ge: {image}").  
+00001720: 2020 2020 2020 6578 6365 7074 2028 646f        except (do
+00001730: 636b 6572 2e65 7272 6f72 732e 4150 4945  cker.errors.APIE
+00001740: 7272 6f72 2c20 646f 636b 6572 2e65 7272  rror, docker.err
+00001750: 6f72 732e 446f 636b 6572 4578 6365 7074  ors.DockerExcept
+00001760: 696f 6e29 2061 7320 6578 633a 0a20 2020  ion) as exc:.   
+00001770: 2020 2020 2020 2020 206c 6f67 6765 722e           logger.
+00001780: 6572 726f 7228 6622 4661 696c 6564 2074  error(f"Failed t
+00001790: 6f20 7075 6c6c 2069 6d61 6765 3a20 7b69  o pull image: {i
+000017a0: 6d61 6765 7d2c 2065 7869 7469 6e67 2065  mage}, exiting e
+000017b0: 6172 6c79 3a20 7b65 7863 7d22 290a 2020  arly: {exc}").  
+000017c0: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+000017d0: 4578 6365 7074 696f 6e28 6622 4661 696c  Exception(f"Fail
+000017e0: 6564 2074 6f20 7075 6c6c 2069 6d61 6765  ed to pull image
+000017f0: 3a20 7b69 6d61 6765 7d2c 2070 6c65 6173  : {image}, pleas
+00001800: 6520 6d6e 6175 6c6c 7920 7075 6c6c 2069  e mnaully pull i
+00001810: 6d61 6765 2076 6961 2060 646f 636b 6572  mage via `docker
+00001820: 2070 756c 6c20 7b69 6d61 6765 7d60 2229   pull {image}`")
+00001830: 0a                                       .
```

## nos/server/docker.py

```diff
@@ -58,14 +58,19 @@
     @classmethod
     def get(cls: "DockerRuntime") -> "DockerRuntime":
         """Get DockerRuntime instance."""
         if cls._instance is None:
             cls._instance = cls()
         return cls._instance
 
+    @classmethod
+    def list(cls, **kwargs) -> Iterable[docker.models.containers.Container]:
+        """List docker containers."""
+        return cls.get()._client.containers.list(**kwargs)
+
     def start(
         self,
         image: str,
         command: Optional[Union[str, List[str]]] = None,
         name: str = None,
         **kwargs: Any,
     ) -> docker.models.containers.Container:
@@ -134,34 +139,34 @@
         try:
             container = self.get_container(name)
             if container is None:
                 logger.debug(f"Container not running: {name}, exiting early.")
                 return
             logger.debug(f"Removing container: {name}")
             container.remove(force=True)
-            logger.info(f"Removed container: {name}")
+            logger.debug(f"Removed container: {name}")
         except (docker.errors.APIError, docker.errors.DockerException) as exc:
             logger.error(f"Failed to stop container: {exc}")
         return container
 
-    def get_container(self, name: str) -> docker.models.containers.Container:
-        """Get container by name."""
-        try:
-            return self._client.containers.get(name)
-        except docker.errors.NotFound:
-            return None
-
     def get_container_id(self, name: str) -> Optional[str]:
         """Get the runtime container ID."""
         container = self.get_container(name)
         return container.id if container else None
 
-    def get_container_status(self, name: str) -> Optional[str]:
-        """Get container status by name."""
-        container = self.get_container(name)
+    def get_container(self, id_or_name: str) -> docker.models.containers.Container:
+        """Get container by id or name."""
+        try:
+            return self._client.containers.get(id_or_name)
+        except docker.errors.NotFound:
+            return None
+
+    def get_container_status(self, id_or_name: str) -> Optional[str]:
+        """Get container status by id or name."""
+        container = self.get_container(id_or_name)
         return container.status if container else None
 
     def get_container_logs(self, name: str, **kwargs) -> Iterable[str]:
         """Get container logs."""
         try:
             container = self.get_container(name)
             if container is None:
```

## nos/server/runtime.py

```diff
@@ -5,22 +5,24 @@
 from typing import Any, Dict, Iterable, List, Optional, Union
 
 import docker
 from nos.constants import DEFAULT_GRPC_PORT  # noqa F401
 from nos.logging import LOGGING_LEVEL, logger
 from nos.protoc import import_module
 from nos.server.docker import DockerRuntime
+from nos.version import __version__
 
 
 nos_service_pb2 = import_module("nos_service_pb2")
 nos_service_pb2_grpc = import_module("nos_service_pb2_grpc")
 
 
-NOS_DOCKER_IMAGE_CPU = "autonomi/nos:latest-cpu"
-NOS_DOCKER_IMAGE_GPU = "autonomi/nos:latest-gpu"
+NOS_DOCKER_IMAGE_CPU = f"autonomi/nos:{__version__}-cpu"
+NOS_DOCKER_IMAGE_GPU = f"autonomi/nos:{__version__}-gpu"
+NOS_DOCKER_IMAGE_TRT_RUNTIME = f"autonomi/nos:{__version__}-trt-runtime"
 
 NOS_INFERENCE_SERVICE_CONTAINER_NAME = "nos-inference-service"
 NOS_INFERENCE_SERVICE_CMD = "nos-grpc-server"
 
 
 @dataclass
 class InferenceServiceRuntimeConfig:
@@ -53,15 +55,20 @@
 
     detach: bool = True
     """Whether to run the container in detached mode."""
 
     gpu: bool = False
     """Whether to start the container with GPU support."""
 
-    kwargs: Dict[str, Any] = field(default_factory=dict)
+    kwargs: Dict[str, Any] = field(
+        default_factory=lambda: {
+            "nano_cpus": int(6e9),
+            "mem_limit": "6g",
+        }
+    )
     """Additional keyword-arguments to pass to `DockerRuntime.start`."""
 
 
 class InferenceServiceRuntime:
     """Inference service runtime.
 
     This class is responsible for handling the lifecycle of the
@@ -72,48 +79,68 @@
     """
 
     configs = {
         "cpu": InferenceServiceRuntimeConfig(
             image=NOS_DOCKER_IMAGE_CPU,
             name=f"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-cpu",
             gpu=False,
+            kwargs={
+                "nano_cpus": int(6e9),
+                "mem_limit": "6g",
+            },
         ),
         "gpu": InferenceServiceRuntimeConfig(
             image=NOS_DOCKER_IMAGE_GPU,
             name=f"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-gpu",
             gpu=True,
+            kwargs={
+                "nano_cpus": int(8e9),
+                "mem_limit": "12g",
+            },
         ),
-        "mmdet-dev": InferenceServiceRuntimeConfig(
-            image="autonomi/nos:latest-mmdet-dev",
-            name=f"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-mmdet",
+        "trt-runtime": InferenceServiceRuntimeConfig(
+            image="autonomi/nos:latest-trt-runtime",
+            name=f"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-trt-runtime",
             gpu=True,
             environment={
                 "NOS_LOGGING_LEVEL": LOGGING_LEVEL,
-                "NOS_ENV": "mmdet-dev",
+            },
+            kwargs={
+                "nano_cpus": int(8e9),
+                "mem_limit": "12g",
             },
         ),
     }
 
-    def __init__(self, runtime: str = "cpu", name: str = NOS_INFERENCE_SERVICE_CONTAINER_NAME):
+    def __init__(self, runtime: str = "cpu", name: str = None):
         """Initialize the inference runtime.
 
         Args:
             runtime (str, optional): Inference runtime. Defaults to "cpu".
             name (str, optional): Inference runtime name. Defaults to "nos-inference-service".
         """
         if runtime not in self.configs:
             raise ValueError(f"Invalid inference runtime: {runtime}, available: {list(self.configs.keys())}")
         self.cfg = copy.deepcopy(self.configs[runtime])
-        self.cfg.name = name
+        if name is not None:
+            self.cfg.name = name
 
         self._runtime = DockerRuntime.get()
 
     def __repr__(self) -> str:
         return f"InferenceServiceRuntime(image={self.cfg.image}, name={self.cfg.name}, gpu={self.cfg.gpu})"
 
+    @classmethod
+    def list(self, **kwargs) -> List[docker.models.containers.Container]:
+        """List running docker containers."""
+        containers = DockerRuntime.get().list(**kwargs)
+        return [
+            container for container in containers if container.name.startswith(NOS_INFERENCE_SERVICE_CONTAINER_NAME)
+        ]
+
     def start(self, **kwargs) -> docker.models.containers.Container:
         """Start the inference runtime.
 
         Args:
             **kwargs: Additional keyword-arguments to pass to `DockerRuntime.start`.
         """
         logger.info(f"Starting inference runtime with image: {self.cfg.image}")
```

## nos/server/service.py

```diff
@@ -1,209 +1,30 @@
-import os
-from collections import OrderedDict
-from dataclasses import dataclass, field
-from enum import Enum
-from typing import Any, Dict, List, Union
+import traceback
+from typing import Any, Dict
 
 import grpc
-import ray
 import rich.console
 import rich.status
-import torch
 from google.protobuf import empty_pb2
 
 from nos import hub
 from nos.common import ModelSpec, TaskType, dumps
 from nos.constants import DEFAULT_GRPC_PORT  # noqa F401
 from nos.exceptions import ModelNotFoundError
 from nos.executors.ray import RayExecutor
 from nos.logging import logger
+from nos.managers import ModelHandle, ModelManager
 from nos.protoc import import_module
 from nos.version import __version__
 
 
 nos_service_pb2 = import_module("nos_service_pb2")
 nos_service_pb2_grpc = import_module("nos_service_pb2_grpc")
 
 
-@dataclass
-class ModelHandle:
-    """Model handles for serving models.
-
-    Usage:
-        ```python
-        # Initialize a model handle
-        >> model_handle = ModelHandle(spec, num_replicas=1)
-
-        # Submit a task to the model handle
-        >> handler = model_handle.handle
-        >> response_ref = handler.submit(**model_inputs)
-
-        # Kill all actors
-        >> model_handle.kill()
-        ```
-    """
-
-    spec: ModelSpec
-    """Model specification."""
-    num_replicas: Union[int, str] = field(default=1)
-    """Number of replicas."""
-
-    _actors: List[Union[ray.remote, ray.actor.ActorHandle]] = field(init=False, default=None)
-    """Ray actor handle."""
-
-    def __post_init__(self):
-        """Initialize the actor handles."""
-        if self.num_replicas > 1:
-            raise NotImplementedError("Multiple replicas not yet supported.")
-        self._actors = [self.actor_from_spec(self.spec) for _ in range(self.num_replicas)]
-
-    @staticmethod
-    def actor_from_spec(spec: ModelSpec) -> Union[ray.remote, ray.actor.ActorHandle]:
-        """Get an actor handle from model specification."""
-        # TODO (spillai): Use the auto-tuned model spec to instantiate an
-        # actor the desired memory requirements. Fractional GPU amounts
-        # will need to be calculated from the target HW and model spec
-        # (i.e. 0.5 on A100 vs. T4 are different).
-        model_cls = spec.signature.func_or_cls
-        actor_options = {"num_gpus": 0.5 if torch.cuda.is_available() else 0}
-        logger.debug(f"Creating actor: {actor_options}, {model_cls}")
-        actor_cls = ray.remote(**actor_options)(model_cls)
-        return actor_cls.remote(*spec.signature.init_args, **spec.signature.init_kwargs)
-
-    def kill(self) -> None:
-        """Kill the actor handle."""
-        for actor_handle in self._actors:
-            ray.kill(actor_handle)
-
-    def remote(self, *args, **kwargs) -> ray.ObjectRef:
-        """Submit a task to the actor handle or pool."""
-        # Get the method function (i.e. `__call__`, or `predict`)
-        try:
-            actor_method_func = getattr(self.actor_handle, self.spec.signature.method_name)
-        except AttributeError as exc:
-            err = f"Failed to get method function: method={self.spec.signature.method_name}"
-            logger.error(f"{err}, exc={exc}")
-            raise Exception(err)
-
-        # Call the method function
-        response_ref: ray.ObjectRef = actor_method_func.remote(**kwargs)
-        return ray.get(response_ref)
-
-    @property
-    def actor_handle(self) -> Union[ray.remote, ray.actor.ActorHandle]:
-        """Get the actor handle."""
-        assert len(self._actors) == 1, "Only one actor handle is supported."
-        return self._actors[0]
-
-
-@dataclass(frozen=True)
-class ModelManager:
-    """Model manager for serving models with ray actors.
-
-    Features:
-      * Concurrency: Support fixed number of concurrent models,
-        running simultaneously with FIFO model eviction policies.
-      * Parallelism: Support multiple replicas of the same model.
-      * Optimal memory management: Model memory consumption
-        are automatically inferred from the model specification
-        and used to optimally bin-pack models on the GPU.
-      * Automatic garbage collection: Models are automatically
-        garbage collected when they are evicted from the manager.
-        Scaling models with the model manager should not result in OOM.
-
-    """
-
-    class EvictionPolicy(str, Enum):
-        FIFO = "FIFO"
-        LRU = "LRU"
-
-    policy: EvictionPolicy = EvictionPolicy.FIFO
-    """Eviction policy."""
-
-    max_concurrent_models: int = os.getenv("NOS_MAX_CONCURRENT_MODELS", 4)
-    """Maximum number of concurrent models."""
-
-    handlers: Dict[str, ModelHandle] = field(default_factory=OrderedDict)
-    """Model handles."""
-
-    def __post_init__(self):
-        if self.policy not in (self.EvictionPolicy.FIFO,):
-            raise NotImplementedError(f"Eviction policy not implemented: {self.policy}")
-
-    def __repr__(self) -> str:
-        """String representation of the model manager (memory consumption, models, in tabular format etc)."""
-        repr_str = f"ModelManager(policy={self.policy}, len(handlers)={len(self.handlers)})"
-        for idx, (model_id, model_handle) in enumerate(self.handlers.items()):
-            repr_str += f"\n\t{idx}: {model_id}, {model_handle}"
-        return repr_str
-
-    def get(self, model_spec: ModelSpec) -> ModelHandle:
-        """Get a model handle from the manager.
-
-        Create a new model handle if it does not exist,
-        else return an existing handle.
-
-        Args:
-            model_spec (ModelSpec): Model specification.
-        Returns:
-            ModelHandle: Model handle.
-        """
-        model_id: str = model_spec.id
-        if model_id not in self.handlers:
-            return self.add(model_spec)
-        else:
-            return self.handlers[model_id]
-
-    @logger.catch
-    def add(self, spec: ModelSpec) -> ModelHandle:
-        """Add a model to the manager.
-
-        Args:
-            spec (ModelSpec): Model specification.
-        Returns:
-            ModelHandle: Model handle.
-        """
-        # If the model already exists, raise an error
-        model_id = spec.id
-        if model_id in self.handlers:
-            raise ValueError(f"Model already exists: {model_id}")
-
-        # If the model handle is full, pop the oldest model
-        if len(self.handlers) >= self.max_concurrent_models:
-            _handle: ModelHandle = self.evict()
-            logger.info(f"Deleting oldest model: {_handle.spec.name}")
-
-        # Create the serve deployment from the model handle
-        logger.info(f"Initializing model with spec: {spec.name}")
-
-        # Note: Currently one model per (model-name, task) is supported.
-        self.handlers[model_id] = ModelHandle(spec)
-        logger.info(f"Created actor: {self.handlers[model_id]}, type={type(self.handlers[model_id])}")
-        logger.info(f"Models ({len(self.handlers)}): {self.handlers.keys()}")
-
-        return self.handlers[model_id]
-
-    @logger.catch
-    def evict(self) -> ModelHandle:
-        """Evict a model from the manager (FIFO, LRU etc)."""
-        # Pop the oldest model
-        # TODO (spillai): Implement LRU policy
-        assert len(self.handlers) > 0, "No models to evict."
-        _, handle = self.handlers.popitem(last=False)
-        model_id = handle.spec.id
-        logger.info(f"Deleting model: {model_id}")
-
-        # Explicitly kill the model handle (including all actors)
-        handle.kill()
-        logger.info(f"Deleted model: {model_id}")
-        assert model_id not in self.handlers, f"Model should have been evicted: {model_id}"
-        return handle
-
-
 class InferenceService:
     """Ray-executor based inference service.
 
     Parameters:
         model_manager (ModelManager): Model manager.
         executor (RayExecutor): Ray executor.
 
@@ -214,43 +35,51 @@
         self.model_manager = ModelManager()
         self.executor = RayExecutor.get()
         try:
             self.executor.init()
         except Exception as e:
             logger.info(f"Failed to initialize executor: {e}")
             raise RuntimeError(f"Failed to initialize executor: {e}")
+        self.current_model_id = None
+        self.current_model_spec = None
 
-    @logger.catch
     def execute(self, model_name: str, task: TaskType = None, inputs: Dict[str, Any] = None) -> Dict[str, Any]:
         """Execute the model.
 
         Args:
             model_name (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).
             task (TaskType): Task type (e.g. `TaskType.OBJECT_DETECTION_2D`).
             inputs (Dict[str, Any]): Model inputs.
         Returns:
             Dict[str, Any]: Model outputs.
         """
-        # Load the model spec
-        try:
-            model_spec: ModelSpec = hub.load_spec(model_name, task=task)
-            logger.debug(f"Loaded model spec: {model_spec}")
-        except Exception as e:
-            raise ModelNotFoundError(f"Failed to load model spec: {model_name}, {e}")
+        # Load model spec if the model has changed
+        if self.current_model_id is None or self.current_model_id != (model_name, task.value):
+            # Load the model spec
+            try:
+                model_spec: ModelSpec = hub.load_spec(model_name, task=task)
+                logger.debug(f"Loaded model spec: {model_spec}")
+            except Exception as e:
+                raise ModelNotFoundError(f"Failed to load model spec: {model_name}, {e}")
+            self.current_model_id = (model_name, task.value)
+            self.current_model_spec = model_spec
+
+        assert self.current_model_spec is not None
+        model_spec = self.current_model_spec
 
         # TODO (spillai): Validate/Decode the inputs
         model_inputs = model_spec.signature._decode_inputs(inputs)
 
         # Initialize the model (if not already initialized)
         # This call should also evict models and garbage collect if
         # too many models are loaded are loaded simultaneously.
         model_handle: ModelHandle = self.model_manager.get(model_spec)
 
         # Get the model handle and call it remotely (with model spec, actor handle)
-        response = model_handle.remote(**model_inputs)
+        response: Dict[str, Any] = model_handle.remote(**model_inputs)
 
         # If the response is a single value, wrap it in a dict with the appropriate key
         if len(model_spec.signature.outputs) == 1:
             response = {k: response for k in model_spec.signature.outputs}
 
         return response
 
@@ -264,69 +93,67 @@
     Refer to the bring-your-own-schema section:
     https://docs.ray.io/en/master/serve/direct-ingress.html?highlight=grpc#bring-your-own-schema
     """
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
-    @logger.catch
     def Ping(self, request: empty_pb2.Empty, context: grpc.ServicerContext) -> nos_service_pb2.PingResponse:
         """Health check."""
         return nos_service_pb2.PingResponse(status="ok")
 
-    @logger.catch
     def GetServiceInfo(
         self, request: empty_pb2.Empty, context: grpc.ServicerContext
     ) -> nos_service_pb2.ServiceInfoResponse:
         """Get information on the service."""
         return nos_service_pb2.ServiceInfoResponse(version=__version__)
 
-    @logger.catch
     def ListModels(self, request: empty_pb2.Empty, context: grpc.ServicerContext) -> nos_service_pb2.ModelListResponse:
         """List all models."""
         response = nos_service_pb2.ModelListResponse()
         for spec in hub.list():
             response.models.append(nos_service_pb2.ModelInfo(name=spec.name, task=spec.task.value))
         return response
 
-    @logger.catch
     def GetModelInfo(
         self, request: nos_service_pb2.ModelInfoRequest, context: grpc.ServicerContext
     ) -> nos_service_pb2.ModelInfoResponse:
         """Get model information."""
         try:
             model_info = request.request
             spec: ModelSpec = hub.load_spec(model_info.name, task=TaskType(model_info.task))
         except KeyError as e:
-            context.abort(context, grpc.StatusCode.NOT_FOUND, str(e))
+            logger.error(f"Failed to load spec: [request={request.request}, e={e}]")
+            context.abort(grpc.StatusCode.NOT_FOUND, str(e))
         return spec._to_proto(public=True)
 
-    @logger.catch
     def Run(
         self, request: nos_service_pb2.InferenceRequest, context: grpc.ServicerContext
     ) -> nos_service_pb2.InferenceResponse:
         """Main model prediction interface."""
         model_request = request.model
         logger.debug(f"Received request: {model_request.task}, {model_request.name}")
         if model_request.task not in (
             TaskType.IMAGE_GENERATION.value,
             TaskType.IMAGE_EMBEDDING.value,
             TaskType.TEXT_EMBEDDING.value,
             TaskType.OBJECT_DETECTION_2D.value,
             TaskType.IMAGE_SEGMENTATION_2D.value,
         ):
-            context.abort(context, grpc.StatusCode.NOT_FOUND, f"Invalid task {model_request.task}")
+            context.abort(grpc.StatusCode.NOT_FOUND, f"Invalid task {model_request.task}")
 
         try:
             logger.debug(f"Executing request: {model_request.task}, {model_request.name}")
             response = self.execute(model_request.name, task=TaskType(model_request.task), inputs=request.inputs)
             return nos_service_pb2.InferenceResponse(response_bytes=dumps(response))
-        except Exception as exc:
-            logger.error(f"Failed to execute request: {model_request.task}, {model_request.name}, {request.inputs}")
-            context.abort(context, grpc.StatusCode.INTERNAL, str(exc))
+        except (grpc.RpcError, Exception) as e:
+            msg = f"Failed to execute request: {model_request.task}, {model_request.name}"
+            msg += f"{traceback.format_exc()}"
+            logger.error(f"{msg}, e={e}")
+            context.abort(grpc.StatusCode.INTERNAL, "Internal Server Error")
 
 
 def serve(address: str = f"[::]:{DEFAULT_GRPC_PORT}", max_workers: int = 1) -> None:
     """Start the gRPC server."""
     from concurrent import futures
 
     options = [
```

## nos/test/conftest.py

```diff
@@ -26,15 +26,15 @@
 
     yield executor
 
     executor.stop()
 
 
 @pytest.fixture(scope="session")
-def grpc_server():
+def grpc_server(ray_executor):
     """Test gRPC server (Port: 50052)."""
     from loguru import logger
 
     from nos.server.service import InferenceServiceImpl
 
     logger.info(f"Starting gRPC test server on port: {GRPC_TEST_PORT}")
     options = [
@@ -97,15 +97,15 @@
     )
     assert container is not None
     assert container.id is not None
     status = runtime.get_container_status()
     assert status is not None and status == "running"
 
     # Yield the running container
-    yield container
+    yield runtime
 
     # Tear down
     try:
         runtime.stop()
     except Exception:
         logger.info(f"Failed to stop existing container with name: {CPU_CONTAINER_NAME}")
 
@@ -133,14 +133,14 @@
     )
     assert container is not None
     assert container.id is not None
     status = runtime.get_container_status()
     assert status is not None and status == "running"
 
     # Yield the running container
-    yield container
+    yield runtime
 
     # Tear down
     try:
         runtime.stop()
     except Exception:
         logger.info(f"Failed to stop existing container with name: {GPU_CONTAINER_NAME}")
```

## nos/test/utils.py

```diff
@@ -14,15 +14,16 @@
 class PyTestGroup(Enum):
     """pytest group for grouping model tests, benchmarks etc."""
 
     UNIT = "unit"
     INTEGRATION = "integration"
     HUB = "hub"
     BENCHMARK = "benchmark"
-    BENCHMARK_MODELS = "benchmark-models"
+    MODEL_BENCHMARK = "model-benchmark"
+    MODEL_COMPILATION = "model-compilation"
 
 
 def skip_if_no_torch_cuda(test_case):
     """Decorator marking a test that requires torch cuda devices.
 
     These tests are skipped when torch.cuda.is_available() is set to False. If
     `CUDA_VISIBLE_DEVICES=""` then, the decorated test is not run.
@@ -61,7 +62,26 @@
         pytestmark = skip_all_unless_nos_env("my-nos-env")
         ```
     """
     import os
 
     env = os.environ.get("NOS_ENV", os.getenv("CONDA_DEFAULT_ENV", "base_gpu"))
     return pytest.mark.skipif(env != nos_env, reason=f"Requires nos env {nos_env}, but using {env}")
+
+
+def get_benchmark_video() -> str:
+    """Download a benchmark video from URL to local file."""
+    import requests
+
+    from nos.constants import NOS_CACHE_DIR
+
+    URL = "https://zackakil.github.io/video-intelligence-api-visualiser/assets/test_video.mp4"
+
+    # Download video from URL to local file
+    tmp_videos_dir = NOS_CACHE_DIR / "test_data" / "videos"
+    tmp_videos_dir.mkdir(parents=True, exist_ok=True)
+    tmp_video_filename = tmp_videos_dir / "test_video.mp4"
+    if not tmp_video_filename.exists():
+        with open(str(tmp_video_filename), "wb") as f:
+            f.write(requests.get(URL).content)
+        assert tmp_video_filename.exists()
+    return str(tmp_video_filename)
```

## Comparing `autonomi_nos-0.0.5.dist-info/LICENSE` & `autonomi_nos-0.0.6a1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `autonomi_nos-0.0.5.dist-info/METADATA` & `autonomi_nos-0.0.6a1.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,394 +1,426 @@
 00000000: 4d65 7461 6461 7461 2d56 6572 7369 6f6e  Metadata-Version
 00000010: 3a20 322e 310a 4e61 6d65 3a20 6175 746f  : 2.1.Name: auto
 00000020: 6e6f 6d69 2d6e 6f73 0a56 6572 7369 6f6e  nomi-nos.Version
-00000030: 3a20 302e 302e 350a 5375 6d6d 6172 793a  : 0.0.5.Summary:
-00000040: 204e 6974 726f 7573 206f 7869 6465 2073   Nitrous oxide s
-00000050: 7973 7465 6d20 284e 4f53 2920 666f 7220  ystem (NOS) for 
-00000060: 636f 6d70 7574 6572 2d76 6973 696f 6e2e  computer-vision.
-00000070: 0a4c 6963 656e 7365 3a20 4d49 5420 4c69  .License: MIT Li
-00000080: 6365 6e73 650a 2020 2020 2020 2020 0a20  cense.        . 
-00000090: 2020 2020 2020 2043 6f70 7972 6967 6874         Copyright
-000000a0: 2028 6329 2032 3032 3320 4175 746f 6e6f   (c) 2023 Autono
-000000b0: 6d69 2041 490a 2020 2020 2020 2020 0a20  mi AI.        . 
-000000c0: 2020 2020 2020 2050 6572 6d69 7373 696f         Permissio
-000000d0: 6e20 6973 2068 6572 6562 7920 6772 616e  n is hereby gran
-000000e0: 7465 642c 2066 7265 6520 6f66 2063 6861  ted, free of cha
-000000f0: 7267 652c 2074 6f20 616e 7920 7065 7273  rge, to any pers
-00000100: 6f6e 206f 6274 6169 6e69 6e67 2061 2063  on obtaining a c
-00000110: 6f70 790a 2020 2020 2020 2020 6f66 2074  opy.        of t
-00000120: 6869 7320 736f 6674 7761 7265 2061 6e64  his software and
-00000130: 2061 7373 6f63 6961 7465 6420 646f 6375   associated docu
-00000140: 6d65 6e74 6174 696f 6e20 6669 6c65 7320  mentation files 
-00000150: 2874 6865 2022 536f 6674 7761 7265 2229  (the "Software")
-00000160: 2c20 746f 2064 6561 6c0a 2020 2020 2020  , to deal.      
-00000170: 2020 696e 2074 6865 2053 6f66 7477 6172    in the Softwar
-00000180: 6520 7769 7468 6f75 7420 7265 7374 7269  e without restri
-00000190: 6374 696f 6e2c 2069 6e63 6c75 6469 6e67  ction, including
-000001a0: 2077 6974 686f 7574 206c 696d 6974 6174   without limitat
-000001b0: 696f 6e20 7468 6520 7269 6768 7473 0a20  ion the rights. 
-000001c0: 2020 2020 2020 2074 6f20 7573 652c 2063         to use, c
-000001d0: 6f70 792c 206d 6f64 6966 792c 206d 6572  opy, modify, mer
-000001e0: 6765 2c20 7075 626c 6973 682c 2064 6973  ge, publish, dis
-000001f0: 7472 6962 7574 652c 2073 7562 6c69 6365  tribute, sublice
-00000200: 6e73 652c 2061 6e64 2f6f 7220 7365 6c6c  nse, and/or sell
-00000210: 0a20 2020 2020 2020 2063 6f70 6965 7320  .        copies 
-00000220: 6f66 2074 6865 2053 6f66 7477 6172 652c  of the Software,
-00000230: 2061 6e64 2074 6f20 7065 726d 6974 2070   and to permit p
-00000240: 6572 736f 6e73 2074 6f20 7768 6f6d 2074  ersons to whom t
-00000250: 6865 2053 6f66 7477 6172 6520 6973 0a20  he Software is. 
-00000260: 2020 2020 2020 2066 7572 6e69 7368 6564         furnished
-00000270: 2074 6f20 646f 2073 6f2c 2073 7562 6a65   to do so, subje
-00000280: 6374 2074 6f20 7468 6520 666f 6c6c 6f77  ct to the follow
-00000290: 696e 6720 636f 6e64 6974 696f 6e73 3a0a  ing conditions:.
-000002a0: 2020 2020 2020 2020 0a20 2020 2020 2020          .       
-000002b0: 2054 6865 2061 626f 7665 2063 6f70 7972   The above copyr
-000002c0: 6967 6874 206e 6f74 6963 6520 616e 6420  ight notice and 
-000002d0: 7468 6973 2070 6572 6d69 7373 696f 6e20  this permission 
-000002e0: 6e6f 7469 6365 2073 6861 6c6c 2062 6520  notice shall be 
-000002f0: 696e 636c 7564 6564 2069 6e20 616c 6c0a  included in all.
-00000300: 2020 2020 2020 2020 636f 7069 6573 206f          copies o
-00000310: 7220 7375 6273 7461 6e74 6961 6c20 706f  r substantial po
-00000320: 7274 696f 6e73 206f 6620 7468 6520 536f  rtions of the So
-00000330: 6674 7761 7265 2e0a 2020 2020 2020 2020  ftware..        
-00000340: 0a20 2020 2020 2020 2054 4845 2053 4f46  .        THE SOF
-00000350: 5457 4152 4520 4953 2050 524f 5649 4445  TWARE IS PROVIDE
-00000360: 4420 2241 5320 4953 222c 2057 4954 484f  D "AS IS", WITHO
-00000370: 5554 2057 4152 5241 4e54 5920 4f46 2041  UT WARRANTY OF A
-00000380: 4e59 204b 494e 442c 2045 5850 5245 5353  NY KIND, EXPRESS
-00000390: 204f 520a 2020 2020 2020 2020 494d 504c   OR.        IMPL
-000003a0: 4945 442c 2049 4e43 4c55 4449 4e47 2042  IED, INCLUDING B
-000003b0: 5554 204e 4f54 204c 494d 4954 4544 2054  UT NOT LIMITED T
-000003c0: 4f20 5448 4520 5741 5252 414e 5449 4553  O THE WARRANTIES
-000003d0: 204f 4620 4d45 5243 4841 4e54 4142 494c   OF MERCHANTABIL
-000003e0: 4954 592c 0a20 2020 2020 2020 2046 4954  ITY,.        FIT
-000003f0: 4e45 5353 2046 4f52 2041 2050 4152 5449  NESS FOR A PARTI
-00000400: 4355 4c41 5220 5055 5250 4f53 4520 414e  CULAR PURPOSE AN
-00000410: 4420 4e4f 4e49 4e46 5249 4e47 454d 454e  D NONINFRINGEMEN
-00000420: 542e 2049 4e20 4e4f 2045 5645 4e54 2053  T. IN NO EVENT S
-00000430: 4841 4c4c 2054 4845 0a20 2020 2020 2020  HALL THE.       
-00000440: 2041 5554 484f 5253 204f 5220 434f 5059   AUTHORS OR COPY
-00000450: 5249 4748 5420 484f 4c44 4552 5320 4245  RIGHT HOLDERS BE
-00000460: 204c 4941 424c 4520 464f 5220 414e 5920   LIABLE FOR ANY 
-00000470: 434c 4149 4d2c 2044 414d 4147 4553 204f  CLAIM, DAMAGES O
-00000480: 5220 4f54 4845 520a 2020 2020 2020 2020  R OTHER.        
-00000490: 4c49 4142 494c 4954 592c 2057 4845 5448  LIABILITY, WHETH
-000004a0: 4552 2049 4e20 414e 2041 4354 494f 4e20  ER IN AN ACTION 
-000004b0: 4f46 2043 4f4e 5452 4143 542c 2054 4f52  OF CONTRACT, TOR
-000004c0: 5420 4f52 204f 5448 4552 5749 5345 2c20  T OR OTHERWISE, 
-000004d0: 4152 4953 494e 4720 4652 4f4d 2c0a 2020  ARISING FROM,.  
-000004e0: 2020 2020 2020 4f55 5420 4f46 204f 5220        OUT OF OR 
-000004f0: 494e 2043 4f4e 4e45 4354 494f 4e20 5749  IN CONNECTION WI
-00000500: 5448 2054 4845 2053 4f46 5457 4152 4520  TH THE SOFTWARE 
-00000510: 4f52 2054 4845 2055 5345 204f 5220 4f54  OR THE USE OR OT
-00000520: 4845 5220 4445 414c 494e 4753 2049 4e20  HER DEALINGS IN 
-00000530: 5448 450a 2020 2020 2020 2020 534f 4654  THE.        SOFT
-00000540: 5741 5245 2e0a 2020 2020 2020 2020 0a50  WARE..        .P
-00000550: 726f 6a65 6374 2d55 524c 3a20 446f 6375  roject-URL: Docu
-00000560: 6d65 6e74 6174 696f 6e2c 2068 7474 7073  mentation, https
-00000570: 3a2f 2f61 7574 6f6e 6f6d 692d 6169 2e67  ://autonomi-ai.g
-00000580: 6974 6875 622e 696f 2f6e 6f73 2f0a 5072  ithub.io/nos/.Pr
-00000590: 6f6a 6563 742d 5552 4c3a 2053 6f75 7263  oject-URL: Sourc
-000005a0: 6520 436f 6465 2c20 6874 7470 733a 2f2f  e Code, https://
-000005b0: 6769 7468 7562 2e63 6f6d 2f61 7574 6f6e  github.com/auton
-000005c0: 6f6d 692d 6169 2f6e 6f73 0a43 6c61 7373  omi-ai/nos.Class
-000005d0: 6966 6965 723a 2044 6576 656c 6f70 6d65  ifier: Developme
-000005e0: 6e74 2053 7461 7475 7320 3a3a 2033 202d  nt Status :: 3 -
-000005f0: 2041 6c70 6861 0a43 6c61 7373 6966 6965   Alpha.Classifie
-00000600: 723a 2050 726f 6772 616d 6d69 6e67 204c  r: Programming L
-00000610: 616e 6775 6167 6520 3a3a 2050 7974 686f  anguage :: Pytho
-00000620: 6e0a 436c 6173 7369 6669 6572 3a20 546f  n.Classifier: To
-00000630: 7069 6320 3a3a 2053 6f66 7477 6172 6520  pic :: Software 
-00000640: 4465 7665 6c6f 706d 656e 7420 3a3a 204c  Development :: L
-00000650: 6962 7261 7269 6573 0a43 6c61 7373 6966  ibraries.Classif
-00000660: 6965 723a 2054 6f70 6963 203a 3a20 5363  ier: Topic :: Sc
-00000670: 6965 6e74 6966 6963 2f45 6e67 696e 6565  ientific/Enginee
-00000680: 7269 6e67 203a 3a20 4172 7469 6669 6369  ring :: Artifici
-00000690: 616c 2049 6e74 656c 6c69 6765 6e63 650a  al Intelligence.
-000006a0: 436c 6173 7369 6669 6572 3a20 546f 7069  Classifier: Topi
-000006b0: 6320 3a3a 2053 6369 656e 7469 6669 632f  c :: Scientific/
-000006c0: 456e 6769 6e65 6572 696e 6720 3a3a 2049  Engineering :: I
-000006d0: 6d61 6765 2050 726f 6365 7373 696e 670a  mage Processing.
-000006e0: 436c 6173 7369 6669 6572 3a20 4c69 6365  Classifier: Lice
-000006f0: 6e73 6520 3a3a 204f 5349 2041 7070 726f  nse :: OSI Appro
-00000700: 7665 6420 3a3a 204d 4954 204c 6963 656e  ved :: MIT Licen
-00000710: 7365 0a43 6c61 7373 6966 6965 723a 2050  se.Classifier: P
-00000720: 726f 6772 616d 6d69 6e67 204c 616e 6775  rogramming Langu
-00000730: 6167 6520 3a3a 2050 7974 686f 6e20 3a3a  age :: Python ::
-00000740: 2033 0a43 6c61 7373 6966 6965 723a 2050   3.Classifier: P
-00000750: 726f 6772 616d 6d69 6e67 204c 616e 6775  rogramming Langu
-00000760: 6167 6520 3a3a 2050 7974 686f 6e20 3a3a  age :: Python ::
-00000770: 2033 2e38 0a52 6571 7569 7265 732d 5079   3.8.Requires-Py
-00000780: 7468 6f6e 3a20 3e3d 332e 372e 3130 0a44  thon: >=3.7.10.D
-00000790: 6573 6372 6970 7469 6f6e 2d43 6f6e 7465  escription-Conte
-000007a0: 6e74 2d54 7970 653a 2074 6578 742f 6d61  nt-Type: text/ma
-000007b0: 726b 646f 776e 0a4c 6963 656e 7365 2d46  rkdown.License-F
-000007c0: 696c 653a 204c 4943 454e 5345 0a52 6571  ile: LICENSE.Req
-000007d0: 7569 7265 732d 4469 7374 3a20 6176 2028  uires-Dist: av (
-000007e0: 3e3d 3130 2e30 2e30 290a 5265 7175 6972  >=10.0.0).Requir
-000007f0: 6573 2d44 6973 743a 2063 6c6f 7564 7069  es-Dist: cloudpi
-00000800: 636b 6c65 2028 3e3d 322e 322e 3129 0a52  ckle (>=2.2.1).R
-00000810: 6571 7569 7265 732d 4469 7374 3a20 646f  equires-Dist: do
-00000820: 636b 6572 2028 3e3d 362e 302e 3029 0a52  cker (>=6.0.0).R
-00000830: 6571 7569 7265 732d 4469 7374 3a20 6772  equires-Dist: gr
-00000840: 7063 696f 2d74 6f6f 6c73 2028 3c3d 312e  pcio-tools (<=1.
-00000850: 3439 2e31 2c3e 3d31 2e33 322e 3029 0a52  49.1,>=1.32.0).R
-00000860: 6571 7569 7265 732d 4469 7374 3a20 6c6f  equires-Dist: lo
-00000870: 6775 7275 2028 3e3d 302e 372e 3029 0a52  guru (>=0.7.0).R
-00000880: 6571 7569 7265 732d 4469 7374 3a20 6f70  equires-Dist: op
-00000890: 656e 6376 2d70 7974 686f 6e2d 6865 6164  encv-python-head
-000008a0: 6c65 7373 2028 3e3d 342e 362e 302e 3636  less (>=4.6.0.66
-000008b0: 290a 5265 7175 6972 6573 2d44 6973 743a  ).Requires-Dist:
-000008c0: 2050 696c 6c6f 770a 5265 7175 6972 6573   Pillow.Requires
-000008d0: 2d44 6973 743a 2070 7375 7469 6c20 283e  -Dist: psutil (>
-000008e0: 3d35 2e39 2e35 290a 5265 7175 6972 6573  =5.9.5).Requires
-000008f0: 2d44 6973 743a 2070 7964 616e 7469 630a  -Dist: pydantic.
-00000900: 5265 7175 6972 6573 2d44 6973 743a 2072  Requires-Dist: r
-00000910: 6963 6820 283e 3d31 322e 352e 3129 0a52  ich (>=12.5.1).R
-00000920: 6571 7569 7265 732d 4469 7374 3a20 7471  equires-Dist: tq
-00000930: 646d 0a52 6571 7569 7265 732d 4469 7374  dm.Requires-Dist
-00000940: 3a20 7479 7065 7220 283e 3d30 2e37 2e30  : typer (>=0.7.0
-00000950: 290a 5265 7175 6972 6573 2d44 6973 743a  ).Requires-Dist:
-00000960: 2074 7970 696e 672d 6578 7465 6e73 696f   typing-extensio
-00000970: 6e73 2028 3e3d 342e 352e 3029 0a50 726f  ns (>=4.5.0).Pro
-00000980: 7669 6465 732d 4578 7472 613a 2064 6576  vides-Extra: dev
-00000990: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
-000009a0: 626c 6163 6b5b 6a75 7079 7465 725d 2028  black[jupyter] (
-000009b0: 3d3d 3232 2e33 2e30 2920 3b20 6578 7472  ==22.3.0) ; extr
-000009c0: 6120 3d3d 2027 6465 7627 0a52 6571 7569  a == 'dev'.Requi
-000009d0: 7265 732d 4469 7374 3a20 6275 696c 6420  res-Dist: build 
-000009e0: 283d 3d30 2e31 302e 3029 203b 2065 7874  (==0.10.0) ; ext
-000009f0: 7261 203d 3d20 2764 6576 270a 5265 7175  ra == 'dev'.Requ
-00000a00: 6972 6573 2d44 6973 743a 2070 7265 2d63  ires-Dist: pre-c
-00000a10: 6f6d 6d69 7420 283d 3d33 2e32 2e32 2920  ommit (==3.2.2) 
-00000a20: 3b20 6578 7472 6120 3d3d 2027 6465 7627  ; extra == 'dev'
-00000a30: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
-00000a40: 7275 6666 2028 3d3d 302e 302e 3236 3229  ruff (==0.0.262)
-00000a50: 203b 2065 7874 7261 203d 3d20 2764 6576   ; extra == 'dev
-00000a60: 270a 5072 6f76 6964 6573 2d45 7874 7261  '.Provides-Extra
-00000a70: 3a20 646f 6373 0a52 6571 7569 7265 732d  : docs.Requires-
-00000a80: 4469 7374 3a20 6d6b 646f 6373 203b 2065  Dist: mkdocs ; e
-00000a90: 7874 7261 203d 3d20 2764 6f63 7327 0a52  xtra == 'docs'.R
-00000aa0: 6571 7569 7265 732d 4469 7374 3a20 6d6b  equires-Dist: mk
-00000ab0: 646f 6373 2d65 7863 6c75 6465 203b 2065  docs-exclude ; e
-00000ac0: 7874 7261 203d 3d20 2764 6f63 7327 0a52  xtra == 'docs'.R
-00000ad0: 6571 7569 7265 732d 4469 7374 3a20 6d6b  equires-Dist: mk
-00000ae0: 646f 6373 2d69 6e63 6c75 6465 2d6d 6172  docs-include-mar
-00000af0: 6b64 6f77 6e2d 706c 7567 696e 203b 2065  kdown-plugin ; e
-00000b00: 7874 7261 203d 3d20 2764 6f63 7327 0a52  xtra == 'docs'.R
-00000b10: 6571 7569 7265 732d 4469 7374 3a20 6d6b  equires-Dist: mk
-00000b20: 646f 6373 2d6d 6174 6572 6961 6c20 3b20  docs-material ; 
-00000b30: 6578 7472 6120 3d3d 2027 646f 6373 270a  extra == 'docs'.
-00000b40: 5265 7175 6972 6573 2d44 6973 743a 206d  Requires-Dist: m
-00000b50: 6b64 6f63 732d 7079 6d64 6f77 6e78 2d6d  kdocs-pymdownx-m
-00000b60: 6174 6572 6961 6c2d 6578 7472 6173 203b  aterial-extras ;
-00000b70: 2065 7874 7261 203d 3d20 2764 6f63 7327   extra == 'docs'
-00000b80: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
-00000b90: 6d6b 646f 6373 2d73 616d 652d 6469 7220  mkdocs-same-dir 
-00000ba0: 3b20 6578 7472 6120 3d3d 2027 646f 6373  ; extra == 'docs
-00000bb0: 270a 5265 7175 6972 6573 2d44 6973 743a  '.Requires-Dist:
-00000bc0: 206d 6b64 6f63 7374 7269 6e67 7320 3b20   mkdocstrings ; 
-00000bd0: 6578 7472 6120 3d3d 2027 646f 6373 270a  extra == 'docs'.
-00000be0: 5265 7175 6972 6573 2d44 6973 743a 206d  Requires-Dist: m
-00000bf0: 6b64 6f63 7374 7269 6e67 732d 7079 7468  kdocstrings-pyth
-00000c00: 6f6e 203b 2065 7874 7261 203d 3d20 2764  on ; extra == 'd
-00000c10: 6f63 7327 0a52 6571 7569 7265 732d 4469  ocs'.Requires-Di
-00000c20: 7374 3a20 6d6b 646f 6373 7472 696e 6773  st: mkdocstrings
-00000c30: 5b70 7974 686f 6e5d 203b 2065 7874 7261  [python] ; extra
-00000c40: 203d 3d20 2764 6f63 7327 0a50 726f 7669   == 'docs'.Provi
-00000c50: 6465 732d 4578 7472 613a 2073 6572 7665  des-Extra: serve
-00000c60: 720a 5265 7175 6972 6573 2d44 6973 743a  r.Requires-Dist:
-00000c70: 2064 6966 6675 7365 7273 203b 2065 7874   diffusers ; ext
-00000c80: 7261 203d 3d20 2773 6572 7665 7227 0a52  ra == 'server'.R
-00000c90: 6571 7569 7265 732d 4469 7374 3a20 6875  equires-Dist: hu
-00000ca0: 6767 696e 6766 6163 652d 6875 6220 3b20  ggingface-hub ; 
-00000cb0: 6578 7472 6120 3d3d 2027 7365 7276 6572  extra == 'server
-00000cc0: 270a 5265 7175 6972 6573 2d44 6973 743a  '.Requires-Dist:
-00000cd0: 2070 7961 7272 6f77 2028 3e3d 3132 2e30   pyarrow (>=12.0
-00000ce0: 2e30 2920 3b20 6578 7472 6120 3d3d 2027  .0) ; extra == '
-00000cf0: 7365 7276 6572 270a 5265 7175 6972 6573  server'.Requires
-00000d00: 2d44 6973 743a 2072 6179 5b73 6572 7665  -Dist: ray[serve
-00000d10: 5d20 283e 3d32 2e34 2e30 2920 3b20 6578  ] (>=2.4.0) ; ex
-00000d20: 7472 6120 3d3d 2027 7365 7276 6572 270a  tra == 'server'.
-00000d30: 5265 7175 6972 6573 2d44 6973 743a 2074  Requires-Dist: t
-00000d40: 6162 756c 6174 6520 3b20 6578 7472 6120  abulate ; extra 
-00000d50: 3d3d 2027 7365 7276 6572 270a 5265 7175  == 'server'.Requ
-00000d60: 6972 6573 2d44 6973 743a 2074 7261 6e73  ires-Dist: trans
-00000d70: 666f 726d 6572 7320 283e 3d34 2e32 392e  formers (>=4.29.
-00000d80: 3229 203b 2065 7874 7261 203d 3d20 2773  2) ; extra == 's
-00000d90: 6572 7665 7227 0a52 6571 7569 7265 732d  erver'.Requires-
-00000da0: 4469 7374 3a20 746f 7263 6820 283c 322e  Dist: torch (<2.
-00000db0: 302e 302c 3e3d 312e 3132 2e31 2920 3b20  0.0,>=1.12.1) ; 
-00000dc0: 6578 7472 6120 3d3d 2027 7365 7276 6572  extra == 'server
-00000dd0: 270a 5265 7175 6972 6573 2d44 6973 743a  '.Requires-Dist:
-00000de0: 2074 6f72 6368 6175 6469 6f20 283e 3d30   torchaudio (>=0
-00000df0: 2e31 332e 3129 203b 2065 7874 7261 203d  .13.1) ; extra =
-00000e00: 3d20 2773 6572 7665 7227 0a52 6571 7569  = 'server'.Requi
-00000e10: 7265 732d 4469 7374 3a20 746f 7263 6876  res-Dist: torchv
-00000e20: 6973 696f 6e20 283e 3d30 2e31 332e 3029  ision (>=0.13.0)
-00000e30: 203b 2065 7874 7261 203d 3d20 2773 6572   ; extra == 'ser
-00000e40: 7665 7227 0a50 726f 7669 6465 732d 4578  ver'.Provides-Ex
-00000e50: 7472 613a 2074 6573 740a 5265 7175 6972  tra: test.Requir
-00000e60: 6573 2d44 6973 743a 2070 7974 6573 7420  es-Dist: pytest 
-00000e70: 3b20 6578 7472 6120 3d3d 2027 7465 7374  ; extra == 'test
-00000e80: 270a 5265 7175 6972 6573 2d44 6973 743a  '.Requires-Dist:
-00000e90: 2070 7974 6573 742d 6173 796e 6369 6f20   pytest-asyncio 
-00000ea0: 3b20 6578 7472 6120 3d3d 2027 7465 7374  ; extra == 'test
-00000eb0: 270a 5265 7175 6972 6573 2d44 6973 743a  '.Requires-Dist:
-00000ec0: 2074 7970 6567 7561 7264 203b 2065 7874   typeguard ; ext
-00000ed0: 7261 203d 3d20 2774 6573 7427 0a50 726f  ra == 'test'.Pro
-00000ee0: 7669 6465 732d 4578 7472 613a 2074 6f72  vides-Extra: tor
-00000ef0: 6368 0a52 6571 7569 7265 732d 4469 7374  ch.Requires-Dist
-00000f00: 3a20 746f 7263 6820 283c 322e 302e 302c  : torch (<2.0.0,
-00000f10: 3e3d 312e 3132 2e31 2920 3b20 6578 7472  >=1.12.1) ; extr
-00000f20: 6120 3d3d 2027 746f 7263 6827 0a52 6571  a == 'torch'.Req
-00000f30: 7569 7265 732d 4469 7374 3a20 746f 7263  uires-Dist: torc
-00000f40: 6861 7564 696f 2028 3e3d 302e 3133 2e31  haudio (>=0.13.1
-00000f50: 2920 3b20 6578 7472 6120 3d3d 2027 746f  ) ; extra == 'to
-00000f60: 7263 6827 0a52 6571 7569 7265 732d 4469  rch'.Requires-Di
-00000f70: 7374 3a20 746f 7263 6876 6973 696f 6e20  st: torchvision 
-00000f80: 283e 3d30 2e31 332e 3029 203b 2065 7874  (>=0.13.0) ; ext
-00000f90: 7261 203d 3d20 2774 6f72 6368 270a 0a23  ra == 'torch'..#
-00000fa0: 206e 6f73 20f0 9f94 a53a 204e 6974 726f   nos ....: Nitro
-00000fb0: 7573 204f 7869 6465 2053 7973 7465 6d20  us Oxide System 
-00000fc0: 284e 4f53 2920 666f 7220 436f 6d70 7574  (NOS) for Comput
-00000fd0: 6572 2056 6973 696f 6e0a 0a3c 7020 616c  er Vision..<p al
-00000fe0: 6967 6e3d 2263 656e 7465 7222 3e0a 2020  ign="center">.  
-00000ff0: 2020 3c61 2068 7265 663d 2268 7474 7073    <a href="https
-00001000: 3a2f 2f70 7970 692e 6f72 672f 7072 6f6a  ://pypi.org/proj
-00001010: 6563 742f 6175 746f 6e6f 6d69 2d6e 6f73  ect/autonomi-nos
-00001020: 2f22 3e0a 2020 2020 2020 2020 3c69 6d67  /">.        <img
-00001030: 2061 6c74 3d22 5079 5069 2056 6572 7369   alt="PyPi Versi
-00001040: 6f6e 2220 7372 633d 2268 7474 7073 3a2f  on" src="https:/
-00001050: 2f62 6164 6765 2e66 7572 792e 696f 2f70  /badge.fury.io/p
-00001060: 792f 6175 746f 6e6f 6d69 2d6e 6f73 2e73  y/autonomi-nos.s
-00001070: 7667 223e 0a20 2020 203c 2f61 3e0a 2020  vg">.    </a>.  
-00001080: 2020 3c61 2068 7265 663d 2268 7474 7073    <a href="https
-00001090: 3a2f 2f70 7970 692e 6f72 672f 7072 6f6a  ://pypi.org/proj
-000010a0: 6563 742f 6175 746f 6e6f 6d69 2d6e 6f73  ect/autonomi-nos
-000010b0: 2f22 3e0a 2020 2020 2020 2020 3c69 6d67  /">.        <img
-000010c0: 2061 6c74 3d22 5079 5069 2056 6572 7369   alt="PyPi Versi
-000010d0: 6f6e 2220 7372 633d 2268 7474 7073 3a2f  on" src="https:/
-000010e0: 2f69 6d67 2e73 6869 656c 6473 2e69 6f2f  /img.shields.io/
-000010f0: 7079 7069 2f70 7976 6572 7369 6f6e 732f  pypi/pyversions/
-00001100: 6175 746f 6e6f 6d69 2d6e 6f73 223e 0a20  autonomi-nos">. 
-00001110: 2020 203c 2f61 3e0a 2020 2020 3c61 2068     </a>.    <a h
-00001120: 7265 663d 2268 7474 7073 3a2f 2f70 7970  ref="https://pyp
-00001130: 692e 6f72 672f 7072 6f6a 6563 742f 6175  i.org/project/au
-00001140: 746f 6e6f 6d69 2d6e 6f73 2f22 3e0a 2020  tonomi-nos/">.  
-00001150: 2020 2020 2020 3c69 6d67 2061 6c74 3d22        <img alt="
-00001160: 5079 5069 2044 6f77 6e6c 6f61 6473 2220  PyPi Downloads" 
-00001170: 7372 633d 2268 7474 7073 3a2f 2f69 6d67  src="https://img
-00001180: 2e73 6869 656c 6473 2e69 6f2f 7079 7069  .shields.io/pypi
-00001190: 2f64 6d2f 6175 746f 6e6f 6d69 2d6e 6f73  /dm/autonomi-nos
-000011a0: 223e 0a20 2020 203c 2f61 3e0a 2020 2020  ">.    </a>.    
-000011b0: 3c61 2068 7265 663d 2268 7474 7073 3a2f  <a href="https:/
-000011c0: 2f64 6973 636f 7264 2e67 672f 5141 4767  /discord.gg/QAGg
-000011d0: 7654 7576 6767 223e 0a20 2020 2020 2020  vTuvgg">.       
-000011e0: 203c 696d 6720 616c 743d 2244 6973 636f   <img alt="Disco
-000011f0: 7264 2220 7372 633d 2268 7474 7073 3a2f  rd" src="https:/
-00001200: 2f69 6d67 2e73 6869 656c 6473 2e69 6f2f  /img.shields.io/
-00001210: 6261 6467 652f 6469 7363 6f72 642d 6368  badge/discord-ch
-00001220: 6174 2d70 7572 706c 653f 636f 6c6f 723d  at-purple?color=
-00001230: 2532 3335 3736 3546 3226 6c61 6265 6c3d  %235765F2&label=
-00001240: 6469 7363 6f72 6426 6c6f 676f 3d64 6973  discord&logo=dis
-00001250: 636f 7264 223e 0a20 2020 203c 2f61 3e0a  cord">.    </a>.
-00001260: 3c2f 703e 0a0a 2a2a 4e4f 532a 2a20 6973  </p>..**NOS** is
-00001270: 2061 2050 7954 6f72 6368 206c 6962 7261   a PyTorch libra
-00001280: 7279 2066 6f72 206f 7074 696d 697a 696e  ry for optimizin
-00001290: 6720 616e 6420 7275 6e6e 696e 6720 6c69  g and running li
-000012a0: 6768 746e 696e 672d 6661 7374 2069 6e66  ghtning-fast inf
-000012b0: 6572 656e 6365 206f 6620 706f 7075 6c61  erence of popula
-000012c0: 7220 636f 6d70 7574 6572 2076 6973 696f  r computer visio
-000012d0: 6e20 6d6f 6465 6c73 2e20 4e4f 5320 696e  n models. NOS in
-000012e0: 6865 7269 7473 2069 7473 206e 616d 6520  herits its name 
-000012f0: 6672 6f6d 2022 4e69 7472 6f75 7320 4f78  from "Nitrous Ox
-00001300: 6964 6520 5379 7374 656d 222c 2074 6865  ide System", the
-00001310: 2070 6572 666f 726d 616e 6365 2d65 6e68   performance-enh
-00001320: 616e 6369 6e67 2073 7973 7465 6d20 7479  ancing system ty
-00001330: 7069 6361 6c6c 7920 7573 6564 2069 6e20  pically used in 
-00001340: 7261 6369 6e67 2063 6172 732e 204e 4f53  racing cars. NOS
-00001350: 2069 7320 6465 7369 676e 6564 2074 6f20   is designed to 
-00001360: 6265 206d 6f64 756c 6172 2061 6e64 2065  be modular and e
-00001370: 6173 7920 746f 2065 7874 656e 642e 0a0a  asy to extend...
-00001380: 2323 2057 6879 204e 4f53 3f0a 2d20 e29a  ## Why NOS?.- ..
-00001390: a1ef b88f 202a 2a46 6173 742a 2a3a 2042  .... **Fast**: B
-000013a0: 7569 6c74 2066 6f72 2050 7954 6f72 6368  uilt for PyTorch
-000013b0: 2061 6e64 2064 6573 6967 6e65 6420 746f   and designed to
-000013c0: 206f 7074 696d 697a 652f 7275 6e20 6d6f   optimize/run mo
-000013d0: 6465 6c73 2066 6173 7465 720a 2d20 f09f  dels faster.- ..
-000013e0: 94a5 202a 2a50 6572 666f 726d 616e 742a  .. **Performant*
-000013f0: 2a3a 2052 756e 206d 6f64 656c 7320 7375  *: Run models su
-00001400: 6368 2061 7320 5344 7632 206f 7220 6f62  ch as SDv2 or ob
-00001410: 6a65 6374 2064 6574 6563 7469 6f6e 2032  ject detection 2
-00001420: 2d33 7820 6661 7374 6572 206f 7574 2d6f  -3x faster out-o
-00001430: 662d 7468 652d 626f 780a 2d20 f09f 91a9  f-the-box.- ....
-00001440: e280 8df0 9f92 bb20 2a2a 4e6f 2050 6844  ....... **No PhD
-00001450: 2072 6571 7569 7265 642a 2a3a 204f 7074   required**: Opt
-00001460: 696d 697a 6520 6d6f 6465 6c73 2066 6f72  imize models for
-00001470: 206d 6178 696d 756d 2048 5720 7065 7266   maximum HW perf
-00001480: 6f72 6d61 6e63 6520 7769 7468 6f75 7420  ormance without 
-00001490: 6120 5068 4420 696e 204d 4c0a 2d20 f09f  a PhD in ML.- ..
-000014a0: 93a6 202a 2a45 7874 656e 7369 626c 652a  .. **Extensible*
-000014b0: 2a3a 2045 6173 696c 7920 6164 6420 6f70  *: Easily add op
-000014c0: 7469 6d69 7a61 7469 6f6e 2061 6e64 2048  timization and H
-000014d0: 572d 7375 7070 6f72 7420 666f 7220 6375  W-support for cu
-000014e0: 7374 6f6d 206d 6f64 656c 730a 2d20 e29a  stom models.- ..
-000014f0: 99ef b88f 202a 2a48 572d 6163 6365 6c65  .... **HW-accele
-00001500: 7261 7465 643a 2a2a 2054 616b 6520 6675  rated:** Take fu
-00001510: 6c6c 2061 6476 616e 7461 6765 206f 6620  ll advantage of 
-00001520: 796f 7572 2048 5720 2847 5055 732c 2041  your HW (GPUs, A
-00001530: 5349 4373 2920 7769 7468 6f75 7420 636f  SICs) without co
-00001540: 6d70 726f 6d69 7365 0a2d 20e2 9881 efb8  mpromise.- .....
-00001550: 8f20 2a2a 436c 6f75 642d 6167 6e6f 7374  . **Cloud-agnost
-00001560: 6963 3a2a 2a20 5275 6e20 6f6e 2061 6e79  ic:** Run on any
-00001570: 2063 6c6f 7564 2048 5720 2841 5753 2c20   cloud HW (AWS, 
-00001580: 4743 502c 2041 7a75 7265 2c20 4c61 6d62  GCP, Azure, Lamb
-00001590: 6461 204c 6162 732c 204f 6e2d 5072 656d  da Labs, On-Prem
-000015a0: 290a 0a23 2320 4261 7474 6572 6965 7320  )..## Batteries 
-000015b0: 496e 636c 7564 6564 0a20 2d20 f09f 92aa  Included. - ....
-000015c0: 202a 2a53 4f54 4120 4d6f 6465 6c20 5375   **SOTA Model Su
-000015d0: 7070 6f72 743a 2a2a 204e 4f53 2070 726f  pport:** NOS pro
-000015e0: 7669 6465 7320 6f75 742d 6f66 2d74 6865  vides out-of-the
-000015f0: 2d62 6f78 2073 7570 706f 7274 2066 6f72  -box support for
-00001600: 2070 6f70 756c 6172 2043 5620 6d6f 6465   popular CV mode
-00001610: 6c73 2073 7563 6820 6173 205b 5374 6162  ls such as [Stab
-00001620: 6c65 2044 6966 6675 7369 6f6e 5d28 7374  le Diffusion](st
-00001630: 6162 696c 6974 7961 692f 7374 6162 6c65  abilityai/stable
-00001640: 2d64 6966 6675 7369 6f6e 2d32 292c 205b  -diffusion-2), [
-00001650: 4f70 656e 4149 2043 4c49 505d 286f 7065  OpenAI CLIP](ope
-00001660: 6e61 692f 636c 6970 2d76 6974 2d62 6173  nai/clip-vit-bas
-00001670: 652d 7061 7463 6833 3229 2c20 5b4f 7065  e-patch32), [Ope
-00001680: 6e4d 4d4c 6162 5d28 6874 7470 733a 2f2f  nMMLab](https://
-00001690: 6769 7468 7562 2e63 6f6d 2f6f 7065 6e2d  github.com/open-
-000016a0: 6d6d 6c61 622f 2920 6f62 6a65 6374 2064  mmlab/) object d
-000016b0: 6574 6563 7469 6f6e 2c20 7472 6163 6b69  etection, tracki
-000016c0: 6e67 2061 6e64 206d 6f72 650a 202d 20f0  ng and more. - .
-000016d0: 9f94 8c20 2a2a 4150 4973 3a2a 2a20 4e4f  ... **APIs:** NO
-000016e0: 5320 7072 6f76 6964 6573 206f 7574 2d6f  S provides out-o
-000016f0: 662d 7468 652d 626f 7820 4150 4973 2061  f-the-box APIs a
-00001700: 6e64 2061 766f 6964 7320 616c 6c20 7468  nd avoids all th
-00001710: 6520 4d4c 206d 6f64 656c 2064 6570 6c6f  e ML model deplo
-00001720: 796d 656e 7420 6861 7373 6c65 730a 202d  yment hassles. -
-00001730: 20f0 9f90 b320 2a2a 446f 636b 6572 3a2a   .... **Docker:*
-00001740: 2a20 4e4f 5320 7368 6970 7320 7769 7468  * NOS ships with
-00001750: 2064 6f63 6b65 7220 696d 6167 6573 2074   docker images t
-00001760: 6f20 7275 6e20 6163 6365 6c65 7261 7465  o run accelerate
-00001770: 6420 616e 6420 7363 616c 6162 6c65 2043  d and scalable C
-00001780: 5620 776f 726b 6c6f 6164 730a 202d 20f0  V workloads. - .
-00001790: 9f93 8820 2a2a 4d75 6c74 692d 506c 6174  ... **Multi-Plat
-000017a0: 666f 726d 2a2a 3a20 4e4f 5320 616c 6c6f  form**: NOS allo
-000017b0: 7773 2079 6f75 2074 6f20 7275 6e20 6d6f  ws you to run mo
-000017c0: 6465 6c73 206f 6e20 6469 6666 6572 656e  dels on differen
-000017d0: 7420 4857 2028 4e56 4944 4941 2c20 6375  t HW (NVIDIA, cu
-000017e0: 7374 6f6d 2041 5349 4373 2920 7769 7468  stom ASICs) with
-000017f0: 6f75 7420 616e 7920 6d6f 6465 6c20 636f  out any model co
-00001800: 6d70 696c 6174 696f 6e20 6f72 2072 756e  mpilation or run
-00001810: 7469 6d65 206d 616e 6167 656d 656e 742e  time management.
-00001820: 0a0a 0a23 2320 436f 6e74 7269 6275 7465  ...## Contribute
-00001830: 0a57 6520 7765 6c63 6f6d 6520 636f 6e74  .We welcome cont
-00001840: 7269 6275 7469 6f6e 7321 2050 6c65 6173  ributions! Pleas
-00001850: 6520 7365 6520 6f75 7220 5b63 6f6e 7472  e see our [contr
-00001860: 6962 7574 696e 6720 6775 6964 655d 2864  ibuting guide](d
-00001870: 6f63 732f 434f 4e54 5249 4255 5449 4e47  ocs/CONTRIBUTING
-00001880: 2e6d 6429 2066 6f72 206d 6f72 6520 696e  .md) for more in
-00001890: 666f 726d 6174 696f 6e2e 0a              formation..
+00000030: 3a20 302e 302e 3661 310a 5375 6d6d 6172  : 0.0.6a1.Summar
+00000040: 793a 204e 6974 726f 7573 206f 7869 6465  y: Nitrous oxide
+00000050: 2073 7973 7465 6d20 284e 4f53 2920 666f   system (NOS) fo
+00000060: 7220 636f 6d70 7574 6572 2d76 6973 696f  r computer-visio
+00000070: 6e2e 0a4c 6963 656e 7365 3a20 4d49 5420  n..License: MIT 
+00000080: 4c69 6365 6e73 650a 2020 2020 2020 2020  License.        
+00000090: 0a20 2020 2020 2020 2043 6f70 7972 6967  .        Copyrig
+000000a0: 6874 2028 6329 2032 3032 3320 4175 746f  ht (c) 2023 Auto
+000000b0: 6e6f 6d69 2041 490a 2020 2020 2020 2020  nomi AI.        
+000000c0: 0a20 2020 2020 2020 2050 6572 6d69 7373  .        Permiss
+000000d0: 696f 6e20 6973 2068 6572 6562 7920 6772  ion is hereby gr
+000000e0: 616e 7465 642c 2066 7265 6520 6f66 2063  anted, free of c
+000000f0: 6861 7267 652c 2074 6f20 616e 7920 7065  harge, to any pe
+00000100: 7273 6f6e 206f 6274 6169 6e69 6e67 2061  rson obtaining a
+00000110: 2063 6f70 790a 2020 2020 2020 2020 6f66   copy.        of
+00000120: 2074 6869 7320 736f 6674 7761 7265 2061   this software a
+00000130: 6e64 2061 7373 6f63 6961 7465 6420 646f  nd associated do
+00000140: 6375 6d65 6e74 6174 696f 6e20 6669 6c65  cumentation file
+00000150: 7320 2874 6865 2022 536f 6674 7761 7265  s (the "Software
+00000160: 2229 2c20 746f 2064 6561 6c0a 2020 2020  "), to deal.    
+00000170: 2020 2020 696e 2074 6865 2053 6f66 7477      in the Softw
+00000180: 6172 6520 7769 7468 6f75 7420 7265 7374  are without rest
+00000190: 7269 6374 696f 6e2c 2069 6e63 6c75 6469  riction, includi
+000001a0: 6e67 2077 6974 686f 7574 206c 696d 6974  ng without limit
+000001b0: 6174 696f 6e20 7468 6520 7269 6768 7473  ation the rights
+000001c0: 0a20 2020 2020 2020 2074 6f20 7573 652c  .        to use,
+000001d0: 2063 6f70 792c 206d 6f64 6966 792c 206d   copy, modify, m
+000001e0: 6572 6765 2c20 7075 626c 6973 682c 2064  erge, publish, d
+000001f0: 6973 7472 6962 7574 652c 2073 7562 6c69  istribute, subli
+00000200: 6365 6e73 652c 2061 6e64 2f6f 7220 7365  cense, and/or se
+00000210: 6c6c 0a20 2020 2020 2020 2063 6f70 6965  ll.        copie
+00000220: 7320 6f66 2074 6865 2053 6f66 7477 6172  s of the Softwar
+00000230: 652c 2061 6e64 2074 6f20 7065 726d 6974  e, and to permit
+00000240: 2070 6572 736f 6e73 2074 6f20 7768 6f6d   persons to whom
+00000250: 2074 6865 2053 6f66 7477 6172 6520 6973   the Software is
+00000260: 0a20 2020 2020 2020 2066 7572 6e69 7368  .        furnish
+00000270: 6564 2074 6f20 646f 2073 6f2c 2073 7562  ed to do so, sub
+00000280: 6a65 6374 2074 6f20 7468 6520 666f 6c6c  ject to the foll
+00000290: 6f77 696e 6720 636f 6e64 6974 696f 6e73  owing conditions
+000002a0: 3a0a 2020 2020 2020 2020 0a20 2020 2020  :.        .     
+000002b0: 2020 2054 6865 2061 626f 7665 2063 6f70     The above cop
+000002c0: 7972 6967 6874 206e 6f74 6963 6520 616e  yright notice an
+000002d0: 6420 7468 6973 2070 6572 6d69 7373 696f  d this permissio
+000002e0: 6e20 6e6f 7469 6365 2073 6861 6c6c 2062  n notice shall b
+000002f0: 6520 696e 636c 7564 6564 2069 6e20 616c  e included in al
+00000300: 6c0a 2020 2020 2020 2020 636f 7069 6573  l.        copies
+00000310: 206f 7220 7375 6273 7461 6e74 6961 6c20   or substantial 
+00000320: 706f 7274 696f 6e73 206f 6620 7468 6520  portions of the 
+00000330: 536f 6674 7761 7265 2e0a 2020 2020 2020  Software..      
+00000340: 2020 0a20 2020 2020 2020 2054 4845 2053    .        THE S
+00000350: 4f46 5457 4152 4520 4953 2050 524f 5649  OFTWARE IS PROVI
+00000360: 4445 4420 2241 5320 4953 222c 2057 4954  DED "AS IS", WIT
+00000370: 484f 5554 2057 4152 5241 4e54 5920 4f46  HOUT WARRANTY OF
+00000380: 2041 4e59 204b 494e 442c 2045 5850 5245   ANY KIND, EXPRE
+00000390: 5353 204f 520a 2020 2020 2020 2020 494d  SS OR.        IM
+000003a0: 504c 4945 442c 2049 4e43 4c55 4449 4e47  PLIED, INCLUDING
+000003b0: 2042 5554 204e 4f54 204c 494d 4954 4544   BUT NOT LIMITED
+000003c0: 2054 4f20 5448 4520 5741 5252 414e 5449   TO THE WARRANTI
+000003d0: 4553 204f 4620 4d45 5243 4841 4e54 4142  ES OF MERCHANTAB
+000003e0: 494c 4954 592c 0a20 2020 2020 2020 2046  ILITY,.        F
+000003f0: 4954 4e45 5353 2046 4f52 2041 2050 4152  ITNESS FOR A PAR
+00000400: 5449 4355 4c41 5220 5055 5250 4f53 4520  TICULAR PURPOSE 
+00000410: 414e 4420 4e4f 4e49 4e46 5249 4e47 454d  AND NONINFRINGEM
+00000420: 454e 542e 2049 4e20 4e4f 2045 5645 4e54  ENT. IN NO EVENT
+00000430: 2053 4841 4c4c 2054 4845 0a20 2020 2020   SHALL THE.     
+00000440: 2020 2041 5554 484f 5253 204f 5220 434f     AUTHORS OR CO
+00000450: 5059 5249 4748 5420 484f 4c44 4552 5320  PYRIGHT HOLDERS 
+00000460: 4245 204c 4941 424c 4520 464f 5220 414e  BE LIABLE FOR AN
+00000470: 5920 434c 4149 4d2c 2044 414d 4147 4553  Y CLAIM, DAMAGES
+00000480: 204f 5220 4f54 4845 520a 2020 2020 2020   OR OTHER.      
+00000490: 2020 4c49 4142 494c 4954 592c 2057 4845    LIABILITY, WHE
+000004a0: 5448 4552 2049 4e20 414e 2041 4354 494f  THER IN AN ACTIO
+000004b0: 4e20 4f46 2043 4f4e 5452 4143 542c 2054  N OF CONTRACT, T
+000004c0: 4f52 5420 4f52 204f 5448 4552 5749 5345  ORT OR OTHERWISE
+000004d0: 2c20 4152 4953 494e 4720 4652 4f4d 2c0a  , ARISING FROM,.
+000004e0: 2020 2020 2020 2020 4f55 5420 4f46 204f          OUT OF O
+000004f0: 5220 494e 2043 4f4e 4e45 4354 494f 4e20  R IN CONNECTION 
+00000500: 5749 5448 2054 4845 2053 4f46 5457 4152  WITH THE SOFTWAR
+00000510: 4520 4f52 2054 4845 2055 5345 204f 5220  E OR THE USE OR 
+00000520: 4f54 4845 5220 4445 414c 494e 4753 2049  OTHER DEALINGS I
+00000530: 4e20 5448 450a 2020 2020 2020 2020 534f  N THE.        SO
+00000540: 4654 5741 5245 2e0a 2020 2020 2020 2020  FTWARE..        
+00000550: 0a50 726f 6a65 6374 2d55 524c 3a20 446f  .Project-URL: Do
+00000560: 6375 6d65 6e74 6174 696f 6e2c 2068 7474  cumentation, htt
+00000570: 7073 3a2f 2f61 7574 6f6e 6f6d 692d 6169  ps://autonomi-ai
+00000580: 2e67 6974 6875 622e 696f 2f6e 6f73 2f0a  .github.io/nos/.
+00000590: 5072 6f6a 6563 742d 5552 4c3a 2053 6f75  Project-URL: Sou
+000005a0: 7263 6520 436f 6465 2c20 6874 7470 733a  rce Code, https:
+000005b0: 2f2f 6769 7468 7562 2e63 6f6d 2f61 7574  //github.com/aut
+000005c0: 6f6e 6f6d 692d 6169 2f6e 6f73 0a43 6c61  onomi-ai/nos.Cla
+000005d0: 7373 6966 6965 723a 2044 6576 656c 6f70  ssifier: Develop
+000005e0: 6d65 6e74 2053 7461 7475 7320 3a3a 2033  ment Status :: 3
+000005f0: 202d 2041 6c70 6861 0a43 6c61 7373 6966   - Alpha.Classif
+00000600: 6965 723a 2050 726f 6772 616d 6d69 6e67  ier: Programming
+00000610: 204c 616e 6775 6167 6520 3a3a 2050 7974   Language :: Pyt
+00000620: 686f 6e0a 436c 6173 7369 6669 6572 3a20  hon.Classifier: 
+00000630: 546f 7069 6320 3a3a 2053 6f66 7477 6172  Topic :: Softwar
+00000640: 6520 4465 7665 6c6f 706d 656e 7420 3a3a  e Development ::
+00000650: 204c 6962 7261 7269 6573 0a43 6c61 7373   Libraries.Class
+00000660: 6966 6965 723a 2054 6f70 6963 203a 3a20  ifier: Topic :: 
+00000670: 5363 6965 6e74 6966 6963 2f45 6e67 696e  Scientific/Engin
+00000680: 6565 7269 6e67 203a 3a20 4172 7469 6669  eering :: Artifi
+00000690: 6369 616c 2049 6e74 656c 6c69 6765 6e63  cial Intelligenc
+000006a0: 650a 436c 6173 7369 6669 6572 3a20 546f  e.Classifier: To
+000006b0: 7069 6320 3a3a 2053 6369 656e 7469 6669  pic :: Scientifi
+000006c0: 632f 456e 6769 6e65 6572 696e 6720 3a3a  c/Engineering ::
+000006d0: 2049 6d61 6765 2050 726f 6365 7373 696e   Image Processin
+000006e0: 670a 436c 6173 7369 6669 6572 3a20 4c69  g.Classifier: Li
+000006f0: 6365 6e73 6520 3a3a 204f 5349 2041 7070  cense :: OSI App
+00000700: 726f 7665 6420 3a3a 204d 4954 204c 6963  roved :: MIT Lic
+00000710: 656e 7365 0a43 6c61 7373 6966 6965 723a  ense.Classifier:
+00000720: 2050 726f 6772 616d 6d69 6e67 204c 616e   Programming Lan
+00000730: 6775 6167 6520 3a3a 2050 7974 686f 6e20  guage :: Python 
+00000740: 3a3a 2033 0a43 6c61 7373 6966 6965 723a  :: 3.Classifier:
+00000750: 2050 726f 6772 616d 6d69 6e67 204c 616e   Programming Lan
+00000760: 6775 6167 6520 3a3a 2050 7974 686f 6e20  guage :: Python 
+00000770: 3a3a 2033 2e38 0a52 6571 7569 7265 732d  :: 3.8.Requires-
+00000780: 5079 7468 6f6e 3a20 3e3d 332e 372e 3130  Python: >=3.7.10
+00000790: 0a44 6573 6372 6970 7469 6f6e 2d43 6f6e  .Description-Con
+000007a0: 7465 6e74 2d54 7970 653a 2074 6578 742f  tent-Type: text/
+000007b0: 6d61 726b 646f 776e 0a4c 6963 656e 7365  markdown.License
+000007c0: 2d46 696c 653a 204c 4943 454e 5345 0a52  -File: LICENSE.R
+000007d0: 6571 7569 7265 732d 4469 7374 3a20 6176  equires-Dist: av
+000007e0: 2028 3e3d 3130 2e30 2e30 290a 5265 7175   (>=10.0.0).Requ
+000007f0: 6972 6573 2d44 6973 743a 2063 6c6f 7564  ires-Dist: cloud
+00000800: 7069 636b 6c65 2028 3e3d 322e 322e 3129  pickle (>=2.2.1)
+00000810: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00000820: 646f 636b 6572 2028 3e3d 362e 302e 3029  docker (>=6.0.0)
+00000830: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00000840: 6772 7063 696f 2d74 6f6f 6c73 2028 3c3d  grpcio-tools (<=
+00000850: 312e 3439 2e31 2c3e 3d31 2e33 322e 3029  1.49.1,>=1.32.0)
+00000860: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00000870: 6c6f 6775 7275 2028 3e3d 302e 372e 3029  loguru (>=0.7.0)
+00000880: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00000890: 6f70 656e 6376 2d70 7974 686f 6e2d 6865  opencv-python-he
+000008a0: 6164 6c65 7373 2028 3e3d 342e 362e 302e  adless (>=4.6.0.
+000008b0: 3636 290a 5265 7175 6972 6573 2d44 6973  66).Requires-Dis
+000008c0: 743a 2070 616e 6461 730a 5265 7175 6972  t: pandas.Requir
+000008d0: 6573 2d44 6973 743a 2050 696c 6c6f 770a  es-Dist: Pillow.
+000008e0: 5265 7175 6972 6573 2d44 6973 743a 2070  Requires-Dist: p
+000008f0: 7375 7469 6c20 283e 3d35 2e39 2e35 290a  sutil (>=5.9.5).
+00000900: 5265 7175 6972 6573 2d44 6973 743a 2070  Requires-Dist: p
+00000910: 792d 6370 7569 6e66 6f20 283e 3d39 2e30  y-cpuinfo (>=9.0
+00000920: 2e30 290a 5265 7175 6972 6573 2d44 6973  .0).Requires-Dis
+00000930: 743a 2070 7964 616e 7469 630a 5265 7175  t: pydantic.Requ
+00000940: 6972 6573 2d44 6973 743a 2072 6963 6820  ires-Dist: rich 
+00000950: 283e 3d31 322e 352e 3129 0a52 6571 7569  (>=12.5.1).Requi
+00000960: 7265 732d 4469 7374 3a20 7471 646d 0a52  res-Dist: tqdm.R
+00000970: 6571 7569 7265 732d 4469 7374 3a20 7479  equires-Dist: ty
+00000980: 7065 7220 283e 3d30 2e37 2e30 290a 5265  per (>=0.7.0).Re
+00000990: 7175 6972 6573 2d44 6973 743a 2074 7970  quires-Dist: typ
+000009a0: 696e 672d 6578 7465 6e73 696f 6e73 2028  ing-extensions (
+000009b0: 3e3d 342e 352e 3029 0a50 726f 7669 6465  >=4.5.0).Provide
+000009c0: 732d 4578 7472 613a 2064 6576 0a52 6571  s-Extra: dev.Req
+000009d0: 7569 7265 732d 4469 7374 3a20 626c 6163  uires-Dist: blac
+000009e0: 6b5b 6a75 7079 7465 725d 2028 3d3d 3232  k[jupyter] (==22
+000009f0: 2e33 2e30 2920 3b20 6578 7472 6120 3d3d  .3.0) ; extra ==
+00000a00: 2027 6465 7627 0a52 6571 7569 7265 732d   'dev'.Requires-
+00000a10: 4469 7374 3a20 6275 696c 6420 283d 3d30  Dist: build (==0
+00000a20: 2e31 302e 3029 203b 2065 7874 7261 203d  .10.0) ; extra =
+00000a30: 3d20 2764 6576 270a 5265 7175 6972 6573  = 'dev'.Requires
+00000a40: 2d44 6973 743a 2070 7265 2d63 6f6d 6d69  -Dist: pre-commi
+00000a50: 7420 283d 3d33 2e32 2e32 2920 3b20 6578  t (==3.2.2) ; ex
+00000a60: 7472 6120 3d3d 2027 6465 7627 0a52 6571  tra == 'dev'.Req
+00000a70: 7569 7265 732d 4469 7374 3a20 7275 6666  uires-Dist: ruff
+00000a80: 2028 3d3d 302e 302e 3236 3229 203b 2065   (==0.0.262) ; e
+00000a90: 7874 7261 203d 3d20 2764 6576 270a 5072  xtra == 'dev'.Pr
+00000aa0: 6f76 6964 6573 2d45 7874 7261 3a20 646f  ovides-Extra: do
+00000ab0: 6373 0a52 6571 7569 7265 732d 4469 7374  cs.Requires-Dist
+00000ac0: 3a20 6d6b 646f 6373 203b 2065 7874 7261  : mkdocs ; extra
+00000ad0: 203d 3d20 2764 6f63 7327 0a52 6571 7569   == 'docs'.Requi
+00000ae0: 7265 732d 4469 7374 3a20 6d6b 646f 6373  res-Dist: mkdocs
+00000af0: 2d65 7863 6c75 6465 203b 2065 7874 7261  -exclude ; extra
+00000b00: 203d 3d20 2764 6f63 7327 0a52 6571 7569   == 'docs'.Requi
+00000b10: 7265 732d 4469 7374 3a20 6d6b 646f 6373  res-Dist: mkdocs
+00000b20: 2d69 6e63 6c75 6465 2d6d 6172 6b64 6f77  -include-markdow
+00000b30: 6e2d 706c 7567 696e 203b 2065 7874 7261  n-plugin ; extra
+00000b40: 203d 3d20 2764 6f63 7327 0a52 6571 7569   == 'docs'.Requi
+00000b50: 7265 732d 4469 7374 3a20 6d6b 646f 6373  res-Dist: mkdocs
+00000b60: 2d6d 6174 6572 6961 6c20 3b20 6578 7472  -material ; extr
+00000b70: 6120 3d3d 2027 646f 6373 270a 5265 7175  a == 'docs'.Requ
+00000b80: 6972 6573 2d44 6973 743a 206d 6b64 6f63  ires-Dist: mkdoc
+00000b90: 732d 7079 6d64 6f77 6e78 2d6d 6174 6572  s-pymdownx-mater
+00000ba0: 6961 6c2d 6578 7472 6173 203b 2065 7874  ial-extras ; ext
+00000bb0: 7261 203d 3d20 2764 6f63 7327 0a52 6571  ra == 'docs'.Req
+00000bc0: 7569 7265 732d 4469 7374 3a20 6d6b 646f  uires-Dist: mkdo
+00000bd0: 6373 2d73 616d 652d 6469 7220 3b20 6578  cs-same-dir ; ex
+00000be0: 7472 6120 3d3d 2027 646f 6373 270a 5265  tra == 'docs'.Re
+00000bf0: 7175 6972 6573 2d44 6973 743a 206d 6b64  quires-Dist: mkd
+00000c00: 6f63 7374 7269 6e67 7320 3b20 6578 7472  ocstrings ; extr
+00000c10: 6120 3d3d 2027 646f 6373 270a 5265 7175  a == 'docs'.Requ
+00000c20: 6972 6573 2d44 6973 743a 206d 6b64 6f63  ires-Dist: mkdoc
+00000c30: 7374 7269 6e67 732d 7079 7468 6f6e 203b  strings-python ;
+00000c40: 2065 7874 7261 203d 3d20 2764 6f63 7327   extra == 'docs'
+00000c50: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00000c60: 6d6b 646f 6373 7472 696e 6773 5b70 7974  mkdocstrings[pyt
+00000c70: 686f 6e5d 203b 2065 7874 7261 203d 3d20  hon] ; extra == 
+00000c80: 2764 6f63 7327 0a50 726f 7669 6465 732d  'docs'.Provides-
+00000c90: 4578 7472 613a 2065 7874 7261 730a 5265  Extra: extras.Re
+00000ca0: 7175 6972 6573 2d44 6973 743a 2063 6f6e  quires-Dist: con
+00000cb0: 7472 6f6c 6e65 742d 6175 7820 283d 3d30  trolnet-aux (==0
+00000cc0: 2e30 2e36 2920 3b20 6578 7472 6120 3d3d  .0.6) ; extra ==
+00000cd0: 2027 6578 7472 6173 270a 5265 7175 6972   'extras'.Requir
+00000ce0: 6573 2d44 6973 743a 206d 6174 706c 6f74  es-Dist: matplot
+00000cf0: 6c69 6220 3b20 6578 7472 6120 3d3d 2027  lib ; extra == '
+00000d00: 6578 7472 6173 270a 5265 7175 6972 6573  extras'.Requires
+00000d10: 2d44 6973 743a 206f 7065 6e63 762d 636f  -Dist: opencv-co
+00000d20: 6e74 7269 622d 7079 7468 6f6e 2d68 6561  ntrib-python-hea
+00000d30: 646c 6573 7320 3b20 6578 7472 6120 3d3d  dless ; extra ==
+00000d40: 2027 6578 7472 6173 270a 5072 6f76 6964   'extras'.Provid
+00000d50: 6573 2d45 7874 7261 3a20 7365 7276 6572  es-Extra: server
+00000d60: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00000d70: 6163 6365 6c65 7261 7465 2028 3e30 2e31  accelerate (>0.1
+00000d80: 382e 3029 203b 2065 7874 7261 203d 3d20  8.0) ; extra == 
+00000d90: 2773 6572 7665 7227 0a52 6571 7569 7265  'server'.Require
+00000da0: 732d 4469 7374 3a20 6469 6666 7573 6572  s-Dist: diffuser
+00000db0: 7320 283e 3d30 2e31 372e 3129 203b 2065  s (>=0.17.1) ; e
+00000dc0: 7874 7261 203d 3d20 2773 6572 7665 7227  xtra == 'server'
+00000dd0: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00000de0: 6875 6767 696e 6766 6163 652d 6875 6220  huggingface-hub 
+00000df0: 3b20 6578 7472 6120 3d3d 2027 7365 7276  ; extra == 'serv
+00000e00: 6572 270a 5265 7175 6972 6573 2d44 6973  er'.Requires-Dis
+00000e10: 743a 2070 7961 7272 6f77 2028 3e3d 3132  t: pyarrow (>=12
+00000e20: 2e30 2e30 2920 3b20 6578 7472 6120 3d3d  .0.0) ; extra ==
+00000e30: 2027 7365 7276 6572 270a 5265 7175 6972   'server'.Requir
+00000e40: 6573 2d44 6973 743a 2072 6179 5b73 6572  es-Dist: ray[ser
+00000e50: 7665 5d20 283d 3d32 2e35 2e30 2920 3b20  ve] (==2.5.0) ; 
+00000e60: 6578 7472 6120 3d3d 2027 7365 7276 6572  extra == 'server
+00000e70: 270a 5265 7175 6972 6573 2d44 6973 743a  '.Requires-Dist:
+00000e80: 2074 6162 756c 6174 6520 3b20 6578 7472   tabulate ; extr
+00000e90: 6120 3d3d 2027 7365 7276 6572 270a 5265  a == 'server'.Re
+00000ea0: 7175 6972 6573 2d44 6973 743a 2074 7261  quires-Dist: tra
+00000eb0: 6e73 666f 726d 6572 7320 283e 3d34 2e32  nsformers (>=4.2
+00000ec0: 392e 3229 203b 2065 7874 7261 203d 3d20  9.2) ; extra == 
+00000ed0: 2773 6572 7665 7227 0a52 6571 7569 7265  'server'.Require
+00000ee0: 732d 4469 7374 3a20 746f 7263 6820 283c  s-Dist: torch (<
+00000ef0: 322e 302e 302c 3e3d 312e 3132 2e31 2920  2.0.0,>=1.12.1) 
+00000f00: 3b20 6578 7472 6120 3d3d 2027 7365 7276  ; extra == 'serv
+00000f10: 6572 270a 5265 7175 6972 6573 2d44 6973  er'.Requires-Dis
+00000f20: 743a 2074 6f72 6368 6175 6469 6f20 283e  t: torchaudio (>
+00000f30: 3d30 2e31 332e 3129 203b 2065 7874 7261  =0.13.1) ; extra
+00000f40: 203d 3d20 2773 6572 7665 7227 0a52 6571   == 'server'.Req
+00000f50: 7569 7265 732d 4469 7374 3a20 746f 7263  uires-Dist: torc
+00000f60: 6876 6973 696f 6e20 283e 3d30 2e31 332e  hvision (>=0.13.
+00000f70: 3029 203b 2065 7874 7261 203d 3d20 2773  0) ; extra == 's
+00000f80: 6572 7665 7227 0a52 6571 7569 7265 732d  erver'.Requires-
+00000f90: 4469 7374 3a20 636f 6e74 726f 6c6e 6574  Dist: controlnet
+00000fa0: 2d61 7578 2028 3d3d 302e 302e 3629 203b  -aux (==0.0.6) ;
+00000fb0: 2065 7874 7261 203d 3d20 2773 6572 7665   extra == 'serve
+00000fc0: 7227 0a52 6571 7569 7265 732d 4469 7374  r'.Requires-Dist
+00000fd0: 3a20 6d61 7470 6c6f 746c 6962 203b 2065  : matplotlib ; e
+00000fe0: 7874 7261 203d 3d20 2773 6572 7665 7227  xtra == 'server'
+00000ff0: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+00001000: 6f70 656e 6376 2d63 6f6e 7472 6962 2d70  opencv-contrib-p
+00001010: 7974 686f 6e2d 6865 6164 6c65 7373 203b  ython-headless ;
+00001020: 2065 7874 7261 203d 3d20 2773 6572 7665   extra == 'serve
+00001030: 7227 0a50 726f 7669 6465 732d 4578 7472  r'.Provides-Extr
+00001040: 613a 2074 6573 740a 5265 7175 6972 6573  a: test.Requires
+00001050: 2d44 6973 743a 2070 7974 6573 7420 3b20  -Dist: pytest ; 
+00001060: 6578 7472 6120 3d3d 2027 7465 7374 270a  extra == 'test'.
+00001070: 5265 7175 6972 6573 2d44 6973 743a 2070  Requires-Dist: p
+00001080: 7974 6573 742d 6173 796e 6369 6f20 3b20  ytest-asyncio ; 
+00001090: 6578 7472 6120 3d3d 2027 7465 7374 270a  extra == 'test'.
+000010a0: 5265 7175 6972 6573 2d44 6973 743a 2074  Requires-Dist: t
+000010b0: 7970 6567 7561 7264 203b 2065 7874 7261  ypeguard ; extra
+000010c0: 203d 3d20 2774 6573 7427 0a50 726f 7669   == 'test'.Provi
+000010d0: 6465 732d 4578 7472 613a 2074 6f72 6368  des-Extra: torch
+000010e0: 0a52 6571 7569 7265 732d 4469 7374 3a20  .Requires-Dist: 
+000010f0: 746f 7263 6820 283c 322e 302e 302c 3e3d  torch (<2.0.0,>=
+00001100: 312e 3132 2e31 2920 3b20 6578 7472 6120  1.12.1) ; extra 
+00001110: 3d3d 2027 746f 7263 6827 0a52 6571 7569  == 'torch'.Requi
+00001120: 7265 732d 4469 7374 3a20 746f 7263 6861  res-Dist: torcha
+00001130: 7564 696f 2028 3e3d 302e 3133 2e31 2920  udio (>=0.13.1) 
+00001140: 3b20 6578 7472 6120 3d3d 2027 746f 7263  ; extra == 'torc
+00001150: 6827 0a52 6571 7569 7265 732d 4469 7374  h'.Requires-Dist
+00001160: 3a20 746f 7263 6876 6973 696f 6e20 283e  : torchvision (>
+00001170: 3d30 2e31 332e 3029 203b 2065 7874 7261  =0.13.0) ; extra
+00001180: 203d 3d20 2774 6f72 6368 270a 0a23 206e   == 'torch'..# n
+00001190: 6f73 20f0 9f94 a53a 204e 6974 726f 7573  os ....: Nitrous
+000011a0: 204f 7869 6465 2053 7973 7465 6d20 284e   Oxide System (N
+000011b0: 4f53 2920 666f 7220 436f 6d70 7574 6572  OS) for Computer
+000011c0: 2056 6973 696f 6e0a 0a3c 7020 616c 6967   Vision..<p alig
+000011d0: 6e3d 2263 656e 7465 7222 3e0a 2020 2020  n="center">.    
+000011e0: 3c61 2068 7265 663d 2268 7474 7073 3a2f  <a href="https:/
+000011f0: 2f70 7970 692e 6f72 672f 7072 6f6a 6563  /pypi.org/projec
+00001200: 742f 6175 746f 6e6f 6d69 2d6e 6f73 2f22  t/autonomi-nos/"
+00001210: 3e0a 2020 2020 2020 2020 3c69 6d67 2061  >.        <img a
+00001220: 6c74 3d22 5079 5069 2056 6572 7369 6f6e  lt="PyPi Version
+00001230: 2220 7372 633d 2268 7474 7073 3a2f 2f62  " src="https://b
+00001240: 6164 6765 2e66 7572 792e 696f 2f70 792f  adge.fury.io/py/
+00001250: 6175 746f 6e6f 6d69 2d6e 6f73 2e73 7667  autonomi-nos.svg
+00001260: 223e 0a20 2020 203c 2f61 3e0a 2020 2020  ">.    </a>.    
+00001270: 3c61 2068 7265 663d 2268 7474 7073 3a2f  <a href="https:/
+00001280: 2f70 7970 692e 6f72 672f 7072 6f6a 6563  /pypi.org/projec
+00001290: 742f 6175 746f 6e6f 6d69 2d6e 6f73 2f22  t/autonomi-nos/"
+000012a0: 3e0a 2020 2020 2020 2020 3c69 6d67 2061  >.        <img a
+000012b0: 6c74 3d22 5079 5069 2056 6572 7369 6f6e  lt="PyPi Version
+000012c0: 2220 7372 633d 2268 7474 7073 3a2f 2f69  " src="https://i
+000012d0: 6d67 2e73 6869 656c 6473 2e69 6f2f 7079  mg.shields.io/py
+000012e0: 7069 2f70 7976 6572 7369 6f6e 732f 6175  pi/pyversions/au
+000012f0: 746f 6e6f 6d69 2d6e 6f73 223e 0a20 2020  tonomi-nos">.   
+00001300: 203c 2f61 3e0a 2020 2020 3c61 2068 7265   </a>.    <a hre
+00001310: 663d 2268 7474 7073 3a2f 2f70 7970 692e  f="https://pypi.
+00001320: 6f72 672f 7072 6f6a 6563 742f 6175 746f  org/project/auto
+00001330: 6e6f 6d69 2d6e 6f73 2f22 3e0a 2020 2020  nomi-nos/">.    
+00001340: 2020 2020 3c69 6d67 2061 6c74 3d22 5079      <img alt="Py
+00001350: 5069 2044 6f77 6e6c 6f61 6473 2220 7372  Pi Downloads" sr
+00001360: 633d 2268 7474 7073 3a2f 2f69 6d67 2e73  c="https://img.s
+00001370: 6869 656c 6473 2e69 6f2f 7079 7069 2f64  hields.io/pypi/d
+00001380: 6d2f 6175 746f 6e6f 6d69 2d6e 6f73 223e  m/autonomi-nos">
+00001390: 0a20 2020 203c 2f61 3e0a 2020 2020 3c61  .    </a>.    <a
+000013a0: 2068 7265 663d 2268 7474 7073 3a2f 2f64   href="https://d
+000013b0: 6973 636f 7264 2e67 672f 5141 4767 7654  iscord.gg/QAGgvT
+000013c0: 7576 6767 223e 0a20 2020 2020 2020 203c  uvgg">.        <
+000013d0: 696d 6720 616c 743d 2244 6973 636f 7264  img alt="Discord
+000013e0: 2220 7372 633d 2268 7474 7073 3a2f 2f69  " src="https://i
+000013f0: 6d67 2e73 6869 656c 6473 2e69 6f2f 6261  mg.shields.io/ba
+00001400: 6467 652f 6469 7363 6f72 642d 6368 6174  dge/discord-chat
+00001410: 2d70 7572 706c 653f 636f 6c6f 723d 2532  -purple?color=%2
+00001420: 3335 3736 3546 3226 6c61 6265 6c3d 6469  35765F2&label=di
+00001430: 7363 6f72 6426 6c6f 676f 3d64 6973 636f  scord&logo=disco
+00001440: 7264 223e 0a20 2020 203c 2f61 3e0a 3c2f  rd">.    </a>.</
+00001450: 703e 0a0a 2a2a 4e4f 532a 2a20 6973 2061  p>..**NOS** is a
+00001460: 2050 7954 6f72 6368 206c 6962 7261 7279   PyTorch library
+00001470: 2066 6f72 206f 7074 696d 697a 696e 6720   for optimizing 
+00001480: 616e 6420 7275 6e6e 696e 6720 6c69 6768  and running ligh
+00001490: 746e 696e 672d 6661 7374 2069 6e66 6572  tning-fast infer
+000014a0: 656e 6365 206f 6620 706f 7075 6c61 7220  ence of popular 
+000014b0: 636f 6d70 7574 6572 2076 6973 696f 6e20  computer vision 
+000014c0: 6d6f 6465 6c73 2e20 4e4f 5320 696e 6865  models. NOS inhe
+000014d0: 7269 7473 2069 7473 206e 616d 6520 6672  rits its name fr
+000014e0: 6f6d 2022 4e69 7472 6f75 7320 4f78 6964  om "Nitrous Oxid
+000014f0: 6520 5379 7374 656d 222c 2074 6865 2070  e System", the p
+00001500: 6572 666f 726d 616e 6365 2d65 6e68 616e  erformance-enhan
+00001510: 6369 6e67 2073 7973 7465 6d20 7479 7069  cing system typi
+00001520: 6361 6c6c 7920 7573 6564 2069 6e20 7261  cally used in ra
+00001530: 6369 6e67 2063 6172 732e 204e 4f53 2069  cing cars. NOS i
+00001540: 7320 6465 7369 676e 6564 2074 6f20 6265  s designed to be
+00001550: 206d 6f64 756c 6172 2061 6e64 2065 6173   modular and eas
+00001560: 7920 746f 2065 7874 656e 642e 0a0a 2323  y to extend...##
+00001570: 2057 6879 204e 4f53 3f0a 2d20 e29a a1ef   Why NOS?.- ....
+00001580: b88f 202a 2a46 6173 742a 2a3a 2042 7569  .. **Fast**: Bui
+00001590: 6c74 2066 6f72 2050 7954 6f72 6368 2061  lt for PyTorch a
+000015a0: 6e64 2064 6573 6967 6e65 6420 746f 206f  nd designed to o
+000015b0: 7074 696d 697a 652f 7275 6e20 6d6f 6465  ptimize/run mode
+000015c0: 6c73 2066 6173 7465 720a 2d20 f09f 94a5  ls faster.- ....
+000015d0: 202a 2a50 6572 666f 726d 616e 742a 2a3a   **Performant**:
+000015e0: 2052 756e 206d 6f64 656c 7320 7375 6368   Run models such
+000015f0: 2061 7320 5344 7632 206f 7220 6f62 6a65   as SDv2 or obje
+00001600: 6374 2064 6574 6563 7469 6f6e 2032 2d33  ct detection 2-3
+00001610: 7820 6661 7374 6572 206f 7574 2d6f 662d  x faster out-of-
+00001620: 7468 652d 626f 780a 2d20 f09f 91a9 e280  the-box.- ......
+00001630: 8df0 9f92 bb20 2a2a 4e6f 2050 6844 2072  ..... **No PhD r
+00001640: 6571 7569 7265 642a 2a3a 204f 7074 696d  equired**: Optim
+00001650: 697a 6520 6d6f 6465 6c73 2066 6f72 206d  ize models for m
+00001660: 6178 696d 756d 2048 5720 7065 7266 6f72  aximum HW perfor
+00001670: 6d61 6e63 6520 7769 7468 6f75 7420 6120  mance without a 
+00001680: 5068 4420 696e 204d 4c0a 2d20 f09f 93a6  PhD in ML.- ....
+00001690: 202a 2a45 7874 656e 7369 626c 652a 2a3a   **Extensible**:
+000016a0: 2045 6173 696c 7920 6164 6420 6f70 7469   Easily add opti
+000016b0: 6d69 7a61 7469 6f6e 2061 6e64 2048 572d  mization and HW-
+000016c0: 7375 7070 6f72 7420 666f 7220 6375 7374  support for cust
+000016d0: 6f6d 206d 6f64 656c 730a 2d20 e29a 99ef  om models.- ....
+000016e0: b88f 202a 2a48 572d 6163 6365 6c65 7261  .. **HW-accelera
+000016f0: 7465 643a 2a2a 2054 616b 6520 6675 6c6c  ted:** Take full
+00001700: 2061 6476 616e 7461 6765 206f 6620 796f   advantage of yo
+00001710: 7572 2048 5720 2847 5055 732c 2041 5349  ur HW (GPUs, ASI
+00001720: 4373 2920 7769 7468 6f75 7420 636f 6d70  Cs) without comp
+00001730: 726f 6d69 7365 0a2d 20e2 9881 efb8 8f20  romise.- ...... 
+00001740: 2a2a 436c 6f75 642d 6167 6e6f 7374 6963  **Cloud-agnostic
+00001750: 3a2a 2a20 5275 6e20 6f6e 2061 6e79 2063  :** Run on any c
+00001760: 6c6f 7564 2048 5720 2841 5753 2c20 4743  loud HW (AWS, GC
+00001770: 502c 2041 7a75 7265 2c20 4c61 6d62 6461  P, Azure, Lambda
+00001780: 204c 6162 732c 204f 6e2d 5072 656d 290a   Labs, On-Prem).
+00001790: 0a23 2320 4261 7474 6572 6965 7320 496e  .## Batteries In
+000017a0: 636c 7564 6564 0a20 2d20 f09f 92aa 202a  cluded. - .... *
+000017b0: 2a53 4f54 4120 4d6f 6465 6c20 5375 7070  *SOTA Model Supp
+000017c0: 6f72 743a 2a2a 204e 4f53 2070 726f 7669  ort:** NOS provi
+000017d0: 6465 7320 6f75 742d 6f66 2d74 6865 2d62  des out-of-the-b
+000017e0: 6f78 2073 7570 706f 7274 2066 6f72 2070  ox support for p
+000017f0: 6f70 756c 6172 2043 5620 6d6f 6465 6c73  opular CV models
+00001800: 2073 7563 6820 6173 205b 5374 6162 6c65   such as [Stable
+00001810: 2044 6966 6675 7369 6f6e 5d28 7374 6162   Diffusion](stab
+00001820: 696c 6974 7961 692f 7374 6162 6c65 2d64  ilityai/stable-d
+00001830: 6966 6675 7369 6f6e 2d32 292c 205b 4f70  iffusion-2), [Op
+00001840: 656e 4149 2043 4c49 505d 286f 7065 6e61  enAI CLIP](opena
+00001850: 692f 636c 6970 2d76 6974 2d62 6173 652d  i/clip-vit-base-
+00001860: 7061 7463 6833 3229 2c20 5b59 4f4c 4f58  patch32), [YOLOX
+00001870: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
+00001880: 2e63 6f6d 2f4d 6567 7669 692d 4261 7365  .com/Megvii-Base
+00001890: 4465 7465 6374 696f 6e2f 594f 4c4f 5829  Detection/YOLOX)
+000018a0: 206f 626a 6563 7420 6465 7465 6374 696f   object detectio
+000018b0: 6e2c 2074 7261 636b 696e 6720 616e 6420  n, tracking and 
+000018c0: 6d6f 7265 0a20 2d20 f09f 948c 202a 2a41  more. - .... **A
+000018d0: 5049 733a 2a2a 204e 4f53 2070 726f 7669  PIs:** NOS provi
+000018e0: 6465 7320 6f75 742d 6f66 2d74 6865 2d62  des out-of-the-b
+000018f0: 6f78 2041 5049 7320 616e 6420 6176 6f69  ox APIs and avoi
+00001900: 6473 2061 6c6c 2074 6865 204d 4c20 6d6f  ds all the ML mo
+00001910: 6465 6c20 6465 706c 6f79 6d65 6e74 2068  del deployment h
+00001920: 6173 736c 6573 0a20 2d20 f09f 90b3 202a  assles. - .... *
+00001930: 2a44 6f63 6b65 723a 2a2a 204e 4f53 2073  *Docker:** NOS s
+00001940: 6869 7073 2077 6974 6820 646f 636b 6572  hips with docker
+00001950: 2069 6d61 6765 7320 746f 2072 756e 2061   images to run a
+00001960: 6363 656c 6572 6174 6564 2061 6e64 2073  ccelerated and s
+00001970: 6361 6c61 626c 6520 4356 2077 6f72 6b6c  calable CV workl
+00001980: 6f61 6473 0a20 2d20 f09f 9388 202a 2a4d  oads. - .... **M
+00001990: 756c 7469 2d50 6c61 7466 6f72 6d2a 2a3a  ulti-Platform**:
+000019a0: 204e 4f53 2061 6c6c 6f77 7320 796f 7520   NOS allows you 
+000019b0: 746f 2072 756e 206d 6f64 656c 7320 6f6e  to run models on
+000019c0: 2064 6966 6665 7265 6e74 2048 5720 284e   different HW (N
+000019d0: 5649 4449 412c 2063 7573 746f 6d20 4153  VIDIA, custom AS
+000019e0: 4943 7329 2077 6974 686f 7574 2061 6e79  ICs) without any
+000019f0: 206d 6f64 656c 2063 6f6d 7069 6c61 7469   model compilati
+00001a00: 6f6e 206f 7220 7275 6e74 696d 6520 6d61  on or runtime ma
+00001a10: 6e61 6765 6d65 6e74 2e0a 0a0a 2323 2043  nagement....## C
+00001a20: 6f6e 7472 6962 7574 650a 5765 2077 656c  ontribute.We wel
+00001a30: 636f 6d65 2063 6f6e 7472 6962 7574 696f  come contributio
+00001a40: 6e73 2120 506c 6561 7365 2073 6565 206f  ns! Please see o
+00001a50: 7572 205b 636f 6e74 7269 6275 7469 6e67  ur [contributing
+00001a60: 2067 7569 6465 5d28 646f 6373 2f43 4f4e   guide](docs/CON
+00001a70: 5452 4942 5554 494e 472e 6d64 2920 666f  TRIBUTING.md) fo
+00001a80: 7220 6d6f 7265 2069 6e66 6f72 6d61 7469  r more informati
+00001a90: 6f6e 2e0a                                on..
```

## Comparing `autonomi_nos-0.0.5.dist-info/RECORD` & `autonomi_nos-0.0.6a1.dist-info/RECORD`

 * *Files 26% similar despite different names*

```diff
@@ -1,59 +1,64 @@
-nos/__init__.py,sha256=51Y_o2LsEI8SmKeCFOZfz1aTYOCqrv8plfrpDyr_ILo,50
+nos/__init__.py,sha256=hZuUzrXY9Px7KEt6deoV_-NKYiXLF_jtlQVA5hIPV0s,164
 nos/constants.py,sha256=j-nz48q31rE7eclMqkZH6xFJ0eGmtx2Jt5yr_wQA_1Y,561
 nos/exceptions.py,sha256=ocRCxTbzT_Q8WTobAbkbo4jl5a6F3xC_D2N7rtzfWeg,93
 nos/logging.py,sha256=D0QDNFGmAYshfnSC-ctEDSIeBO6TnLPz1babUmJGRSA,940
 nos/protoc.py,sha256=xUck1U30UqUEnTaFuWMGWagyDcIrjmcQo8xozqbypmI,2778
-nos/version.py,sha256=S7u1lbuWmM3A3ajykBialmPoJUK6Jg-WmNqM-9OZFdk,22
-nos/cli/benchmark.py,sha256=p-EGzEUDMFFv2hpB9Nn8-Xk1zW-hrYDaUzyNvWmSgzk,110
-nos/cli/cli.py,sha256=JlkhPPinuMH6MFwm2TA97cZOWqYKXjQEORJ8eBSfw9M,446
-nos/cli/docker.py,sha256=uPGrLe8FlvwSNFGUqp-vw5VAZhCeyvrfUfwJZdqXypk,1983
+nos/version.py,sha256=gXd1D56xj6WOvmCczvF5j2U21R-6e_8jAsOhKTHAwGk,24
+nos/cli/cli.py,sha256=59Q5GUat7rQsNZco1JX27l1W6KCYfri8ctAGvOuFzec,369
+nos/cli/docker.py,sha256=SDGg8b343r6ndzmyESGbHLQx7EjGwkV2Plo025Y-U5A,4230
 nos/cli/hub.py,sha256=USdzVgaZlPZABrrIZzCA7ySwXg1iseEk7GJ7iwanXIg,1294
 nos/cli/predict.py,sha256=eo9Fc2ZmzaA6z1QenacKIYHMX4DYwCnIQHf3XTpyMho,7543
 nos/cli/serve_http.py,sha256=xMwSE46-_uHLgif1gnJCRoLbBE3ttC2zkm2JXVooWg0,5962
-nos/cli/system.py,sha256=GjB_Nb0t1FYT-X9Nu5eigbaAIC5lx5PblQG_tTBQDxc,3469
+nos/cli/system.py,sha256=MyVBcPZDlpiAJHhdK4JiFKyTJfD1iez3JzLvG6lsMAw,1713
 nos/cli/utils.py,sha256=uR1lGZyyDEuflNvxKuAmVUP-DTE55PzZL5c0c3l2h4U,425
 nos/client/__init__.py,sha256=oXG9pYr-XdtjExlDenSsleOzeCBzLGYIWwzD9-rkm6o,396
 nos/client/exceptions.py,sha256=Lqqeu_6oNSZgOUipXHFx20yeASJi9bEewpG1UYuy1gk,92
-nos/client/grpc.py,sha256=cKEStHfNqvu1dyaSaS3oNHfkyWFsLVOW3zuzEdqos7A,12309
-nos/common/__init__.py,sha256=Jxs4Gvl5H4Nx9QRjaP7mUYLhL2XjtTs1tZXpp_y64dY,275
+nos/client/grpc.py,sha256=T0Rq3YEy9RQ8t-6Kjsbyn5vd1MxoxowKLNhiuqB4RBQ,12378
+nos/common/__init__.py,sha256=I4hKyXjU0xs80tbzE4sjDLAiOCFUci67xBpYMkhsa4Y,1285
 nos/common/cloudpickle.py,sha256=2VBtGaLHbVtKg9ICT-xUITweHQCd6PxsFobDKwSYE2I,204
-nos/common/spec.py,sha256=ItoK2g6rXNFXIo9MDuhodB0j76P9mSfHBNvlCSd1LX8,9360
+nos/common/profiler.py,sha256=8Q82nyNd81xHAQAj1MMdc1t9tfdTygTHmiNLPMdY-KY,2712
+nos/common/spec.py,sha256=_mcwc_OhflG19OwT5SkOtTsHDRuiceJKixOqf-HKI2g,9446
+nos/common/system.py,sha256=OpcOu91GsNbESiY6KADz4RRn9uk4xKLF0AtYbG3MvME,6076
 nos/common/tasks.py,sha256=AcEbh5gbpKt_77Ar9zRrRyiD8tyw264Wv2buyX9yxZ8,518
-nos/common/types.py,sha256=H026vunyM7tNfr4yphsV1VohhfWBQ8nPr4yINiuaIkA,5725
-nos/common/io/__init__.py,sha256=Ja7JJcLwpZBFR-SE20vQ_awfmYHhlelepnJZaQG-eNo,50
-nos/common/io/video/base.py,sha256=ACMNHGHSG4028Ra_ld5vhQQKcvu9FEm-TNHcUguct2I,1048
-nos/common/io/video/opencv.py,sha256=5ax7JgOelshe-__gDpgaljeK1sxGwYeZPBsm6MnHf3U,4272
+nos/common/types.py,sha256=MKNH-7qW8N1L-7nYH7b7NIgoSkt2JepcWxoLUvzX6QM,5984
+nos/common/io/__init__.py,sha256=r8y2TSd0Zzv7bsImJCSjP7S1RMckRFTPyGmKHQnxTgI,942
+nos/common/io/video/base.py,sha256=rAEBnj-LqkeqlCAXqGbAP49zKL3Q-m3sHhtLo6vh2ak,1050
+nos/common/io/video/opencv.py,sha256=s5CXfsbbk0IAXA42VvAFuaBO-d0nqaClZ2Qw7xU8taA,8424
+nos/compilers/__init__.py,sha256=viIY6XbOWebZnYFwKsgKxx9GbE5SXkn8alAjJbFTp_g,5832
+nos/compilers/trt/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+nos/compilers/trt/ops/group_norm.py,sha256=suIAbosSrDPMyKk_uuEhRPCggK73_wbE8YGSIMWanvk,2739
 nos/executors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-nos/executors/ray.py,sha256=aurGem2kgMMCdV-38kN6ozsNMbmKuIh4ywDEIuWNkBk,7401
-nos/hub/__init__.py,sha256=oQfRE2xCSGpn5PEjvzBWqMf4AHqfRKDOBZdqsIXxEIQ,3508
+nos/executors/ray.py,sha256=np0tmPRRRqVfO__ziKcYqPrZ_llLOSk_N8-BNXlzpOs,7544
+nos/hub/__init__.py,sha256=OOPhhVKINO8Jp-JZ6FmxsPfOW5k1ikauD-5k7Poi6Fk,3531
 nos/hub/config.py,sha256=DK0t6xrB6Sgf71YWnX_htNbbU4H2Bcdu9b7ujiA6QKY,2195
-nos/models/__init__.py,sha256=TVJu8Zsf3qdspn6YjTZV95TsKmt1R5g_OPKWvEwFC7g,316
-nos/models/clip.py,sha256=ehhS9G6KrLzd14GDPLQYjtOMAGqI8NfrVVIksJQgKDQ,3605
+nos/managers/__init__.py,sha256=ZLKQSo3Wj5B2fFyUiTTQlgQ97KWcpxJZtO27DseVbsU,59
+nos/managers/model.py,sha256=k7E-5wW7iJiCUNLfyM2DG3kZJ36Af8UX4e85jdmZJLQ,7911
+nos/models/__init__.py,sha256=uoT6_uSo7nN9DZE-z8xl-ul5hC2RUvqHHufHi7_axQQ,301
+nos/models/clip.py,sha256=N2QB4gWVEHqfBKoSdvX2o2nX9_9TCE7MySGRIj49IQA,9931
 nos/models/faster_rcnn.py,sha256=KF-NYOYQbyUHvhgkQKekSFbTnfxYSBHbN1N_uLkOxRA,3285
 nos/models/sam.py,sha256=bMGxM9unwmCA9TfHOI9IFwW5iLlOwxbO7OmcBCtLNJg,1507
-nos/models/stable_diffusion.py,sha256=q--CdCy0QLCEV5kfHOK8FOrUoQUhg63D6pVsdRnMLi4,3150
-nos/models/yolox.py,sha256=yWOAlzT7m1m2sAz3QygafBnQQ7s1gsWEkhYBHKitfns,6956
+nos/models/stable_diffusion.py,sha256=oCXjjwlMXssLS3f7-voN-2Sj-84fyW62-KEu7U1ZgJc,16095
+nos/models/yolox.py,sha256=wuOkm6jCmNLgZyu2Uad2WnW_YKlqqqYx7R19_nBGpGs,10128
 nos/models/openmmlab/__init__.py,sha256=VzlwVzMv9EVqdyKwvTDBaVhH9RYnYv0zEU_v4wEdR4w,63
-nos/models/openmmlab/mmdetection/detection_tensorrt_dynamic-320x320-1344x1344.py,sha256=YFx2dzUKL2MkS5mIqw6gDO1TxLkwLIWEFVOxwHIhnYg,66
 nos/models/openmmlab/mmdetection/mmdetection.py,sha256=gjWBD3t4CXEFd5t-k_qBHbk_Hi588nn-YGll68jM4Eo,3066
 nos/models/openmmlab/mmdetection/configs/_base_/default_runtime.py,sha256=tAbybUUg9TtBy_dqiXD5Zgl0kSs39JX7TiTeIYZrOUo,370
 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_detection.py,sha256=MxNi_Sf1m8CNkR64vZWaFN2O2P2LBI1-EexpLr166YU,3187
 nos/models/openmmlab/mmdetection/configs/_base_/datasets/coco_instance.py,sha256=DVkj2zskGrXxiF6hVQ1ofsOsqQGHsdjAh5sXRkaxjiw,1765
 nos/models/openmmlab/mmdetection/configs/_base_/models/faster-rcnn_r50_fpn.py,sha256=Al2qpY2P-MzsfBCod5pjB6HrH7ZzFvQxICc1CVmpnRY,3828
 nos/models/openmmlab/mmdetection/configs/_base_/schedules/schedule_1x.py,sha256=G8gXisLhM7mBRlxrhTOLCsWD17zCyH7kBvVi7J4BICI,304
 nos/models/openmmlab/mmdetection/configs/efficientdet/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py,sha256=RSkd9o1IO-YkWhIwnXU83tRyGeySiR1IMlSk14dTw9s,5340
 nos/models/openmmlab/mmdetection/configs/faster-rcnn/faster-rcnn_r50_fpn_1x_coco.py,sha256=Cm8lyQ3wOT5I9KR0m1TfTsPAGybs-buILRPLDZKzOPQ,177
 nos/proto/nos_service.proto,sha256=BEWdRsehHLaSjH2K2QEhtmaDqkwDZYdxKm1t4yg_0V8,2437
-nos/server/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-nos/server/docker.py,sha256=LKOqWWka7AdVTviIHqQqktJOntJovwJ5wkyP56ftTP4,6327
-nos/server/runtime.py,sha256=sD6CVHhegtg6VI7sQmXr5coLDgi5bJ1n6a0NQQ83HhQ,5502
-nos/server/service.py,sha256=AU6Kcnc9y6dfc4HmShI-pV1ljMD3iNVxpfNTogkEkes,13616
+nos/server/__init__.py,sha256=5RiKswYy4smsuFOVba9Rq3beaeI5Mifwxdfi0mODH5M,6193
+nos/server/docker.py,sha256=XvubrQzj1XgYH-QDOWPG5SIBy3fhoexFn_mW6bKZk_M,6556
+nos/server/runtime.py,sha256=maiFx1bpygU17ZOdJr5T2wu5kRahM4kP2R1B2Uj7X04,6363
+nos/server/service.py,sha256=k2Xkv-uNnEm9AlMGcSHy-dL81uKQqyyf8O2u__ak3-4,7326
 nos/test/benchmark.py,sha256=b_QMHfStY5iRjHGPZwaY7VrXzy_VeFKfjld9UEVbn-g,373
-nos/test/conftest.py,sha256=06B3HO_HY2aA1uGKwuILw5rooet1iA1ZR3jxbpLaqUw,4244
-nos/test/utils.py,sha256=Ihisq7DD-BXKeD7VE3ZzVf3eErLR_GOJ5Uu_VbsRHAo,1888
-autonomi_nos-0.0.5.dist-info/LICENSE,sha256=9TQFxQ2AkXOQuIHy9GueB_a18hayRXT7pDt9fJv9WLo,1068
-autonomi_nos-0.0.5.dist-info/METADATA,sha256=5SNNMWwZFmGNWsMBvkZFuTAY3FLZJiIAsdO7Oa1nIvc,6299
-autonomi_nos-0.0.5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-autonomi_nos-0.0.5.dist-info/entry_points.txt,sha256=UYtJAmFT3RPWmlKM11MMZvZPRHTeKyIh1BbZ3QpbsJs,86
-autonomi_nos-0.0.5.dist-info/top_level.txt,sha256=Tgqk49XI1nXvi6W_Ryy7_YwQ7iFU-mAlIsbNMR1HS6s,4
-autonomi_nos-0.0.5.dist-info/RECORD,,
+nos/test/conftest.py,sha256=-NWElgDWfFZpYwmGk1WpSB9JCAtZz3gKx1DJhDivoPs,4252
+nos/test/utils.py,sha256=bi-aoXsTmDiwmwlWVyhznW_ZS7k7N9bZJuq-QuMz2OA,2626
+autonomi_nos-0.0.6a1.dist-info/LICENSE,sha256=9TQFxQ2AkXOQuIHy9GueB_a18hayRXT7pDt9fJv9WLo,1068
+autonomi_nos-0.0.6a1.dist-info/METADATA,sha256=A1QLJKOL6vuRkxoahpJfWCNkhWVReb6qYaAF3zOTcgM,6804
+autonomi_nos-0.0.6a1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+autonomi_nos-0.0.6a1.dist-info/entry_points.txt,sha256=UYtJAmFT3RPWmlKM11MMZvZPRHTeKyIh1BbZ3QpbsJs,86
+autonomi_nos-0.0.6a1.dist-info/top_level.txt,sha256=Tgqk49XI1nXvi6W_Ryy7_YwQ7iFU-mAlIsbNMR1HS6s,4
+autonomi_nos-0.0.6a1.dist-info/RECORD,,
```

