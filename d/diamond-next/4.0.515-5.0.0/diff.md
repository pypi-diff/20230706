# Comparing `tmp/diamond-next-4.0.515.tar.gz` & `tmp/diamond-next-5.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/diamond-next-4.0.515.tar", last modified: Tue Feb 19 19:06:02 2019, max compression
+gzip compressed data, was "diamond-next-5.0.0.tar", last modified: Thu Jul  6 10:59:42 2023, max compression
```

## Comparing `diamond-next-4.0.515.tar` & `diamond-next-5.0.0.tar`

### file list

```diff
@@ -1,362 +1,389 @@
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        8 2019-02-19 19:06:02.000000 diamond-next-4.0.515/version.txt
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     4949 2019-02-19 19:00:15.000000 diamond-next-4.0.515/setup.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      378 2019-02-19 18:57:43.000000 diamond-next-4.0.515/MANIFEST.in
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 18:57:43.000000 diamond-next-4.0.515/.keep
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/rpm/
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/rpm/systemd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      207 2019-02-19 18:58:34.000000 diamond-next-4.0.515/rpm/systemd/diamond.service
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/rpm/upstart/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      297 2019-02-19 18:57:43.000000 diamond-next-4.0.515/rpm/upstart/diamond.conf
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      392 2019-02-19 19:06:02.000000 diamond-next-4.0.515/PKG-INFO
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      245 2019-02-19 18:57:43.000000 diamond-next-4.0.515/setup.cfg
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/bin/
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)    10724 2019-02-19 18:58:34.000000 diamond-next-4.0.515/bin/diamond
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/bin/init.d/
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     2041 2019-02-19 18:57:43.000000 diamond-next-4.0.515/bin/init.d/diamond
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     8102 2019-02-19 18:58:34.000000 diamond-next-4.0.515/bin/diamond-setup
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/diamond/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       40 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/diamond/version.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      222 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/error.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/diamond/utils/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      698 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/utils/signals.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3642 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/utils/scheduler.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/utils/__init__.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1975 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/utils/log.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7325 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/utils/classes.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4445 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/utils/config.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1353 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/util.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7493 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/convertor.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       49 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/__init__.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7357 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/gmetric.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8185 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/server.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5762 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/metric.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/diamond/handler/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2577 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/datadog.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2501 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/hostedgraphite.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4623 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/libratohandler.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      896 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/null.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5740 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/mqtt.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6655 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/rabbitmq_pubsub.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4688 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/rabbitmq_topic.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5910 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/tsdb.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2412 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/multigraphite.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2969 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/riemann.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5987 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/statsite.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2192 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/archive.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4013 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/signalfx.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2494 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/multigraphitepickle.py
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     1629 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/httpHandler.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2339 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/logentries_diamond.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6131 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/cloudwatch.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1365 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/queue.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2931 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/mysql.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3000 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/graphitepickle.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2760 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/g_metric.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       15 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/__init__.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4833 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/stats_d.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10750 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/sentry.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1993 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/zmq_pubsub.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7273 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/influxdbHandler.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7698 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/rrdtool.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4249 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/handler/Handler.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9334 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/diamond/handler/graphite.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    18819 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/diamond/collector.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/kvm/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1279 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/kvm/kvm.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/tcp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    12385 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/tcp/tcp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/smart/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3688 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/smart/smart.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/postfix/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3729 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/postfix/postfix.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/sqs/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2330 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/sqs/sqs.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/jbossapi/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    15256 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/jbossapi/jbossapi.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/numa/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1615 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/numa/numa.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/users/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1906 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/users/users.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/lmsensors/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2219 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/lmsensors/lmsensors.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/haproxy/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6290 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/haproxy/haproxy.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/kafkastat/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4706 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/kafkastat/kafkastat.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/puppetdb/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6748 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/puppetdb/puppetdb.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/exim/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1658 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/exim/exim.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ping/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2130 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/ping/ping.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/zookeeper/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4574 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/zookeeper/zookeeper.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/nagiosperfdata/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9230 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/nagiosperfdata/nagiosperfdata.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/vmsdoms/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3218 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/vmsdoms/vmsdoms.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/onewire/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2416 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/onewire/onewire.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/cephstats/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1581 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/cephstats/cephstats.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/gridengine/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4889 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/gridengine/gridengine.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/icinga_stats/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    14634 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/icinga_stats/icinga_stats.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/dseopscenter/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7115 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/dseopscenter/dseopscenter.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/libvirtkvm/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5375 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/libvirtkvm/libvirtkvm.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/proc/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2023 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/proc/proc.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/eventstoreprojections/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4414 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/eventstoreprojections/eventstoreprojections.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/eventstoreprojections/tests/
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/eventstoreprojections/tests/fixtures/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5794 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/eventstoreprojections/tests/fixtures/projections
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/beanstalkd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2524 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/beanstalkd/beanstalkd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/vmstat/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1698 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/vmstat/vmstat.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/loadavg/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2175 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/loadavg/loadavg.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/scribe/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2069 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/scribe/scribe.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/squid/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3054 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/squid/squid.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ipmisensor/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4747 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/ipmisensor/ipmisensor.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ipvs/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5817 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/ipvs/ipvs.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/varnish/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8453 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/varnish/varnish.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/files/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1995 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/files/files.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/http/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3120 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/http/http.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/pgbouncer/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3642 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/pgbouncer/pgbouncer.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ossec/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2344 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/ossec/ossec.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/disktemp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3036 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/disktemp/disktemp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/example/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3103 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/example/example.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/sockstat/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1714 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/sockstat/sockstat.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/puppetdashboard/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1698 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/puppetdashboard/puppetdashboard.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/vmsfs/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3023 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/vmsfs/vmsfs.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/nginx/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3924 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/nginx/nginx.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/network/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4536 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/network/network.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/memory_cgroup/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3634 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/memory_cgroup/memory_cgroup.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/portstat/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1984 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/portstat/portstat.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/puppetagent/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1534 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/puppetagent/puppetagent.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/entropy/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      889 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/entropy/entropy.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/tokumx/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10885 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/tokumx/tokumx.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/aurora/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1601 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/aurora/aurora.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/powerdns/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2283 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/powerdns/powerdns.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/diskspace/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9895 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/diskspace/diskspace.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/apcupsd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3108 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/apcupsd/apcupsd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ip/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3207 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/ip/ip.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/uptime/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1175 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/uptime/uptime.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/mysqlstat/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    20451 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/mysqlstat/mysqlstat.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8243 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/mysqlstat/mysql55.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/slabinfo/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2532 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/slabinfo/slabinfo.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/nfsd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8920 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/nfsd/nfsd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/memory_lxc/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2599 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/memory_lxc/memory_lxc.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/elasticsearch/
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)    16680 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/elasticsearch/elasticsearch.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ups/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2112 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/ups/ups.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/udp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2669 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/udp/udp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/docker_collector/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3143 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/docker_collector/docker_collector.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/supervisord/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2861 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/supervisord/supervisord.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/slony/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4425 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/slony/slony.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/memcached/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5210 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/memcached/memcached.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/postgres/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    20727 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/postgres/postgres.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ceph/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4645 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/ceph/ceph.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/filestat/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11203 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/filestat/filestat.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ntp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3199 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/ntp/ntp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/bind/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5681 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/bind/bind.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/openldap/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5683 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/openldap/openldap.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/hadoop/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3798 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/hadoop/hadoop.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/postqueue/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1633 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/postqueue/postqueue.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/iodrivesnmp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4089 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/iodrivesnmp/iodrivesnmp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/darner/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5177 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/darner/darner.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/websitemonitor/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2759 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/websitemonitor/websitemonitor.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/interrupt/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2024 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/interrupt/soft.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3193 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/interrupt/interrupt.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/cpu/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9305 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/cpu/cpu.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/mongodb/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    14405 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/mongodb/mongodb.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/celerymon/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2506 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/celerymon/celerymon.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/s3/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2360 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/s3/s3.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/snmpraw/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6074 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/snmpraw/snmpraw.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/solr/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7034 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/solr/solr.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/openstackswift/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3996 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/openstackswift/openstackswift.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/amavis/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3854 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/amavis/amavis.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/memory/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4536 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/memory/memory.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/cpuacct_cgroup/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2359 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/cpuacct_cgroup/cpuacct_cgroup.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/conntrack/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3012 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/conntrack/conntrack.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/elb/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11483 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/elb/elb.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/xfs/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7077 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/xfs/xfs.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/nfs/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8775 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/nfs/nfs.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/openvpn/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7766 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/openvpn/openvpn.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/passenger_stats/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5440 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/passenger_stats/passenger_stats.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/mesos_cgroup/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4632 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/mesos_cgroup/mesos_cgroup.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ntpd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5066 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/ntpd/ntpd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/diskusage/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10481 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/diskusage/diskusage.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/unbound/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2942 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/unbound/unbound.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/snmp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4242 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/snmp/snmp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/pgq/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3505 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/pgq/pgq.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/endecadgraph/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3912 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/endecadgraph/endecadgraph.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/rabbitmq/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9613 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/rabbitmq/rabbitmq.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/httpd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5810 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/httpd/httpd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/flume/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3401 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/flume/flume.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/xen_collector/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2217 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/xen_collector/xen_collector.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/mountstats/
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     8068 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/mountstats/mountstats.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/hbase/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3375 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/hbase/hbase.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/netapp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3492 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/netapp/netapp_inode.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10829 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/netapp/netappDisk.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    16269 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/netapp/netapp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/nagios/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3227 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/nagios/nagios.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/servertechpdu/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4354 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/servertechpdu/servertechpdu.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/sidekiqweb/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1905 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/sidekiqweb/sidekiqweb.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/resqueweb/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1713 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/resqueweb/resqueweb.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/chronyd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2401 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/chronyd/chronyd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/monit/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4568 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/monit/monit.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/mesos/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1728 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/mesos/mesos.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/jcollectd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5979 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/jcollectd/jcollectd.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10495 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/jcollectd/collectd_network.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/fluentd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1711 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/fluentd/fluentd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/httpjson/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2118 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/httpjson/httpjson.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/ksm/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1644 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/ksm/ksm.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/netscalersnmp/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11764 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/netscalersnmp/netscalersnmp.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/jolokia/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    12268 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/jolokia/jolokia.py
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4144 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/jolokia/cassandra_jolokia.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/drbd/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2737 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/drbd/drbd.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/nfacct/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2275 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/nfacct/nfacct.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/userscripts/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3548 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/userscripts/userscripts.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/redisstat/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11740 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/redisstat/redisstat.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/dropwizard/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3333 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/dropwizard/dropwizard.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/snmpinterface/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8595 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/snmpinterface/snmpinterface.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/twemproxy/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5811 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/twemproxy/twemproxy.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/phpfpm/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2213 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/phpfpm/phpfpm.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/memory_docker/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1168 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/memory_docker/memory_docker.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/processresources/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7247 2019-02-19 18:58:34.000000 diamond-next-4.0.515/src/collectors/processresources/processresources.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/src/collectors/openstackswiftrecon/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2814 2019-02-19 18:57:43.000000 diamond-next-4.0.515/src/collectors/openstackswiftrecon/openstackswiftrecon.py
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/conf/
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/conf/vagrant/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6238 2019-02-19 18:57:43.000000 diamond-next-4.0.515/conf/vagrant/diamond.conf
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/conf/vagrant/collectors/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       38 2019-02-19 18:57:43.000000 diamond-next-4.0.515/conf/vagrant/collectors/RedisCollector.conf
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5355 2019-02-19 18:57:43.000000 diamond-next-4.0.515/conf/diamond.conf.example.windows
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6194 2019-02-19 18:58:34.000000 diamond-next-4.0.515/conf/diamond.conf.example
-drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2019-02-19 19:06:02.000000 diamond-next-4.0.515/debian/
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      964 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/diamond.upstart
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        5 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/pyversions
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      929 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/prerm
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1574 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/postinst
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      299 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/diamond.default
--rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)      245 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/rules
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1059 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/postrm
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4384 2019-02-19 18:58:34.000000 diamond-next-4.0.515/debian/diamond.init
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        2 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/compat
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      690 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/preinst
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      625 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/changelog
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       39 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/dirs
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1334 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/copyright
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      637 2019-02-19 18:57:43.000000 diamond-next-4.0.515/debian/control
--rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1365 2019-02-19 18:57:43.000000 diamond-next-4.0.515/LICENSE
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.307040 diamond-next-5.0.0/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:28:24.727695 diamond-next-5.0.0/.keep
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1548 2023-07-06 10:28:24.727695 diamond-next-5.0.0/LICENSE
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      222 2023-07-06 10:28:24.727695 diamond-next-5.0.0/MANIFEST.in
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      390 2023-07-06 10:59:42.307040 diamond-next-5.0.0/PKG-INFO
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/bin/
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)    11002 2023-07-06 10:28:24.727695 diamond-next-5.0.0/bin/diamond
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     7606 2023-07-06 10:28:24.727695 diamond-next-5.0.0/bin/diamond-setup
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/bin/init.d/
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     2041 2023-07-06 10:28:24.727695 diamond-next-5.0.0/bin/init.d/diamond
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/conf/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6432 2023-07-06 10:28:24.727695 diamond-next-5.0.0/conf/diamond.conf.example
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5372 2023-07-06 10:28:24.727695 diamond-next-5.0.0/conf/diamond.conf.example.windows
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/conf/vagrant/
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/conf/vagrant/collectors/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       38 2023-07-06 10:28:24.727695 diamond-next-5.0.0/conf/vagrant/collectors/RedisCollector.conf
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6254 2023-07-06 10:28:24.727695 diamond-next-5.0.0/conf/vagrant/diamond.conf
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/debian/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      625 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/changelog
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        2 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/compat
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      625 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/control
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1334 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/copyright
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      299 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/diamond.default
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4384 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/diamond.init
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      964 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/diamond.upstart
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       39 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/dirs
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1574 2023-07-06 10:28:24.727695 diamond-next-5.0.0/debian/postinst
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1059 2023-07-06 10:28:24.731695 diamond-next-5.0.0/debian/postrm
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      690 2023-07-06 10:28:24.731695 diamond-next-5.0.0/debian/preinst
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      929 2023-07-06 10:28:24.731695 diamond-next-5.0.0/debian/prerm
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        5 2023-07-06 10:28:24.731695 diamond-next-5.0.0/debian/pyversions
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)      245 2023-07-06 10:28:24.731695 diamond-next-5.0.0/debian/rules
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/rpm/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      562 2023-07-06 10:28:24.743695 diamond-next-5.0.0/rpm/install
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)      594 2023-07-06 10:28:24.743695 diamond-next-5.0.0/rpm/postinstall
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      188 2023-07-06 10:28:24.743695 diamond-next-5.0.0/rpm/preinstall
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)      552 2023-07-06 10:28:24.743695 diamond-next-5.0.0/rpm/preuninstall
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/rpm/systemd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      277 2023-07-06 10:28:24.743695 diamond-next-5.0.0/rpm/systemd/diamond.service
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/rpm/upstart/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      297 2023-07-06 10:28:24.743695 diamond-next-5.0.0/rpm/upstart/diamond.conf
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      246 2023-07-06 10:33:46.264157 diamond-next-5.0.0/setup.cfg
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     4562 2023-07-06 10:36:50.844478 diamond-next-5.0.0/setup.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/aerospike/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10646 2023-07-06 10:28:24.743695 diamond-next-5.0.0/src/collectors/aerospike/aerospike.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/amavis/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3935 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/amavis/amavis.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/apcupsd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3403 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/apcupsd/apcupsd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/aurora/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1570 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/aurora/aurora.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/beanstalkd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2573 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/beanstalkd/beanstalkd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/bind/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5582 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/bind/bind.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/celerymon/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2562 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/celerymon/celerymon.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/ceph/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4602 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/ceph/ceph.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/cephstats/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2300 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/cephstats/cephstats.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/chronyd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2369 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/chronyd/chronyd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/conntrack/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2976 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/conntrack/conntrack.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/cpu/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9537 2023-07-06 10:28:24.747695 diamond-next-5.0.0/src/collectors/cpu/cpu.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/cpuacct_cgroup/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2321 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/cpuacct_cgroup/cpuacct_cgroup.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/darner/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5176 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/darner/darner.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/diskspace/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9760 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/diskspace/diskspace.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/disktemp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3045 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/disktemp/disktemp.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/diskusage/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10155 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/diskusage/diskusage.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/docker_collector/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3136 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/docker_collector/docker_collector.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/drbd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2780 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/drbd/drbd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/dropwizard/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3395 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/dropwizard/dropwizard.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/dseopscenter/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7107 2023-07-06 10:28:24.751695 diamond-next-5.0.0/src/collectors/dseopscenter/dseopscenter.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/elasticsearch/
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)    17895 2023-07-06 10:28:24.755695 diamond-next-5.0.0/src/collectors/elasticsearch/elasticsearch.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/elb/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11150 2023-07-06 10:28:24.755695 diamond-next-5.0.0/src/collectors/elb/elb.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/endecadgraph/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3838 2023-07-06 10:28:24.755695 diamond-next-5.0.0/src/collectors/endecadgraph/endecadgraph.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/entropy/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      863 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/entropy/entropy.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/etcdstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3182 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/etcdstat/etcdstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/eventstoreprojections/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4516 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/eventstoreprojections/eventstoreprojections.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.295040 diamond-next-5.0.0/src/collectors/eventstoreprojections/tests/
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/eventstoreprojections/tests/fixtures/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5794 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/eventstoreprojections/tests/fixtures/projections
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/example/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3318 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/example/example.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/exim/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1639 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/exim/exim.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/files/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2115 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/files/files.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/filestat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11072 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/filestat/filestat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/fluentd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1728 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/fluentd/fluentd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/flume/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3464 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/flume/flume.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/glusterfs/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4901 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/glusterfs/glusterfs.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/gridengine/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5094 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/gridengine/gridengine.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/hadoop/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4671 2023-07-06 10:28:24.759695 diamond-next-5.0.0/src/collectors/hadoop/hadoop.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/haproxy/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6347 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/haproxy/haproxy.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/hbase/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4269 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/hbase/hbase.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.299040 diamond-next-5.0.0/src/collectors/httpc/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3116 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/httpc/httpc.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/httpd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7211 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/httpd/httpd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/httpjson/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2187 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/httpjson/httpjson.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/icinga_stats/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    14484 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/icinga_stats/icinga_stats.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/interrupt/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3176 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/interrupt/interrupt.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1920 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/interrupt/soft.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/iodrivesnmp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4017 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/iodrivesnmp/iodrivesnmp.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ip/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3266 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/ip/ip.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ipmisensor/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4969 2023-07-06 10:28:24.763695 diamond-next-5.0.0/src/collectors/ipmisensor/ipmisensor.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ipvs/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5755 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/ipvs/ipvs.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/jbossapi/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    15791 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/jbossapi/jbossapi.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/jcollectd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9838 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/jcollectd/collectd_network.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5764 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/jcollectd/jcollectd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/jolokia/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4110 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/jolokia/cassandra_jolokia.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    13071 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/jolokia/jolokia.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/kafka_consumer_lag/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2601 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/kafka_consumer_lag/kafka_consumer_lag.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/kafkastat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5373 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/kafkastat/kafkastat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ksm/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1666 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/ksm/ksm.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/kvm/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1214 2023-07-06 10:28:24.767695 diamond-next-5.0.0/src/collectors/kvm/kvm.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/libvirtkvm/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5266 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/libvirtkvm/libvirtkvm.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/lmsensors/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2090 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/lmsensors/lmsensors.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/loadavg/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2025 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/loadavg/loadavg.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/mdstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11189 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/mdstat/mdstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/memcached/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5694 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/memcached/memcached.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/memcached_slab/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3520 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/memcached_slab/memcached_slab.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/memory/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5467 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/memory/memory.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/memory_cgroup/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3613 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/memory_cgroup/memory_cgroup.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/memory_docker/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1164 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/memory_docker/memory_docker.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/memory_lxc/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2632 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/memory_lxc/memory_lxc.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/mesos/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9114 2023-07-06 10:28:24.771695 diamond-next-5.0.0/src/collectors/mesos/mesos.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/mesos_cgroup/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4535 2023-07-06 10:28:24.775695 diamond-next-5.0.0/src/collectors/mesos_cgroup/mesos_cgroup.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/mogilefs/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1216 2023-07-06 10:28:24.775695 diamond-next-5.0.0/src/collectors/mogilefs/mogilefs.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/mongodb/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    14667 2023-07-06 10:28:24.775695 diamond-next-5.0.0/src/collectors/mongodb/mongodb.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/monit/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4772 2023-07-06 10:28:24.775695 diamond-next-5.0.0/src/collectors/monit/monit.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/mountstats/
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     8592 2023-07-06 10:28:24.775695 diamond-next-5.0.0/src/collectors/mountstats/mountstats.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/mysqlstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8511 2023-07-06 10:28:24.775695 diamond-next-5.0.0/src/collectors/mysqlstat/mysql55.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    20557 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/mysqlstat/mysqlstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/nagios/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2954 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/nagios/nagios.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/nagiosperfdata/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9209 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/nagiosperfdata/nagiosperfdata.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/netapp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    16402 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/netapp/netapp.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10727 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/netapp/netappDisk.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3428 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/netapp/netapp_inode.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/netscalersnmp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11228 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/netscalersnmp/netscalersnmp.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/netstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2215 2023-07-06 10:28:24.779695 diamond-next-5.0.0/src/collectors/netstat/netstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/network/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4441 2023-07-06 10:28:24.783695 diamond-next-5.0.0/src/collectors/network/network.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/nfacct/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2439 2023-07-06 10:28:24.783695 diamond-next-5.0.0/src/collectors/nfacct/nfacct.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/nfs/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8753 2023-07-06 10:28:24.783695 diamond-next-5.0.0/src/collectors/nfs/nfs.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/nfsd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9157 2023-07-06 10:28:24.783695 diamond-next-5.0.0/src/collectors/nfsd/nfsd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/nginx/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7987 2023-07-06 10:28:24.787695 diamond-next-5.0.0/src/collectors/nginx/nginx.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ntp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3132 2023-07-06 10:28:24.787695 diamond-next-5.0.0/src/collectors/ntp/ntp.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ntpd/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5057 2023-07-06 10:28:24.787695 diamond-next-5.0.0/src/collectors/ntpd/ntpd.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/numa/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1594 2023-07-06 10:28:24.787695 diamond-next-5.0.0/src/collectors/numa/numa.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/nvidia_gpu/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4222 2023-07-06 10:28:24.791695 diamond-next-5.0.0/src/collectors/nvidia_gpu/nvidia_gpu.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/onewire/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2458 2023-07-06 10:28:24.791695 diamond-next-5.0.0/src/collectors/onewire/onewire.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/openldap/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5859 2023-07-06 10:28:24.791695 diamond-next-5.0.0/src/collectors/openldap/openldap.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/openstackswift/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4090 2023-07-06 10:28:24.791695 diamond-next-5.0.0/src/collectors/openstackswift/openstackswift.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/openstackswiftrecon/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2915 2023-07-06 10:28:24.791695 diamond-next-5.0.0/src/collectors/openstackswiftrecon/openstackswiftrecon.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/openvpn/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8040 2023-07-06 10:28:24.795695 diamond-next-5.0.0/src/collectors/openvpn/openvpn.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/openvz/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3430 2023-07-06 10:28:24.795695 diamond-next-5.0.0/src/collectors/openvz/openvz.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ossec/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2340 2023-07-06 10:28:24.795695 diamond-next-5.0.0/src/collectors/ossec/ossec.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/passenger_stats/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8352 2023-07-06 10:28:24.795695 diamond-next-5.0.0/src/collectors/passenger_stats/passenger_stats.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/pgbouncer/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3613 2023-07-06 10:28:24.795695 diamond-next-5.0.0/src/collectors/pgbouncer/pgbouncer.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/pgq/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3567 2023-07-06 10:28:24.795695 diamond-next-5.0.0/src/collectors/pgq/pgq.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/phpfpm/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2749 2023-07-06 10:28:24.795695 diamond-next-5.0.0/src/collectors/phpfpm/phpfpm.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ping/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2236 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/ping/ping.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/portstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2017 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/portstat/portstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/postfix/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3487 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/postfix/postfix.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/postgres/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    22737 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/postgres/postgres.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/postqueue/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1598 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/postqueue/postqueue.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/powerdns/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2368 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/powerdns/powerdns.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/proc/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1892 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/proc/proc.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/processresources/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7464 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/processresources/processresources.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/puppetagent/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1569 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/puppetagent/puppetagent.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/puppetdashboard/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1795 2023-07-06 10:28:24.799695 diamond-next-5.0.0/src/collectors/puppetdashboard/puppetdashboard.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/puppetdb/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6565 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/puppetdb/puppetdb.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/rabbitmq/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10722 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/rabbitmq/rabbitmq.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/redisstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    12486 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/redisstat/redisstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/resqueweb/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1756 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/resqueweb/resqueweb.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/s3/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2234 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/s3/s3.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/scribe/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2113 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/scribe/scribe.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/servertechpdu/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4228 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/servertechpdu/servertechpdu.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/sidekiq/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5509 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/sidekiq/sidekiq.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/sidekiqweb/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1952 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/sidekiqweb/sidekiqweb.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/slabinfo/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2583 2023-07-06 10:28:24.803695 diamond-next-5.0.0/src/collectors/slabinfo/slabinfo.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/slony/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4508 2023-07-06 10:28:24.807695 diamond-next-5.0.0/src/collectors/slony/slony.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/smart/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4319 2023-07-06 10:28:24.807695 diamond-next-5.0.0/src/collectors/smart/smart.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/snmp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4160 2023-07-06 10:28:24.807695 diamond-next-5.0.0/src/collectors/snmp/snmp.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/snmpinterface/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7898 2023-07-06 10:28:24.807695 diamond-next-5.0.0/src/collectors/snmpinterface/snmpinterface.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/snmpraw/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6227 2023-07-06 10:28:24.807695 diamond-next-5.0.0/src/collectors/snmpraw/snmpraw.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/sockstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1812 2023-07-06 10:28:24.807695 diamond-next-5.0.0/src/collectors/sockstat/sockstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/solr/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8385 2023-07-06 10:28:24.807695 diamond-next-5.0.0/src/collectors/solr/solr.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/sqs/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2930 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/sqs/sqs.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/squid/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3039 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/squid/squid.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/supervisord/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2869 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/supervisord/supervisord.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/tcp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    12445 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/tcp/tcp.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/tokumx/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10571 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/tokumx/tokumx.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/twemproxy/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6163 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/twemproxy/twemproxy.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/udp/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2704 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/udp/udp.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/unbound/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2956 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/unbound/unbound.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/ups/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2107 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/ups/ups.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/uptime/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1085 2023-07-06 10:28:24.811695 diamond-next-5.0.0/src/collectors/uptime/uptime.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/users/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1906 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/users/users.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/userscripts/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3628 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/userscripts/userscripts.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/varnish/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    11274 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/varnish/varnish.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/vmsdoms/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3187 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/vmsdoms/vmsdoms.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/vmsfs/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2923 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/vmsfs/vmsfs.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/vmstat/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1667 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/vmstat/vmstat.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/websitemonitor/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2689 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/websitemonitor/websitemonitor.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/xen_collector/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2185 2023-07-06 10:28:24.815695 diamond-next-5.0.0/src/collectors/xen_collector/xen_collector.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/xfs/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6288 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/collectors/xfs/xfs.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/collectors/zookeeper/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4634 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/collectors/zookeeper/zookeeper.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.303040 diamond-next-5.0.0/src/diamond/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       49 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/__init__.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    18979 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/collector.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7470 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/convertor.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      221 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/error.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7061 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/gmetric.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.307040 diamond-next-5.0.0/src/diamond/handler/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4174 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/Handler.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       15 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/__init__.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2557 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/archive.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7796 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/cloudwatch.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2615 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/datadog.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2698 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/g_metric.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     9441 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/graphite.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2980 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/graphitepickle.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2648 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/hostedgraphite.py
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     1771 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/httpHandler.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7567 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/influxdbHandler.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4641 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/libratohandler.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2399 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/logentries_diamond.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5735 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/mqtt.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2554 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/multigraphite.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2613 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/multigraphitepickle.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2939 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/mysql.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      894 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/null.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1380 2023-07-06 10:28:24.819695 diamond-next-5.0.0/src/diamond/handler/queue.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6724 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/rabbitmq_pubsub.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4691 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/rabbitmq_topic.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3343 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/riemann.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7835 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/rrdtool.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    10762 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/sentry.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5450 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/signalfx.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4805 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/stats_d.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     6167 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/statsite.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)    14165 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/tsdb.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     2057 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/handler/zmq_pubsub.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     5575 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/metric.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     8108 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/server.py
+-rwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)     5469 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/testing.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1361 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/util.py
+drwxrwxr-x   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:59:42.307040 diamond-next-5.0.0/src/diamond/utils/
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        0 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/utils/__init__.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     7469 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/utils/classes.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     4421 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/utils/config.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     1707 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/utils/log.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)     3692 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/utils/scheduler.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)      702 2023-07-06 10:28:24.823695 diamond-next-5.0.0/src/diamond/utils/signals.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)       38 2023-07-06 10:59:42.167040 diamond-next-5.0.0/src/diamond/version.py
+-rw-rw-r--   0 kwarunek  (1000) kwarunek  (1000)        6 2023-07-06 10:59:42.167040 diamond-next-5.0.0/version.txt
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `diamond-next-4.0.515/setup.py` & `diamond-next-5.0.0/setup.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,152 +1,156 @@
 #!/usr/bin/env python
 # coding=utf-8
 
-import sys
+import distro
 import os
-from glob import glob
 import platform
+import sys
+from glob import glob
+
 
 
 def running_under_virtualenv():
-    if hasattr(sys, 'real_prefix'):
+    if hasattr(sys, "real_prefix"):
         return True
     elif sys.prefix != getattr(sys, "base_prefix", sys.prefix):
         return True
-    if os.getenv('VIRTUAL_ENV', False):
+
+    if os.getenv("VIRTUAL_ENV", False):
         return True
+
     return False
 
 
-if os.environ.get('USE_SETUPTOOLS'):
+if os.environ.get("USE_SETUPTOOLS"):
     from setuptools import setup
+
     setup_kwargs = dict(zip_safe=0)
 else:
     from distutils.core import setup
+
     setup_kwargs = dict()
 
-if os.name == 'nt':
-    pgm_files = os.environ["ProgramFiles"]
-    base_files = os.path.join(pgm_files, 'diamond')
+if os.name == "nt":
+    base_files = os.path.join(os.environ["ProgramFiles"], "diamond")
     data_files = [
-        (base_files, ['LICENSE', 'version.txt']),
-        (os.path.join(base_files, 'user_scripts'), []),
-        (os.path.join(base_files, 'conf'), glob('conf/*.conf.*')),
-        (os.path.join(base_files, 'collectors'), glob('conf/collectors/*')),
-        (os.path.join(base_files, 'handlers'), glob('conf/handlers/*')),
+        (base_files, ["LICENSE", "version.txt"]),
+        (os.path.join(base_files, "user_scripts"), []),
+        (os.path.join(base_files, "conf"), glob("conf/*.conf.*")),
+        (os.path.join(base_files, "collectors"), glob("conf/collectors/*")),
+        (os.path.join(base_files, "handlers"), glob("conf/handlers/*")),
     ]
-    install_requires = ['configobj', 'psutil', ],
-
 else:
+    base_files = []
     data_files = [
-        ('share/diamond', ['LICENSE', 'version.txt']),
-        ('share/diamond/user_scripts', []),
+        ("share/diamond", ["LICENSE", "version.txt"]),
+        ("share/diamond/user_scripts", []),
     ]
+    distro_name = distro.name()
+    distro_major_version = int(distro.version().split(".")[0])
 
-    distro = platform.dist()[0]
-    distro_major_version = platform.dist()[1].split('.')[0]
-
-    if running_under_virtualenv():
-        data_files.append(('etc/diamond',
-                           glob('conf/*.conf.*')))
-        data_files.append(('etc/diamond/collectors',
-                           glob('conf/collectors/*')))
-        data_files.append(('etc/diamond/handlers',
-                           glob('conf/handlers/*')))
-    else:
-        data_files.append(('/etc/diamond',
-                           glob('conf/*.conf.*')))
-        data_files.append(('/etc/diamond/collectors',
-                           glob('conf/collectors/*')))
-        data_files.append(('/etc/diamond/handlers',
-                           glob('conf/handlers/*')))
-
-        if distro == 'Ubuntu':
-            data_files.append(('/etc/init',
-                               ['debian/diamond.upstart']))
-        if distro in ['centos', 'redhat', 'debian', 'fedora', 'oracle']:
-            data_files.append(('/etc/init.d',
-                               ['bin/init.d/diamond']))
-            data_files.append(('/var/log/diamond',
-                               ['.keep']))
-            if distro_major_version >= 7 and not distro == 'debian':
-                data_files.append(('/usr/lib/systemd/system',
-                                   ['rpm/systemd/diamond.service']))
-            elif distro_major_version >= 6 and not distro == 'debian':
-                data_files.append(('/etc/init',
-                                   ['rpm/upstart/diamond.conf']))
-
-    # Support packages being called differently on different distros
+    if not distro_name:
+        if "amzn" in platform.uname()[2]:
+            distro_name = "centos"
 
-    # Are we in a virtenv?
     if running_under_virtualenv():
-        install_requires = ['configobj', 'psutil', ]
+        data_files.append(("etc/diamond", glob("conf/*.conf.*")))
+        data_files.append(("etc/diamond/collectors", glob("conf/collectors/*")))
+        data_files.append(("etc/diamond/handlers", glob("conf/handlers/*")))
     else:
-        if distro == ['debian', 'ubuntu']:
-            install_requires = ['python-configobj', 'python-psutil', ]
-        # Default back to pip style requires
-        else:
-            install_requires = ['configobj', 'psutil', ]
+        data_files.append(("/etc/diamond", glob("conf/*.conf.*")))
+        data_files.append(("/etc/diamond/collectors", glob("conf/collectors/*")))
+        data_files.append(("/etc/diamond/handlers", glob("conf/handlers/*")))
+        data_files.append(("/var/log/diamond", [".keep"]))
+
+        if distro_name == "Ubuntu":
+            if distro_major_version >= 16:
+                data_files.append(
+                    ("/usr/lib/systemd/system", ["rpm/systemd/diamond.service"])
+                )
+            else:
+                data_files.append(("/etc/init", ["debian/diamond.upstart"]))
+
+        if distro_name in ["centos", "redhat", "debian", "fedora", "oracle"]:
+            data_files.append(("/etc/init.d", ["bin/init.d/diamond"]))
+
+            if distro_major_version >= 7 and not distro_name == "debian":
+                data_files.append(
+                    ("/usr/lib/systemd/system", ["rpm/systemd/diamond.service"])
+                )
+            elif distro_major_version >= 6 and not distro_name == "debian":
+                data_files.append(("/etc/init", ["rpm/upstart/diamond.conf"]))
 
 
 def get_version():
     """
-        Read the version.txt file to get the new version string
-        Generate it if version.txt is not available. Generation
-        is required for pip installs
+    Read the version.txt file to get the new version string
+    Generate it if version.txt is not available. Generation
+    is required for pip installs
     """
     try:
-        f = open('version.txt')
+        f = open("version.txt")
     except IOError:
         os.system("./version.sh > version.txt")
-        f = open('version.txt')
-    version = ''.join(f.readlines()).rstrip()
+        f = open("version.txt")
+
+    version_name = "".join(f.readlines()).rstrip()
     f.close()
-    return version
 
+    return version_name
 
-def pkgPath(root, path, rpath="/"):
+
+def pkg_path(root, path, rpath="/"):
     """
-        Package up a path recursively
+    Package up a path recursively
     """
     global data_files
+
     if not os.path.exists(path):
         return
+
     files = []
+
     for spath in os.listdir(path):
         # Ignore test directories
-        if spath == 'test':
+        if spath == "test":
             continue
+
         subpath = os.path.join(path, spath)
         spath = os.path.join(rpath, spath)
+
         if os.path.isfile(subpath):
             files.append(subpath)
+
         if os.path.isdir(subpath):
-            pkgPath(root, subpath, spath)
+            pkg_path(root, subpath, spath)
+
     data_files.append((root + rpath, files))
 
-if os.name == 'nt':
-    pkgPath(os.path.join(base_files, 'collectors'), 'src/collectors', '\\')
+
+if os.name == "nt":
+    pkg_path(os.path.join(base_files, "collectors"), "src/collectors", "\\")
 else:
-    pkgPath('share/diamond/collectors', 'src/collectors')
+    pkg_path("share/diamond/collectors", "src/collectors")
 
 version = get_version()
 
 setup(
-    name='diamond-next',
+    name="diamond-next",
     version=version,
-    url='https://github.com/diamond-next/diamond-next',
-    author='The Diamond (Next) Team',
-    author_email='krzysztof@warunek.net',
-    license='MIT License',
-    description='Smart data producer for graphite graphing package',
-    package_dir={'': 'src'},
-    packages=['diamond', 'diamond.handler', 'diamond.utils'],
-    scripts=['bin/diamond', 'bin/diamond-setup'],
+    url="https://github.com/diamond-next/diamond-next",
+    author="The Diamond (Next) Team",
+    author_email="krzysztof@warunek.net",
+    license="MIT License",
+    description="Smart data producer for graphite graphing package",
+    package_dir={"": "src"},
+    packages=["diamond", "diamond.handler", "diamond.utils"],
+    scripts=["bin/diamond", "bin/diamond-setup"],
     data_files=data_files,
-    install_requires=install_requires,
+    python_requires=">=3.8",
+    install_requires=["configobj", "psutil"],
     classifiers=[
-        'Programming Language :: Python',
-        'Programming Language :: Python :: 2',
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 3",
     ],
-    ** setup_kwargs
+    **setup_kwargs
 )
```

### Comparing `diamond-next-4.0.515/bin/diamond` & `diamond-next-5.0.0/bin/diamond`

 * *Files 26% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 #!/usr/bin/env python
 # coding=utf-8
 
+from __future__ import print_function
+
 import os
 import sys
+
 import configobj
+
 if os.name != 'nt':
     import pwd
     import grp
 
 try:
     from setproctitle import setproctitle
 except ImportError:
@@ -39,102 +43,55 @@
         }
 
         if os.name == 'nt':
             defaults['skip_pidfile'] = True
 
         parser = optparse.OptionParser()
 
-        parser.add_option("-c", "--configfile",
-                          dest="configfile",
-                          default="/etc/diamond/diamond.conf",
-                          help="config file")
-
-        parser.add_option("-f", "--foreground",
-                          dest="foreground",
-                          default=False,
-                          action="store_true",
-                          help="run in foreground")
-
-        parser.add_option("-l", "--log-stdout",
-                          dest="log_stdout",
-                          default=False,
-                          action="store_true",
-                          help="log to stdout")
-
-        parser.add_option("-p", "--pidfile",
-                          dest="pidfile",
-                          default=None,
-                          help="pid file")
-
-        parser.add_option("-r", "--run",
-                          dest="collector",
-                          default=None,
-                          help="run a given collector once and exit")
-
-        parser.add_option("-v", "--version",
-                          dest="version",
-                          default=False,
-                          action="store_true",
-                          help="display the version and exit")
-
-        parser.add_option("--skip-pidfile",
-                          dest="skip_pidfile",
-                          default=defaults['skip_pidfile'],
-                          action="store_true",
-                          help="Skip creating PID file")
-
-        parser.add_option("-u", "--user",
-                          dest="user",
-                          default=None,
-                          help="Change to specified unprivilegd user")
-        parser.add_option("-g", "--group",
-                          dest="group",
-                          default=None,
-                          help="Change to specified unprivilegd group")
-        parser.add_option("--skip-change-user",
-                          dest="skip_change_user",
-                          default=False,
-                          action="store_true",
-                          help="Skip changing to an unprivilegd user")
-
-        parser.add_option("--skip-fork",
-                          dest="skip_fork",
-                          default=False,
-                          action="store_true",
-                          help="Skip forking (damonizing) process")
+        parser.add_option("-c", "--configfile", dest="configfile", default="/etc/diamond/diamond.conf", help="config file")
+        parser.add_option("-f", "--foreground", dest="foreground", default=False, action="store_true", help="run in foreground")
+        parser.add_option("-l", "--log-stdout", dest="log_stdout", default=False, action="store_true", help="log to stdout")
+        parser.add_option("-p", "--pidfile", dest="pidfile", default=None, help="pid file")
+        parser.add_option("-r", "--run", dest="collector", default=None, help="run a given collector once and exit")
+        parser.add_option("-v", "--version", dest="version", default=False, action="store_true", help="display the version and exit")
+        parser.add_option("--skip-pidfile", dest="skip_pidfile", default=defaults['skip_pidfile'], action="store_true", help="Skip creating PID file")
+        parser.add_option("-u", "--user", dest="user", default=None, help="Change to specified unprivilegd user")
+        parser.add_option("-g", "--group", dest="group", default=None, help="Change to specified unprivilegd group")
+        parser.add_option("--skip-change-user", dest="skip_change_user", default=False, action="store_true", help="Skip changing to an unprivilegd user")
+        parser.add_option("--skip-fork", dest="skip_fork", default=False, action="store_true", help="Skip forking (damonizing) process")
 
         # Parse Command Line Args
         (options, args) = parser.parse_args()
 
         # Initial variables
         uid = -1
         gid = -1
 
         if options.version:
-            print "Diamond version %s" % (get_diamond_version())
+            print("Diamond version %s" % (get_diamond_version()))
             sys.exit(0)
 
         # Initialize Config
         options.configfile = os.path.abspath(options.configfile)
+
         if os.path.exists(options.configfile):
             config = configobj.ConfigObj(options.configfile)
         else:
-            print >> sys.stderr, "ERROR: Config file: %s does not exist." % (
-                options.configfile)
+            print("ERROR: Config file: %s does not exist." % options.configfile, file=sys.stderr)
             parser.print_help(sys.stderr)
             sys.exit(1)
 
         # Initialize Logging
         log = setup_logging(options.configfile, options.log_stdout)
 
     # Pass the exit up stream rather then handle it as an general exception
-    except SystemExit, e:
+    except SystemExit:
         raise SystemExit
 
-    except Exception, e:
+    except Exception as e:
         import traceback
         sys.stderr.write("Unhandled exception: %s" % str(e))
         sys.stderr.write("traceback: %s" % traceback.format_exc())
         sys.exit(1)
 
     # Switch to using the logging system
     try:
@@ -142,32 +99,30 @@
         if not options.skip_pidfile:
             # Initialize Pid file
             if not options.pidfile:
                 options.pidfile = str(config['server']['pid_file'])
 
             # Read existing pid file
             try:
-                pf = file(options.pidfile, 'r')
+                pf = open(options.pidfile, 'r')
                 pid = int(pf.read().strip())
                 pf.close()
             except (IOError, ValueError):
                 pid = None
 
             # Check existing pid file
             if pid:
                 # Check if pid is real
                 if not os.path.exists("/".join(["/proc", str(pid), "cmdline"])):
                     # Pid is not real
                     os.unlink(options.pidfile)
                     pid = None
-                    print >> sys.stderr, (
-                        "WARN: Bogus pid file was found. I deleted it.")
+                    print("WARN: Bogus pid file was found. I deleted it.", file=sys.stderr)
                 else:
-                    print >> sys.stderr, (
-                        "ERROR: Pidfile exists. Server already running?")
+                    print("ERROR: Pidfile exists. Server already running?", file=sys.stderr)
                     sys.exit(1)
 
             # Get final GIDs
             if os.name != 'nt':
                 if options.group is not None:
                     gid = grp.getgrnam(options.group).gr_gid
                 elif len(config['server']['group']):
@@ -180,24 +135,27 @@
                 elif len(config['server']['user']):
                     uid = pwd.getpwnam(config['server']['user']).pw_uid
 
             # Fix up pid permissions
             if not options.foreground and not options.collector:
                 # Write pid file
                 pid = str(os.getpid())
+
                 try:
-                    pf = file(options.pidfile, 'w+')
-                except IOError, e:
-                    print >> sys.stderr, "Failed to write PID file: %s" % (e)
+                    pf = open(options.pidfile, 'w+')
+                except IOError as e:
+                    print("Failed to write PID file: %s" % e, file=sys.stderr)
                     sys.exit(1)
+
                 pf.write("%s\n" % pid)
                 pf.close()
                 os.chown(options.pidfile, uid, gid)
+
                 # Log
-                log.debug("Wrote First PID file: %s" % (options.pidfile))
+                log.debug("Wrote First PID file: %s" % options.pidfile)
 
         # USER MANAGEMENT
         if not options.skip_change_user:
             # Switch user to specified user/group if required
             try:
                 if gid != -1 and uid != -1:
                     # Manually set the groups since they aren't set by default
@@ -207,54 +165,55 @@
                     # Set GID
                     os.setgid(gid)
 
                 if uid != -1 and os.getuid() != uid:
                     # Set UID
                     os.setuid(uid)
 
-            except Exception, e:
-                print >> sys.stderr, "ERROR: Failed to set UID/GID. %s" % (e)
+            except Exception as e:
+                print("ERROR: Failed to set UID/GID. %s" % e, file=sys.stderr)
                 sys.exit(1)
 
             # Log
-            log.info('Changed UID: %d (%s) GID: %d (%s).' % (
-                os.getuid(),
-                config['server']['user'],
-                os.getgid(),
-                config['server']['group']))
+            log.info('Changed UID: %d (%s) GID: %d (%s).' % (os.getuid(), config['server']['user'], os.getgid(), config['server']['group']))
 
         # DAEMONIZE MANAGEMENT
         if not options.skip_fork:
             # Detatch Process
             if not options.foreground and not options.collector:
 
                 # Double fork to serverize process
                 log.info('Detaching Process.')
 
                 # Fork 1
                 try:
                     pid = os.fork()
+
                     if pid > 0:
                         # Exit first paren
                         sys.exit(0)
-                except OSError, e:
-                    print >> sys.stderr, "Failed to fork process." % (e)
+                except OSError as e:
+                    print("Failed to fork process." % e, file=sys.stderr)
                     sys.exit(1)
+
                 # Decouple from parent environmen
                 os.setsid()
                 os.umask(0o022)
+
                 # Fork 2
                 try:
                     pid = os.fork()
+
                     if pid > 0:
                         # Exit second paren
                         sys.exit(0)
-                except OSError, e:
-                    print >> sys.stderr, "Failed to fork process." % (e)
+                except OSError as e:
+                    print("Failed to fork process." % e, file=sys.stderr)
                     sys.exit(1)
+
                 # Close file descriptors so that we can detach
                 sys.stdout.close()
                 sys.stderr.close()
                 sys.stdin.close()
                 os.close(0)
                 os.close(1)
                 os.close(2)
@@ -263,51 +222,78 @@
 
         # PID MANAGEMENT
         if not options.skip_pidfile:
             # Finish Initialize PID file
             if not options.foreground and not options.collector:
                 # Write pid file
                 pid = str(os.getpid())
+
                 try:
-                    pf = file(options.pidfile, 'w+')
-                except IOError, e:
-                    log.error("Failed to write child PID file: %s" % (e))
+                    pf = open(options.pidfile, 'w+')
+                except IOError as e:
+                    log.error("Failed to write child PID file: %s" % e)
                     sys.exit(1)
+
                 pf.write("%s\n" % pid)
                 pf.close()
+
                 # Log
-                log.debug("Wrote child PID file: %s" % (options.pidfile))
+                log.debug("Wrote child PID file: %s" % options.pidfile)
 
         # Initialize Server
         server = Server(configfile=options.configfile)
 
-        def sigint_handler(signum, frame):
-            log.info("Signal Received: %d" % (signum))
+        def shutdown_handler(signum, frame):
+            log.info("Signal Received: %d" % signum)
+
             # Delete Pidfile
             if not options.skip_pidfile and os.path.exists(options.pidfile):
                 os.remove(options.pidfile)
+
                 # Log
-                log.debug("Removed PID file: %s" % (options.pidfile))
+                log.debug("Removed PID file: %s" % options.pidfile)
             for child in multiprocessing.active_children():
-                child.terminate()
+                if 'SyncManager' not in child.name:
+                    # The SyncManager process will immediately shutdown once
+                    # the parent (us) exits. If we explicitly terminate the
+                    # SyncManager here, we can't guarantee that it will exit
+                    # after all collector processes and the handler process have
+                    # exited first. As a result, since the collector and handler
+                    # processes push/retrieve items to/from the shared queue via
+                    # the SyncManager, it's possible for those processes to
+                    # terminate unexpectedly (crash). When this happens, when we
+                    # call exit(), we just hang and never actually terminate
+                    # gracefully. Therefore, do not explicitly terminate the
+                    # SyncManager process here but continue to terminate all
+                    # other processes (collectors and the handler) and perform a
+                    # join() on each of them. This guarantees that the
+                    # SyncManager is terminated last (implicitly as a result of
+                    # us exiting).
+                    child_debug = "Terminating and joining on: {} ({})"
+                    log.debug(child_debug.format(child.name, child.pid))
+                    child.terminate()
+                    child.join()
+
             sys.exit(0)
 
         # Set the signal handlers
-        signal.signal(signal.SIGINT, sigint_handler)
-        signal.signal(signal.SIGTERM, sigint_handler)
+        signal.signal(signal.SIGINT, shutdown_handler)
+        signal.signal(signal.SIGTERM, shutdown_handler)
 
         server.run()
 
     # Pass the exit up stream rather then handle it as an general exception
-    except SystemExit, e:
+    except SystemExit:
         raise SystemExit
 
-    except Exception, e:
+    except Exception as e:
         import traceback
         log.error("Unhandled exception: %s" % str(e))
         log.error("traceback: %s" % traceback.format_exc())
         sys.exit(1)
 
+
 if __name__ == "__main__":
     if setproctitle:
         setproctitle(os.path.basename(__file__))
+
     main()
```

### Comparing `diamond-next-4.0.515/bin/init.d/diamond` & `diamond-next-5.0.0/bin/init.d/diamond`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/bin/diamond-setup` & `diamond-next-5.0.0/bin/diamond-setup`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 #!/usr/bin/env python
-##########################################################################
 
+from __future__ import print_function
+
+import optparse
 import os
 import sys
-import optparse
 import traceback
 
 from configobj import ConfigObj
 
 try:
     from setproctitle import setproctitle
 except ImportError:
@@ -21,215 +22,204 @@
         sys.path.append(path)
         break
 
 from diamond.collector import Collector
 from diamond.collector import str_to_bool
 
 
-def getIncludePaths(path):
+def get_include_paths(path):
     for f in os.listdir(path):
-        cPath = os.path.abspath(os.path.join(path, f))
+        c_path = os.path.abspath(os.path.join(path, f))
 
-        if os.path.isfile(cPath) and len(f) > 3 and f[-3:] == '.py':
-            sys.path.append(os.path.dirname(cPath))
+        if os.path.isfile(c_path) and len(f) > 3 and f[-3:] == '.py':
+            sys.path.append(os.path.dirname(c_path))
 
     for f in os.listdir(path):
-        cPath = os.path.abspath(os.path.join(path, f))
-        if os.path.isdir(cPath):
-            getIncludePaths(cPath)
+        c_path = os.path.abspath(os.path.join(path, f))
+
+        if os.path.isdir(c_path):
+            get_include_paths(c_path)
+
 
 collectors = {}
 
 
-def getCollectors(path):
+def get_collectors(path):
     for f in os.listdir(path):
-        cPath = os.path.abspath(os.path.join(path, f))
+        c_path = os.path.abspath(os.path.join(path, f))
 
-        if (os.path.isfile(cPath)
-                and len(f) > 3
-                and f[-3:] == '.py'
-                and f[0:4] != 'test'):
+        if os.path.isfile(c_path) and len(f) > 3 and f[-3:] == '.py' and f[0:4] != 'test':
             modname = f[:-3]
+
             try:
                 # Import the module
                 module = __import__(modname, globals(), locals(), ['*'])
 
                 # Find the name
                 for attr in dir(module):
                     cls = getattr(module, attr)
+
                     try:
-                        if (issubclass(cls, Collector)
-                                and cls.__name__ not in collectors):
+                        if issubclass(cls, Collector) and cls.__name__ not in collectors:
                             collectors[cls.__name__] = module
                             break
                     except TypeError:
                         continue
-                # print "Imported module: %s %s" % (modname, cls.__name__)
+
+                # print("Imported module: %s %s" % (modname, cls.__name__))
             except Exception:
-                print "Failed to import module: %s. %s" % (
-                    modname, traceback.format_exc())
+                print("Failed to import module: %s. %s" % (modname, traceback.format_exc()))
                 collectors[modname] = False
                 continue
 
     for f in os.listdir(path):
-        cPath = os.path.abspath(os.path.join(path, f))
-        if os.path.isdir(cPath):
-            getCollectors(cPath)
+        c_path = os.path.abspath(os.path.join(path, f))
 
+        if os.path.isdir(c_path):
+            get_collectors(c_path)
 
-def typeToString(key):
-    if isinstance(obj.config[key], basestring):
+
+def type_to_string(key):
+    if isinstance(obj.config[key], str):
         user_val = obj.config[key]
     elif isinstance(obj.config[key], bool):
         user_val = str(obj.config[key])
     elif isinstance(obj.config[key], int):
         user_val = str(obj.config[key])
     elif isinstance(obj.config[key], list):
         user_val = str(obj.config[key])[1:-1]
     else:
         raise NotImplementedError("Unknown type!")
 
     return user_val
 
 
-def stringToType(key, val):
+def string_to_type(key, val):
     if type(obj.config[key]) is type(val):
         config_file[key] = val
-    elif isinstance(obj.config[key], basestring):
+    elif isinstance(obj.config[key], str):
         if val.lower() == 'false':
             config_file[key] = False
         elif val.lower() == 'true':
             config_file[key] = True
         else:
             config_file[key] = val
     elif isinstance(obj.config[key], bool):
-        if isinstance(val, basestring):
+        if isinstance(val, str):
             config_file[key] = str_to_bool(val)
         else:
             config_file[key] = bool(val)
     elif isinstance(obj.config[key], int):
         config_file[key] = int(val)
     elif isinstance(obj.config[key], list):
         entry = ConfigObj([key + ' = ' + val])
         config_file[key] = entry[key]
     else:
         raise NotImplementedError("Unknown type!")
 
 
-def boolCheck(val):
-    if isinstance(val, basestring):
+def bool_check(val):
+    if isinstance(val, str):
         return str_to_bool(val)
     elif isinstance(val, bool):
         return val
     else:
         raise NotImplementedError("Unknown type!")
 
 
-def configureKey(key):
+def configure_key(key):
     if not config_keys[key]:
         return
 
     try:
-        user_val = typeToString(key)
+        user_val = type_to_string(key)
     except NotImplementedError:
         return
 
-    print "\n"
+    print("\n")
+
     if key in default_conf_help:
-        print default_conf_help[key]
-    val = raw_input(key + ' [' + user_val + ']: ')
+        print(default_conf_help[key])
+
+    val = input(key + ' [' + user_val + ']: ')
 
     # Empty user input? Default to current value
     if len(val) == 0:
         val = obj.config[key]
 
     try:
-        stringToType(key, val)
+        string_to_type(key, val)
     except NotImplementedError:
         return
 
-##########################################################################
 
 if __name__ == "__main__":
-
     if setproctitle:
         setproctitle('diamond-setup')
 
     # Initialize Options
     parser = optparse.OptionParser()
-    parser.add_option("-c", "--configfile",
-                      dest="configfile",
-                      default="/etc/diamond/diamond.conf",
-                      help="Path to the config file")
-    parser.add_option("-C", "--collector",
-                      dest="collector",
-                      default=None,
-                      help="Configure a single collector")
-    parser.add_option("-p", "--print",
-                      action="store_true",
-                      dest="dump",
-                      default=False,
-                      help="Just print the defaults")
+    parser.add_option("-c", "--configfile", dest="configfile", default="/etc/diamond/diamond.conf", help="Path to the config file")
+    parser.add_option("-C", "--collector", dest="collector", default=None, help="Configure a single collector")
+    parser.add_option("-p", "--print", action="store_true", dest="dump", default=False, help="Just print the defaults")
 
     # Parse Command Line Args
     (options, args) = parser.parse_args()
 
     # Initialize Config
     if os.path.exists(options.configfile):
         config = ConfigObj(os.path.abspath(options.configfile))
     else:
-        print >> sys.stderr, "ERROR: Config file: %s does not exist." % (
-            options.configfile)
-        print >> sys.stderr, ("Please run python config.py -c"
-                              + " /path/to/diamond.conf")
+        print("ERROR: Config file: %s does not exist." % options.configfile, file=sys.stderr)
+        print("Please run python build_doc.py -c /path/to/diamond.conf", file=sys.stderr)
         parser.print_help(sys.stderr)
         sys.exit(1)
 
     if not options.dump:
-        print ''
-        print 'I will be over writing files in'
-        print config['server']['collectors_config_path']
-        print 'Please type yes to continue'
+        print('')
+        print('I will be over writing files in')
+        print(config['server']['collectors_config_path'])
+        print('Please type yes to continue')
+
+        val = input('Are you sure? ')
 
-        val = raw_input('Are you sure? ')
         if val != 'yes':
             sys.exit()
 
-    getIncludePaths(config['server']['collectors_path'])
-    getCollectors(config['server']['collectors_path'])
-    tests = []
+    get_include_paths(config['server']['collectors_path'])
+    get_collectors(config['server']['collectors_path'])
 
+    tests = []
     foundcollector = False
+
     for collector in collectors:
         if options.collector and collector != options.collector:
             continue
 
         # Skip configuring the basic collector object
         if collector == "Collector":
             continue
 
         foundcollector = True
 
         config_keys = {}
         config_file = ConfigObj()
-        config_file.filename = (config['server']['collectors_config_path']
-                                + "/" + collector + ".conf")
+        config_file.filename = (config['server']['collectors_config_path'] + "/" + collector + ".conf")
 
         # Find the class and load it from the collector module
         try:
-
             # We can for the name above, so we dont have to scan here anymore
             if not hasattr(collectors[collector], collector):
                 continue
 
             cls = getattr(collectors[collector], collector)
             obj = cls(config=config, handlers={})
 
             if options.dump:
-                print collector + " " + str(obj.config)
+                print(collector + " " + str(obj.config))
                 continue
 
             default_conf = obj.get_default_config()
             default_conf_help = obj.get_default_config_help()
 
             for key in obj.get_default_config():
                 config_keys[key] = True
@@ -237,31 +227,33 @@
             # Manage Keys
             config_keys['enabled'] = True
             config_keys['path'] = False
             config_keys['path_prefix'] = False
             config_keys['instance_prefix'] = False
             config_keys['interval'] = False
 
-            print "*" * 60
-            print "\n\t\tNow configuring " + collector
-            print collectors[collector].__doc__
+            print("*" * 60)
+            print("\n\t\tNow configuring " + collector)
+            print(collectors[collector].__doc__)
 
-            print "(%s)" % collector
-            configureKey('enabled')
+            print("(%s)" % collector)
+            configure_key('enabled')
 
-            if boolCheck(config_file['enabled']):
+            if bool_check(config_file['enabled']):
                 for key in config_keys:
                     if key == 'enabled':
                         continue
-                    configureKey(key)
+
+                    configure_key(key)
 
             config_file.write()
 
-        except IOError, (errno, strerror):
-            print "I/O error({0}): {1}".format(errno, strerror)
+        except IOError as ex:
+            print("I/O error({}): {}".format(ex.errno, ex.strerror))
         except KeyboardInterrupt:
-            print
+            print()
             sys.exit()
         except:
             continue
+
     if not foundcollector:
-        print "Collector not found."
+        print("Collector not found.")
```

### Comparing `diamond-next-4.0.515/src/diamond/utils/signals.py` & `diamond-next-5.0.0/src/diamond/utils/signals.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,20 +5,24 @@
 
 def signal_to_exception(signum, frame):
     """
     Called by the timeout alarm during the collector run time
     """
     if signum == signal.SIGALRM:
         raise SIGALRMException()
+
     if signum == signal.SIGHUP:
         raise SIGHUPException()
+
     if signum == signal.SIGUSR1:
         raise SIGUSR1Exception()
+
     if signum == signal.SIGUSR2:
         raise SIGUSR2Exception()
+
     raise SignalException(signum)
 
 
 class SignalException(Exception):
     pass
```

### Comparing `diamond-next-4.0.515/src/diamond/utils/scheduler.py` & `diamond-next-5.0.0/src/diamond/utils/scheduler.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,67 +1,69 @@
 # coding=utf-8
 
-import time
 import math
 import multiprocessing
 import os
 import random
-import sys
 import signal
+import sys
+import time
+
+from diamond.utils.signals import SIGALRMException, SIGHUPException, signal_to_exception
 
 try:
     from setproctitle import getproctitle, setproctitle
 except ImportError:
     setproctitle = None
 
-from diamond.utils.signals import signal_to_exception
-from diamond.utils.signals import SIGALRMException
-from diamond.utils.signals import SIGHUPException
-
 
 def collector_process(collector, metric_queue, log):
-    """
-    """
     proc = multiprocessing.current_process()
+
     if setproctitle:
-        setproctitle('%s - %s' % (getproctitle(), proc.name))
+        setproctitle("%s - %s" % (getproctitle(), proc.name))
 
     signal.signal(signal.SIGALRM, signal_to_exception)
     signal.signal(signal.SIGHUP, signal_to_exception)
     signal.signal(signal.SIGUSR2, signal_to_exception)
 
-    interval = float(collector.config['interval'])
+    # Reset signal handlers of forks/threads
+    signal.signal(signal.SIGINT, signal.SIG_DFL)
+    signal.signal(signal.SIGTERM, signal.SIG_DFL)
+
+    interval = float(collector.config["interval"])
 
-    log.debug('Starting')
-    log.debug('Interval: %s seconds', interval)
+    log.debug("Starting")
+    log.debug("Interval: %s seconds", interval)
 
     # Validate the interval
     if interval <= 0:
-        log.critical('interval of %s is not valid!', interval)
+        log.critical("interval of %s is not valid!", interval)
         sys.exit(1)
 
     # Start the next execution at the next window plus some stagger delay to
     # avoid having all collectors running at the same time
     next_window = math.floor(time.time() / interval) * interval
     stagger_offset = random.uniform(0, interval - 1)
 
     # Allocate time till the end of the window for the collector to run. With a
     # minimum of 1 second
     max_time = int(max(interval - stagger_offset, 1))
-    log.debug('Max collection time: %s seconds', max_time)
+    log.debug("Max collection time: %s seconds", max_time)
 
     # Setup stderr/stdout as /dev/null so random print statements in thrid
     # party libs do not fail and prevent collectors from running.
     # https://github.com/BrightcoveOS/Diamond/issues/722
-    sys.stdout = open(os.devnull, 'w')
-    sys.stderr = open(os.devnull, 'w')
+    sys.stdout = open(os.devnull, "w")
+    sys.stderr = open(os.devnull, "w")
 
-    while(True):
+    while True:
         try:
             time_to_sleep = (next_window + stagger_offset) - time.time()
+
             if time_to_sleep > 0:
                 time.sleep(time_to_sleep)
             elif time_to_sleep < 0:
                 # clock has jumped, lets skip missed intervals
                 next_window = time.time()
 
             next_window += interval
@@ -72,45 +74,46 @@
             # Collect!
             collector._run()
 
             # Success! Disable the alarm
             signal.alarm(0)
 
         except SIGALRMException:
-            log.error('Took too long to run! Killed!')
+            log.error("Took too long to run! Killed!")
 
-            # Adjust  the stagger_offset to allow for more time to run the
-            # collector
+            # Adjust the stagger_offset to allow for more time to run the collector
             stagger_offset = stagger_offset * 0.9
 
             max_time = int(max(interval - stagger_offset, 1))
-            log.debug('Max collection time: %s seconds', max_time)
+            log.debug("Max collection time: %s seconds", max_time)
 
         except SIGHUPException:
             # Reload the config if requested
             # We must first disable the alarm as we don't want it to interrupt
             # us and end up with half a loaded config
             signal.alarm(0)
 
-            log.info('Reloading config reload due to HUP')
+            log.info("Reloading config reload due to HUP")
             collector.load_config()
-            log.info('Config reloaded')
+            log.info("Config reloaded")
 
         except Exception:
-            log.exception('Collector failed!')
+            log.exception("Collector failed!")
             break
 
 
 def handler_process(handlers, metric_queue, log):
     proc = multiprocessing.current_process()
+
     if setproctitle:
-        setproctitle('%s - %s' % (getproctitle(), proc.name))
+        setproctitle("%s - %s" % (getproctitle(), proc.name))
 
-    log.debug('Starting process %s', proc.name)
+    log.debug("Starting process %s", proc.name)
 
-    while(True):
+    while True:
         metric = metric_queue.get(block=True, timeout=None)
+
         for handler in handlers:
             if metric is not None:
                 handler._process(metric)
             else:
                 handler._flush()
```

### Comparing `diamond-next-4.0.515/src/diamond/utils/log.py` & `diamond-next-5.0.0/src/diamond/utils/log.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,61 +1,56 @@
 # coding=utf-8
 
-
 import logging
 import logging.config
-import sys
 import os
 
+import sys
 
-class DebugFormatter(logging.Formatter):
 
+class DebugFormatter(logging.Formatter):
     def __init__(self, fmt=None):
         if fmt is None:
-            fmt = ('%(created)s\t' +
-                   '[%(processName)s:%(process)d:%(levelname)s]\t' +
-                   '%(message)s')
+            fmt = (
+                "%(created)s\t[%(processName)s:%(process)d:%(levelname)s]\t%(message)s"
+            )
+
         self.fmt_default = fmt
-        self.fmt_prefix = fmt.replace('%(message)s', '')
+        self.fmt_prefix = fmt.replace("%(message)s", "")
         logging.Formatter.__init__(self, fmt)
 
     def format(self, record):
         self._fmt = self.fmt_default
 
         if record.levelno in [logging.ERROR, logging.CRITICAL]:
-            self._fmt = ''
+            self._fmt = ""
             self._fmt += self.fmt_prefix
-            self._fmt += '%(message)s'
-            self._fmt += '\n'
+            self._fmt += "%(message)s"
+            self._fmt += "\n"
             self._fmt += self.fmt_prefix
-            self._fmt += '%(pathname)s:%(lineno)d'
+            self._fmt += "%(pathname)s:%(lineno)d"
 
         return logging.Formatter.format(self, record)
 
 
 def setup_logging(configfile, stdout=False):
-    log = logging.getLogger('diamond')
+    log = logging.getLogger("diamond")
+
+    try:
+        logging.config.fileConfig(configfile, disable_existing_loggers=False)
 
-    if stdout:
-        log.setLevel(logging.DEBUG)
-        streamHandler = logging.StreamHandler(sys.stdout)
-        streamHandler.setFormatter(DebugFormatter())
-        streamHandler.setLevel(logging.DEBUG)
-        log.addHandler(streamHandler)
-    else:
-        try:
-            if sys.version_info >= (2, 6):
-                logging.config.fileConfig(configfile,
-                                          disable_existing_loggers=False)
-            else:
-                # python <= 2.5 does not have disable_existing_loggers
-                # default was to always disable them, in our case we want to
-                # keep any logger created by handlers
-                logging.config.fileConfig(configfile)
-                for logger in logging.root.manager.loggerDict.values():
-                    logger.disabled = 0
-        except Exception, e:
-            sys.stderr.write("Error occurs when initialize logging: ")
-            sys.stderr.write(str(e))
-            sys.stderr.write(os.linesep)
+        # if the stdout flag is set, we use the log level of the root logger
+        # for logging to stdout, and keep all loggers defined in the conf file
+        if stdout:
+            root_log_level = logging.getLogger().getEffectiveLevel()
+
+            log.setLevel(root_log_level)
+            stream_handler = logging.StreamHandler(sys.stdout)
+            stream_handler.setFormatter(DebugFormatter())
+            stream_handler.setLevel(root_log_level)
+            log.addHandler(stream_handler)
+    except Exception as e:
+        sys.stderr.write("Error occurs when initialize logging: ")
+        sys.stderr.write(str(e))
+        sys.stderr.write(os.linesep)
 
     return log
```

### Comparing `diamond-next-4.0.515/src/diamond/utils/classes.py` & `diamond-next-5.0.0/src/diamond/utils/classes.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,238 +1,263 @@
 # coding=utf-8
 
-import configobj
-import os
-import sys
-import logging
+import importlib
+import importlib.util
 import inspect
+import logging
+import os
 import traceback
+
+import configobj
 import pkg_resources
+import sys
 
-from diamond.util import load_class_from_name
 from diamond.collector import Collector
 from diamond.handler.Handler import Handler
+from diamond.util import load_class_from_name
 
-logger = logging.getLogger('diamond')
+logger = logging.getLogger("diamond")
 
 
 def load_include_path(paths):
     """
     Scan for and add paths to the include path
     """
     for path in paths:
         # Verify the path is valid
         if not os.path.isdir(path):
             continue
-        # Add path to the system path, to avoid name clashes
-        # with mysql-connector for example ...
+
+        # Add path to the system path, to avoid name clashes with mysql-connector for example ...
         if path not in sys.path:
             sys.path.insert(1, path)
+
         # Load all the files in path
         for f in os.listdir(path):
             # Are we a directory? If so process down the tree
             fpath = os.path.join(path, f)
+
             if os.path.isdir(fpath):
                 load_include_path([fpath])
 
 
 def load_dynamic_class(fqn, subclass):
     """
     Dynamically load fqn class and verify it's a subclass of subclass
     """
-    if not isinstance(fqn, basestring):
+    if not isinstance(fqn, str):
         return fqn
 
     cls = load_class_from_name(fqn)
 
     if cls == subclass or not issubclass(cls, subclass):
         raise TypeError("%s is not a valid %s" % (fqn, subclass.__name__))
 
     return cls
 
 
 def load_handlers(config, handler_names):
     """
     Load handlers
     """
-
     handlers = []
 
-    if isinstance(handler_names, basestring):
+    if isinstance(handler_names, str):
         handler_names = [handler_names]
 
     for handler in handler_names:
-        logger.debug('Loading Handler %s', handler)
+        logger.debug("Loading Handler %s", handler)
+
         try:
             # Load Handler Class
             cls = load_dynamic_class(handler, Handler)
             cls_name = cls.__name__
 
             # Initialize Handler config
             handler_config = configobj.ConfigObj()
+
             # Merge default Handler default config
-            handler_config.merge(config['handlers']['default'])
+            handler_config.merge(config["handlers"]["handlersDefault"])
+
             # Check if Handler config exists
-            if cls_name in config['handlers']:
+            if cls_name in config["handlers"]:
                 # Merge Handler config section
-                handler_config.merge(config['handlers'][cls_name])
+                handler_config.merge(config["handlers"][cls_name])
 
             # Check for config file in config directory
-            if 'handlers_config_path' in config['server']:
-                configfile = os.path.join(
-                    config['server']['handlers_config_path'],
-                    cls_name) + '.conf'
+            if "handlers_config_path" in config["server"]:
+                configfile = (
+                    os.path.join(config["server"]["handlers_config_path"], cls_name)
+                    + ".conf"
+                )
+
                 if os.path.exists(configfile):
                     # Merge Collector config file
                     handler_config.merge(configobj.ConfigObj(configfile))
 
             # Initialize Handler class
             h = cls(handler_config)
             handlers.append(h)
 
         except (ImportError, SyntaxError):
             # Log Error
-            logger.warning("Failed to load handler %s. %s",
-                           handler,
-                           traceback.format_exc())
+            logger.warning(
+                "Failed to load handler %s. %s", handler, traceback.format_exc()
+            )
             continue
 
     return handlers
 
 
 def load_collectors(paths):
     """
     Load all collectors
     """
     collectors = load_collectors_from_paths(paths)
-    collectors.update(load_collectors_from_entry_point('diamond.collectors'))
+    collectors.update(load_collectors_from_entry_point("diamond.collectors"))
+
     return collectors
 
 
 def load_collectors_from_paths(paths):
     """
     Scan for collectors to load from path
     """
     # Initialize return value
     collectors = {}
 
     if paths is None:
         return
 
-    if isinstance(paths, basestring):
-        paths = paths.split(',')
-        paths = map(str.strip, paths)
+    if isinstance(paths, str):
+        paths = paths.split(",")
+        paths = list(map(str.strip, paths))
 
     load_include_path(paths)
 
     for path in paths:
         # Get a list of files in the directory, if the directory exists
         if not os.path.exists(path):
             raise OSError("Directory does not exist: %s" % path)
 
-        if path.endswith('tests') or path.endswith('fixtures'):
+        if path.endswith("tests") or path.endswith("fixtures"):
             return collectors
 
         # Load all the files in path
         for f in os.listdir(path):
-
             # Are we a directory? If so process down the tree
             fpath = os.path.join(path, f)
+
             if os.path.isdir(fpath):
                 subcollectors = load_collectors_from_paths([fpath])
+
                 for key in subcollectors:
                     collectors[key] = subcollectors[key]
 
             # Ignore anything that isn't a .py file
-            elif (os.path.isfile(fpath) and
-                  len(f) > 3 and
-                  f[-3:] == '.py' and
-                  f[0:4] != 'test' and
-                  f[0] != '.'):
-
+            elif (
+                os.path.isfile(fpath)
+                and len(f) > 3
+                and f[-3:] == ".py"
+                and f[0:4] != "test"
+                and f[0] != "."
+            ):
                 modname = f[:-3]
 
+                spec = importlib.util.spec_from_file_location(
+                    modname, "%s/%s.py" % (path, modname)
+                )
+
                 try:
                     # Import the module
-                    mod = __import__(modname, globals(), locals(), ['*'])
-                except (KeyboardInterrupt, SystemExit), err:
+                    mod = spec.loader.load_module(modname)
+                except (KeyboardInterrupt, SystemExit) as err:
                     logger.error(
-                        "System or keyboard interrupt "
-                        "while loading module %s"
-                        % modname)
+                        "System or keyboard interrupt while loading module %s" % modname
+                    )
+
                     if isinstance(err, SystemExit):
                         sys.exit(err.code)
+
                     raise KeyboardInterrupt
                 except Exception:
                     # Log error
-                    logger.error("Failed to import module: %s. %s",
-                                 modname,
-                                 traceback.format_exc())
+                    logger.error(
+                        "Failed to import module: %s. %s",
+                        modname,
+                        traceback.format_exc(),
+                    )
                 else:
                     for name, cls in get_collectors_from_module(mod):
                         collectors[name] = cls
 
     # Return Collector classes
     return collectors
 
 
 def load_collectors_from_entry_point(path):
     """
     Load collectors that were installed into an entry_point.
     """
     collectors = {}
+
     for ep in pkg_resources.iter_entry_points(path):
         try:
             mod = ep.load()
         except Exception:
-            logger.error('Failed to import entry_point: %s. %s',
-                         ep.name,
-                         traceback.format_exc())
+            logger.error(
+                "Failed to import entry_point: %s. %s", ep.name, traceback.format_exc()
+            )
         else:
             collectors.update(get_collectors_from_module(mod))
+
     return collectors
 
 
 def get_collectors_from_module(mod):
     """
     Locate all of the collector classes within a given module
     """
     for attrname in dir(mod):
         attr = getattr(mod, attrname)
-        # Only attempt to load classes that are infact classes
-        # are Collectors but are not the base Collector class
-        if ((inspect.isclass(attr) and
-             issubclass(attr, Collector) and
-             attr != Collector)):
-            if attrname.startswith('parent_'):
+
+        # Only attempt to load classes that are infact classes are Collectors but are not the base Collector class
+        if inspect.isclass(attr) and issubclass(attr, Collector) and attr != Collector:
+            if attrname.startswith("parent_"):
                 continue
+
             # Get class name
-            fqcn = '.'.join([mod.__name__, attrname])
+            fqcn = ".".join([mod.__name__, attrname])
+
             try:
                 # Load Collector class
                 cls = load_dynamic_class(fqcn, Collector)
+
                 # Add Collector class
                 yield cls.__name__, cls
             except Exception:
                 # Log error
                 logger.error(
-                    "Failed to load Collector: %s. %s",
-                    fqcn, traceback.format_exc())
+                    "Failed to load Collector: %s. %s", fqcn, traceback.format_exc()
+                )
                 continue
 
 
 def initialize_collector(cls, name=None, configfile=None, handlers=[]):
     """
     Initialize collector
     """
     collector = None
 
     try:
         # Initialize Collector
         collector = cls(name=name, configfile=configfile, handlers=handlers)
     except Exception:
         # Log error
-        logger.error("Failed to initialize Collector: %s. %s",
-                     cls.__name__, traceback.format_exc())
+        logger.error(
+            "Failed to initialize Collector: %s. %s",
+            cls.__name__,
+            traceback.format_exc(),
+        )
 
     # Return collector
     return collector
```

### Comparing `diamond-next-4.0.515/src/diamond/utils/config.py` & `diamond-next-5.0.0/src/diamond/utils/config.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 # coding=utf-8
 
-import configobj
 import os
 
+import configobj
+
 
 def str_to_bool(value):
     """
     Converts string truthy/falsey strings to a bool
     Empty strings are false
     """
-    if isinstance(value, basestring):
+    if isinstance(value, str):
         value = value.strip().lower()
-        if value in ['true', 't', 'yes', 'y']:
+
+        if value in ["true", "t", "yes", "y"]:
             return True
-        elif value in ['false', 'f', 'no', 'n', '']:
+        elif value in ["false", "f", "no", "n", ""]:
             return False
         else:
             raise NotImplementedError("Unknown bool %s" % value)
 
     return value
 
 
@@ -25,95 +27,103 @@
     """
     Load the full config / merge splitted configs if configured
     """
 
     configfile = os.path.abspath(configfile)
     config = configobj.ConfigObj(configfile)
 
-    config_extension = '.conf'
+    config_extension = ".conf"
 
     #########################################################################
     # Load up other config files
     #########################################################################
 
-    if 'configs' in config:
-        config_extension = config['configs'].get('extension', config_extension)
+    if "configs" in config:
+        config_extension = config["configs"].get("extension", config_extension)
 
         # Load other configs
-        if 'path' in config['configs']:
-            for cfgfile in os.listdir(config['configs']['path']):
-                cfgfile = os.path.join(config['configs']['path'],
-                                       cfgfile)
+        if "path" in config["configs"]:
+            for cfgfile in os.listdir(config["configs"]["path"]):
+                cfgfile = os.path.join(config["configs"]["path"], cfgfile)
                 cfgfile = os.path.abspath(cfgfile)
+
                 if not cfgfile.endswith(config_extension):
                     continue
+
                 newconfig = configobj.ConfigObj(cfgfile)
                 config.merge(newconfig)
 
     #########################################################################
 
-    if 'server' not in config:
-        raise Exception('Failed to load config file %s!' % configfile)
+    if "server" not in config:
+        raise Exception("Failed to load config file %s!" % configfile)
 
     #########################################################################
     # Load up handler specific configs
     #########################################################################
 
-    if 'handlers' not in config:
-        config['handlers'] = configobj.ConfigObj()
+    if "handlers" not in config:
+        config["handlers"] = configobj.ConfigObj()
+
+    if "handlers_config_path" in config["server"]:
+        handlers_config_path = config["server"]["handlers_config_path"]
 
-    if 'handlers_config_path' in config['server']:
-        handlers_config_path = config['server']['handlers_config_path']
         if os.path.exists(handlers_config_path):
             for cfgfile in os.listdir(handlers_config_path):
                 cfgfile = os.path.join(handlers_config_path, cfgfile)
                 cfgfile = os.path.abspath(cfgfile)
+
                 if not cfgfile.endswith(config_extension):
                     continue
+
                 filename = os.path.basename(cfgfile)
                 handler = os.path.splitext(filename)[0]
 
-                if handler not in config['handlers']:
-                    config['handlers'][handler] = configobj.ConfigObj()
+                if handler not in config["handlers"]:
+                    config["handlers"][handler] = configobj.ConfigObj()
 
                 newconfig = configobj.ConfigObj(cfgfile)
-                config['handlers'][handler].merge(newconfig)
+                config["handlers"][handler].merge(newconfig)
 
     #########################################################################
     # Load up Collector specific configs
     #########################################################################
 
-    if 'collectors' not in config:
-        config['collectors'] = configobj.ConfigObj()
+    if "collectors" not in config:
+        config["collectors"] = configobj.ConfigObj()
+
+    if "collectors_config_path" in config["server"]:
+        collectors_config_path = config["server"]["collectors_config_path"]
 
-    if 'collectors_config_path' in config['server']:
-        collectors_config_path = config['server']['collectors_config_path']
         if os.path.exists(collectors_config_path):
             for cfgfile in os.listdir(collectors_config_path):
                 cfgfile = os.path.join(collectors_config_path, cfgfile)
                 cfgfile = os.path.abspath(cfgfile)
+
                 if not cfgfile.endswith(config_extension):
                     continue
+
                 filename = os.path.basename(cfgfile)
                 collector = os.path.splitext(filename)[0]
 
-                if collector not in config['collectors']:
-                    config['collectors'][collector] = configobj.ConfigObj()
+                if collector not in config["collectors"]:
+                    config["collectors"][collector] = configobj.ConfigObj()
 
                 try:
                     newconfig = configobj.ConfigObj(cfgfile)
-                except Exception, e:
-                    raise Exception("Failed to load config file %s due to %s" %
-                                    (cfgfile, e))
+                except Exception as e:
+                    raise Exception(
+                        "Failed to load config file %s due to %s" % (cfgfile, e)
+                    )
 
-                config['collectors'][collector].merge(newconfig)
+                config["collectors"][collector].merge(newconfig)
 
     # Convert enabled to a bool
-    for collector in config['collectors']:
-        if 'enabled' in config['collectors'][collector]:
-            config['collectors'][collector]['enabled'] = str_to_bool(
-                config['collectors'][collector]['enabled']
+    for collector in config["collectors"]:
+        if "enabled" in config["collectors"][collector]:
+            config["collectors"][collector]["enabled"] = str_to_bool(
+                config["collectors"][collector]["enabled"]
             )
 
     #########################################################################
 
     return config
```

### Comparing `diamond-next-4.0.515/src/diamond/util.py` & `diamond-next-5.0.0/src/diamond/util.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,52 +1,60 @@
 # coding=utf-8
 
+import inspect
 import os
 import sys
-import inspect
 
 
 def get_diamond_version():
     try:
         from diamond.version import __VERSION__
+
         return __VERSION__
     except ImportError:
         return "Unknown"
 
 
 def load_modules_from_path(path):
     """
     Import all modules from the given directory
     """
+
     # Check and fix the path
-    if path[-1:] != '/':
-        path += '/'
+    if path[-1:] != "/":
+        path += "/"
 
     # Get a list of files in the directory, if the directory exists
     if not os.path.exists(path):
         raise OSError("Directory does not exist: %s" % path)
 
     # Add path to the system path
     sys.path.append(path)
+
     # Load all the files in path
     for f in os.listdir(path):
         # Ignore anything that isn't a .py file
-        if len(f) > 3 and f[-3:] == '.py':
+        if len(f) > 3 and f[-3:] == ".py":
             modname = f[:-3]
+
             # Import the module
-            __import__(modname, globals(), locals(), ['*'])
+            __import__(modname, globals(), locals(), ["*"])
 
 
 def load_class_from_name(fqcn):
     # Break apart fqcn to get module and classname
-    paths = fqcn.split('.')
-    modulename = '.'.join(paths[:-1])
+    paths = fqcn.split(".")
+    modulename = ".".join(paths[:-1])
     classname = paths[-1]
+
     # Import the module
-    __import__(modulename, globals(), locals(), ['*'])
+    __import__(modulename, globals(), locals(), ["*"])
+
     # Get the class
     cls = getattr(sys.modules[modulename], classname)
+
     # Check cls
     if not inspect.isclass(cls):
         raise TypeError("%s is not a class" % fqcn)
+
     # Return class
     return cls
```

### Comparing `diamond-next-4.0.515/src/diamond/convertor.py` & `diamond-next-5.0.0/src/diamond/convertor.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,81 +1,99 @@
 # coding=utf-8
 
 import re
 
-_RE_FIND_FIRST_CAP = re.compile('(.)([A-Z][a-z]+)')
-_RE_SPAN_OF_CAPS = re.compile('([a-z0-9])([A-Z])')
+_RE_FIND_FIRST_CAP = re.compile("(.)([A-Z][a-z]+)")
+_RE_SPAN_OF_CAPS = re.compile("([a-z0-9])([A-Z])")
 
 
 def camelcase_to_underscore(name):
-    return _RE_SPAN_OF_CAPS.sub(r'\1_\2',
-                                _RE_FIND_FIRST_CAP.sub(r'\1_\2', name)
-                                ).lower()
+    return _RE_SPAN_OF_CAPS.sub(
+        r"\1_\2", _RE_FIND_FIRST_CAP.sub(r"\1_\2", name)
+    ).lower()
 
 
 class binary:
     """
     Store the value in bits so we can convert between things easily
     """
+
     value = None
 
     def __init__(self, value=None, unit=None):
         self.do(value=value, unit=unit)
 
     @staticmethod
-    def convert(value=None, oldUnit=None, newUnit=None):
-        convertor = binary(value=value, unit=oldUnit)
-        return convertor.get(unit=newUnit)
+    def convert(value=None, old_unit=None, new_unit=None):
+        convertor = binary(value=value, unit=old_unit)
+
+        return convertor.get(unit=new_unit)
 
     def set(self, value, unit=None):
         return self.do(value=value, unit=unit)
 
     def get(self, unit=None):
         return self.do(unit=unit)
 
     def do(self, value=None, unit=None):
         if not unit:
             return self.bit(value=value)
 
-        if unit in ['bit', 'b']:
+        if unit in ["bit", "b"]:
             return self.bit(value=value)
-        if unit in ['kilobit', 'kbit', 'Kibit']:
+
+        if unit in ["kilobit", "kbit", "Kibit"]:
             return self.kilobit(value=value)
-        if unit in ['megabit', 'Mbit', 'Mibit', 'Mbit']:
+
+        if unit in ["megabit", "Mbit", "Mibit", "Mbit"]:
             return self.megabit(value=value)
-        if unit in ['gigabit', 'Gbit', 'Gibit']:
+
+        if unit in ["gigabit", "Gbit", "Gibit"]:
             return self.gigabit(value=value)
-        if unit in ['terabit', 'Tbit', 'Tibit']:
+
+        if unit in ["terabit", "Tbit", "Tibit"]:
             return self.terabit(value=value)
-        if unit in ['petabit', 'Pbit', 'Pibit']:
+
+        if unit in ["petabit", "Pbit", "Pibit"]:
             return self.petabit(value=value)
-        if unit in ['exabit', 'Ebit', 'Eibit']:
+
+        if unit in ["exabit", "Ebit", "Eibit"]:
             return self.exabit(value=value)
-        if unit in ['zettabit', 'Zbit', 'Zibit']:
+
+        if unit in ["zettabit", "Zbit", "Zibit"]:
             return self.zettabit(value=value)
-        if unit in ['yottabit', 'Ybit', 'Yibit']:
+
+        if unit in ["yottabit", "Ybit", "Yibit"]:
             return self.yottabit(value=value)
 
-        if unit in ['byte', 'B']:
+        if unit in ["byte", "B"]:
             return self.byte(value=value)
-        if unit in ['kilobyte', 'kB', 'KiB']:
+
+        if unit in ["kilobyte", "kB", "KiB"]:
             return self.kilobyte(value=value)
-        if unit in ['megabyte', 'MB', 'MiB', 'Mbyte']:
+
+        if unit in ["megabyte", "MB", "MiB", "Mbyte"]:
             return self.megabyte(value=value)
-        if unit in ['gigabyte', 'GB', 'GiB']:
+
+        if unit in ["gigabyte", "GB", "GiB"]:
             return self.gigabyte(value=value)
-        if unit in ['terabyte', 'TB', 'TiB']:
+
+        if unit in ["terabyte", "TB", "TiB"]:
             return self.terabyte(value=value)
-        if unit in ['petabyte', 'PB', 'PiB']:
+
+        if unit in ["petabyte", "PB", "PiB"]:
             return self.petabyte(value=value)
-        if unit in ['exabyte', 'EB', 'EiB']:
+
+        if unit in ["exabyte", "EB", "EiB"]:
             return self.exabyte(value=value)
-        if unit in ['zettabyte', 'ZB', 'ZiB']:
+
+        if unit in ["zettabyte", "ZB", "ZiB"]:
             return self.zettabyte(value=value)
-        if unit in ['yottabyte', 'YB', 'YiB']:
+
+        if unit in ["yottabyte", "YB", "YiB"]:
             return self.yottabyte(value=value)
 
         raise NotImplementedError("unit %s" % unit)
 
     def bit(self, value=None):
         if value is None:
             return self.value
@@ -143,48 +161,50 @@
         return self.convertb(value, self.byte, 8)
 
 
 class time:
     """
     Store the value in miliseconds so we can convert between things easily
     """
+
     value = None
 
     def __init__(self, value=None, unit=None):
         self.do(value=value, unit=unit)
 
     @staticmethod
-    def convert(value=None, oldUnit=None, newUnit=None):
-        convertor = time(value=value, unit=oldUnit)
-        return convertor.get(unit=newUnit)
+    def convert(value=None, old_unit=None, new_unit=None):
+        convertor = time(value=value, unit=old_unit)
+
+        return convertor.get(unit=new_unit)
 
     def set(self, value, unit=None):
         return self.do(value=value, unit=unit)
 
     def get(self, unit=None):
         return self.do(unit=unit)
 
     def do(self, value=None, unit=None):
         if not unit:
             v = self.millisecond(value=value)
-        elif unit.lower() in ['millisecond', 'milliseconds', 'ms']:
+        elif unit.lower() in ["millisecond", "milliseconds", "ms"]:
             v = self.millisecond(value=value)
-        elif unit.lower() in ['second', 'seconds', 's']:
+        elif unit.lower() in ["second", "seconds", "s"]:
             v = self.second(value=value)
-        elif unit.lower() in ['minute', 'minutes', 'm']:
+        elif unit.lower() in ["minute", "minutes", "m"]:
             v = self.minute(value=value)
-        elif unit.lower() in ['hour', 'hours', 'h']:
+        elif unit.lower() in ["hour", "hours", "h"]:
             v = self.hour(value=value)
-        elif unit.lower() in ['day', 'days', 'd']:
+        elif unit.lower() in ["day", "days", "d"]:
             v = self.day(value=value)
-        elif unit.lower() in ['year', 'years', 'y']:
+        elif unit.lower() in ["year", "years", "y"]:
             v = self.year(value=value)
-        elif unit.lower() in ['microsecond', 'microseconds', 'us']:
+        elif unit.lower() in ["microsecond", "microseconds", "us"]:
             v = self.microsecond(value=value)
-        elif unit.lower() in ['nanosecond', 'nanoseconds', 'ns']:
+        elif unit.lower() in ["nanosecond", "nanoseconds", "ns"]:
             v = self.nanosecond(value=value)
         else:
             raise NotImplementedError("unit %s" % unit)
 
         return v
 
     def millisecond(self, value=None):
```

### Comparing `diamond-next-4.0.515/src/diamond/gmetric.py` & `diamond-next-5.0.0/src/diamond/gmetric.py`

 * *Files 9% similar despite different names*

```diff
@@ -33,158 +33,212 @@
 #   made gmetrix xdr writers _and readers_
 #   Now this only works for gmond 2.X packets, not tested with 3.X
 #
 # Version 3.0 - 09-Jan-2011 Author: Vladimir Vuksan
 #   Made it work with the Ganglia 3.1 data format
 
 
-from xdrlib import Packer, Unpacker
+from __future__ import print_function
+
 import socket
+from xdrlib import Packer, Unpacker
 
-slope_str2int = {'zero': 0,
-                 'positive': 1,
-                 'negative': 2,
-                 'both': 3,
-                 'unspecified': 4}
+slope_str2int = {"zero": 0, "positive": 1, "negative": 2, "both": 3, "unspecified": 4}
 
 # could be autogenerated from previous but whatever
-slope_int2str = {0: 'zero',
-                 1: 'positive',
-                 2: 'negative',
-                 3: 'both',
-                 4: 'unspecified'}
+slope_int2str = {0: "zero", 1: "positive", 2: "negative", 3: "both", 4: "unspecified"}
 
 
 class Gmetric:
     """
     Class to send gmetric/gmond 2.X packets
 
     Thread safe
     """
 
-    type = ('', 'string', 'uint16', 'int16', 'uint32', 'int32', 'float',
-            'double', 'timestamp')
-    protocol = ('udp', 'multicast')
+    type = (
+        "",
+        "string",
+        "uint16",
+        "int16",
+        "uint32",
+        "int32",
+        "float",
+        "double",
+        "timestamp",
+    )
+    protocol = ("udp", "multicast")
 
     def __init__(self, host, port, protocol):
         if protocol not in self.protocol:
             raise ValueError("Protocol must be one of: " + str(self.protocol))
 
         self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-        if protocol == 'multicast':
-            self.socket.setsockopt(socket.IPPROTO_IP,
-                                   socket.IP_MULTICAST_TTL, 20)
+
+        if protocol == "multicast":
+            self.socket.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 20)
+
         self.hostport = (host, int(port))
         # self.socket.connect(self.hostport)
 
-    def send(self, NAME, VAL, TYPE='', UNITS='', SLOPE='both',
-             TMAX=60, DMAX=0, GROUP=""):
-        if SLOPE not in slope_str2int:
+    def send(
+        self, name, val, ttype="", units="", slope="both", tmax=60, dmax=0, group=""
+    ):
+        if slope not in slope_str2int:
             raise ValueError("Slope must be one of: " + str(self.slope.keys()))
-        if TYPE not in self.type:
+
+        if ttype not in self.type:
             raise ValueError("Type must be one of: " + str(self.type))
-        if len(NAME) == 0:
+
+        if len(name) == 0:
             raise ValueError("Name must be non-empty")
 
-        (meta_msg, data_msg) = gmetric_write(NAME,
-                                             VAL,
-                                             TYPE,
-                                             UNITS,
-                                             SLOPE,
-                                             TMAX,
-                                             DMAX,
-                                             GROUP)
-        # print msg
+        (meta_msg, data_msg) = gmetric_write(
+            name, val, ttype, units, slope, tmax, dmax, group
+        )
 
         self.socket.sendto(meta_msg, self.hostport)
         self.socket.sendto(data_msg, self.hostport)
 
 
-def gmetric_write(NAME, VAL, TYPE, UNITS, SLOPE, TMAX, DMAX, GROUP):
+def gmetric_write(name, val, ttype, units, slope, tmax, dmax, group):
     """
     Arguments are in all upper-case to match XML
     """
     packer = Packer()
-    HOSTNAME = "test"
-    SPOOF = 0
+    hostname = "test"
+    spoof = 0
+
     # Meta data about a metric
     packer.pack_int(128)
-    packer.pack_string(HOSTNAME)
-    packer.pack_string(NAME)
-    packer.pack_int(SPOOF)
-    packer.pack_string(TYPE)
-    packer.pack_string(NAME)
-    packer.pack_string(UNITS)
-    packer.pack_int(slope_str2int[SLOPE])  # map slope string to int
-    packer.pack_uint(int(TMAX))
-    packer.pack_uint(int(DMAX))
+    packer.pack_string(hostname)
+    packer.pack_string(name)
+    packer.pack_int(spoof)
+    packer.pack_string(ttype)
+    packer.pack_string(name)
+    packer.pack_string(units)
+    packer.pack_int(slope_str2int[slope])  # map slope string to int
+    packer.pack_uint(int(tmax))
+    packer.pack_uint(int(dmax))
+
     # Magic number. Indicates number of entries to follow. Put in 1 for GROUP
-    if GROUP == "":
+    if group == "":
         packer.pack_int(0)
     else:
         packer.pack_int(1)
         packer.pack_string("GROUP")
-        packer.pack_string(GROUP)
+        packer.pack_string(group)
 
     # Actual data sent in a separate packet
     data = Packer()
     data.pack_int(128 + 5)
-    data.pack_string(HOSTNAME)
-    data.pack_string(NAME)
-    data.pack_int(SPOOF)
+    data.pack_string(hostname)
+    data.pack_string(name)
+    data.pack_int(spoof)
     data.pack_string("%s")
-    data.pack_string(str(VAL))
+    data.pack_string(str(val))
 
-    return packer.get_buffer(),  data.get_buffer()
+    return packer.get_buffer(), data.get_buffer()
 
 
 def gmetric_read(msg):
     unpacker = Unpacker(msg)
     values = dict()
     unpacker.unpack_int()
-    values['TYPE'] = unpacker.unpack_string()
-    values['NAME'] = unpacker.unpack_string()
-    values['VAL'] = unpacker.unpack_string()
-    values['UNITS'] = unpacker.unpack_string()
-    values['SLOPE'] = slope_int2str[unpacker.unpack_int()]
-    values['TMAX'] = unpacker.unpack_uint()
-    values['DMAX'] = unpacker.unpack_uint()
+    values["TYPE"] = unpacker.unpack_string()
+    values["NAME"] = unpacker.unpack_string()
+    values["VAL"] = unpacker.unpack_string()
+    values["UNITS"] = unpacker.unpack_string()
+    values["SLOPE"] = slope_int2str[unpacker.unpack_int()]
+    values["TMAX"] = unpacker.unpack_uint()
+    values["DMAX"] = unpacker.unpack_uint()
     unpacker.done()
+
     return values
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     import optparse
+
     parser = optparse.OptionParser()
-    parser.add_option("", "--protocol", dest="protocol", default="udp",
-                      help="The gmetric internet protocol, either udp or" +
-                           "multicast, default udp")
-    parser.add_option("", "--host",  dest="host",  default="127.0.0.1",
-                      help="GMond aggregator hostname to send data to")
-    parser.add_option("", "--port",  dest="port",  default="8649",
-                      help="GMond aggregator port to send data to")
-    parser.add_option("", "--name",  dest="name",  default="",
-                      help="The name of the metric")
-    parser.add_option("", "--value", dest="value", default="",
-                      help="The value of the metric")
-    parser.add_option("", "--units", dest="units", default="",
-                      help="The units for the value, e.g. 'kb/sec'")
-    parser.add_option("", "--slope", dest="slope", default="both",
-                      help="Sign of the derivative of the value over time," +
-                           "one of zero, positive, negative, both (default)")
-    parser.add_option("", "--type",  dest="type",  default="",
-                      help="The value data type, one of string, int8, uint8," +
-                           "int16, uint16, int32, uint32, float, double")
-    parser.add_option("", "--tmax",  dest="tmax",  default="60",
-                      help="Maximum time in seconds between gmetric calls," +
-                           "default 60")
-    parser.add_option("", "--dmax",  dest="dmax",  default="0",
-                      help="Lifetime in seconds of this metric, default=0," +
-                           "meaning unlimited")
-    parser.add_option("", "--group",  dest="group",  default="",
-                      help="Group metric belongs to. If not specified Ganglia" +
-                           "will show it as no_group")
+    parser.add_option(
+        "",
+        "--protocol",
+        dest="protocol",
+        default="udp",
+        help="The gmetric internet protocol, either udp or multicast, default udp",
+    )
+    parser.add_option(
+        "",
+        "--host",
+        dest="host",
+        default="127.0.0.1",
+        help="GMond aggregator hostname to send data to",
+    )
+    parser.add_option(
+        "",
+        "--port",
+        dest="port",
+        default="8649",
+        help="GMond aggregator port to send data to",
+    )
+    parser.add_option(
+        "", "--name", dest="name", default="", help="The name of the metric"
+    )
+    parser.add_option(
+        "", "--value", dest="value", default="", help="The value of the metric"
+    )
+    parser.add_option(
+        "",
+        "--units",
+        dest="units",
+        default="",
+        help="The units for the value, e.g. 'kb/sec'",
+    )
+    parser.add_option(
+        "",
+        "--slope",
+        dest="slope",
+        default="both",
+        help="Sign of the derivative of the value over time, one of zero, positive, negative, both (default)",
+    )
+    parser.add_option(
+        "",
+        "--type",
+        dest="type",
+        default="",
+        help="The value data type, one of string, int8, uint8, int16, uint16, int32, uint32, float, double",
+    )
+    parser.add_option(
+        "",
+        "--tmax",
+        dest="tmax",
+        default="60",
+        help="Maximum time in seconds between gmetric calls, default 60",
+    )
+    parser.add_option(
+        "",
+        "--dmax",
+        dest="dmax",
+        default="0",
+        help="Lifetime in seconds of this metric, default=0, meaning unlimited",
+    )
+    parser.add_option(
+        "",
+        "--group",
+        dest="group",
+        default="",
+        help="Group metric belongs to. If not specified Ganglia will show it as no_group",
+    )
     (options, args) = parser.parse_args()
 
     g = Gmetric(options.host, options.port, options.protocol)
-    g.send(options.name, options.value, options.type, options.units,
-           options.slope, options.tmax, options.dmax, options.group)
+    g.send(
+        options.name,
+        options.value,
+        options.type,
+        options.units,
+        options.slope,
+        options.tmax,
+        options.dmax,
+        options.group,
+    )
```

### Comparing `diamond-next-4.0.515/src/diamond/server.py` & `diamond-next-5.0.0/src/diamond/server.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,220 +1,225 @@
 # coding=utf-8
 
 import logging
 import multiprocessing
-import os
 import signal
 import sys
 import time
 
+from diamond.handler.Handler import Handler
+from diamond.utils.classes import (
+    initialize_collector,
+    load_collectors,
+    load_dynamic_class,
+    load_handlers,
+    load_include_path,
+)
+from diamond.utils.config import load_config, str_to_bool
+from diamond.utils.scheduler import collector_process, handler_process
+from diamond.utils.signals import SIGHUPException, signal_to_exception
+
 try:
     from setproctitle import getproctitle, setproctitle
 except ImportError:
+    getproctitle = None
     setproctitle = None
 
-# Path Fix
-sys.path.append(
-    os.path.abspath(
-        os.path.join(
-            os.path.dirname(__file__), "../")))
-
-from diamond.utils.classes import initialize_collector
-from diamond.utils.classes import load_collectors
-from diamond.utils.classes import load_dynamic_class
-from diamond.utils.classes import load_handlers
-from diamond.utils.classes import load_include_path
-
-from diamond.utils.config import load_config
-
-from diamond.utils.scheduler import collector_process
-from diamond.utils.scheduler import handler_process
-
-from diamond.handler.Handler import Handler
-
-from diamond.utils.signals import signal_to_exception
-from diamond.utils.signals import SIGHUPException
-
 
 class Server(object):
     """
     Server class loads and starts Handlers and Collectors
     """
 
     def __init__(self, configfile):
         # Initialize Logging
-        self.log = logging.getLogger('diamond')
+        self.log = logging.getLogger("diamond")
+
         # Initialize Members
         self.configfile = configfile
         self.config = None
         self.handlers = []
         self.handler_queue = []
         self.modules = {}
         self.metric_queue = None
 
-        # We do this weird process title swap around to get the sync manager
-        # title correct for ps
+        # We do this weird process title swap around to get the sync manager title correct for ps
         if setproctitle:
             oldproctitle = getproctitle()
-            setproctitle('%s - SyncManager' % getproctitle())
+            setproctitle("%s - SyncManager" % getproctitle())
+
         self.manager = multiprocessing.Manager()
+
         if setproctitle:
             setproctitle(oldproctitle)
 
     def run(self):
         """
         Load handler and collector classes and then start collectors
         """
 
         #######################################################################
         # Config
         #######################################################################
         self.config = load_config(self.configfile)
 
-        collectors = load_collectors(self.config['server']['collectors_path'])
-        metric_queue_size = int(self.config['server'].get('metric_queue_size',
-                                                          16384))
+        collectors = load_collectors(self.config["server"]["collectors_path"])
+        metric_queue_size = int(self.config["server"].get("metric_queue_size", 16384))
         self.metric_queue = self.manager.Queue(maxsize=metric_queue_size)
-        self.log.debug('metric_queue_size: %d', metric_queue_size)
+        self.log.debug("metric_queue_size: %d", metric_queue_size)
 
         #######################################################################
         # Handlers
         #
         # TODO: Eventually move each handler to it's own process space?
         #######################################################################
 
-        if 'handlers_path' in self.config['server']:
-            handlers_path = self.config['server']['handlers_path']
+        if "handlers_path" in self.config["server"]:
+            handlers_path = self.config["server"]["handlers_path"]
 
             # Make an list if not one
-            if isinstance(handlers_path, basestring):
-                handlers_path = handlers_path.split(',')
+            if isinstance(handlers_path, str):
+                handlers_path = handlers_path.split(",")
                 handlers_path = map(str.strip, handlers_path)
-                self.config['server']['handlers_path'] = handlers_path
+                self.config["server"]["handlers_path"] = handlers_path
 
             load_include_path(handlers_path)
 
-        if 'handlers' not in self.config['server']:
-            self.log.critical('handlers missing from server section in config')
+        if "handlers" not in self.config["server"]:
+            self.log.critical("handlers missing from server section in config")
             sys.exit(1)
 
-        handlers = self.config['server'].get('handlers')
-        if isinstance(handlers, basestring):
+        handlers = self.config["server"].get("handlers")
+
+        if isinstance(handlers, str):
             handlers = [handlers]
 
         # Prevent the Queue Handler from being a normal handler
-        if 'diamond.handler.queue.QueueHandler' in handlers:
-            handlers.remove('diamond.handler.queue.QueueHandler')
+        if "diamond.handler.queue.QueueHandler" in handlers:
+            handlers.remove("diamond.handler.queue.QueueHandler")
 
         self.handlers = load_handlers(self.config, handlers)
 
-        QueueHandler = load_dynamic_class(
-            'diamond.handler.queue.QueueHandler',
-            Handler
+        queue_handler = load_dynamic_class(
+            "diamond.handler.queue.QueueHandler", Handler
         )
 
-        self.handler_queue = QueueHandler(
-            config=self.config, queue=self.metric_queue, log=self.log)
+        self.handler_queue = queue_handler(
+            config=self.config, queue=self.metric_queue, log=self.log
+        )
 
-        process = multiprocessing.Process(
+        handlers_process = multiprocessing.Process(
             name="Handlers",
             target=handler_process,
             args=(self.handlers, self.metric_queue, self.log),
         )
-
-        process.daemon = True
-        process.start()
+        handlers_process.daemon = True
+        handlers_process.start()
 
         #######################################################################
         # Signals
         #######################################################################
 
-        if hasattr(signal, 'SIGHUP'):
+        if hasattr(signal, "SIGHUP"):
             signal.signal(signal.SIGHUP, signal_to_exception)
 
         #######################################################################
 
         while True:
             try:
                 active_children = multiprocessing.active_children()
                 running_processes = []
+
                 for process in active_children:
                     running_processes.append(process.name)
+
                 running_processes = set(running_processes)
 
                 ##############################################################
                 # Collectors
                 ##############################################################
 
                 running_collectors = []
-                for collector, config in self.config['collectors'].iteritems():
-                    if config.get('enabled', False) is not True:
+
+                for collector, config in iter(self.config["collectors"].items()):
+                    if config.get("enabled", False) is not True:
                         continue
+
                     running_collectors.append(collector)
+
                 running_collectors = set(running_collectors)
 
                 # Collectors that are running but shouldn't be
                 for process_name in running_processes - running_collectors:
-                    if 'Collector' not in process_name:
+                    if "Collector" not in process_name:
                         continue
+
                     for process in active_children:
                         if process.name == process_name:
                             process.terminate()
 
                 collector_classes = dict(
-                    (cls.__name__.split('.')[-1], cls)
-                    for cls in collectors.values()
+                    (cls.__name__.split(".")[-1], cls) for cls in collectors.values()
                 )
 
-                load_delay = self.config['server'].get('collectors_load_delay',
-                                                       1.0)
+                load_delay = self.config["server"].get("collectors_load_delay", 1.0)
+
                 for process_name in running_collectors - running_processes:
                     # To handle running multiple collectors concurrently, we
                     # split on white space and use the first word as the
                     # collector name to spin
                     collector_name = process_name.split()[0]
 
-                    if 'Collector' not in collector_name:
+                    if "Collector" not in collector_name:
                         continue
 
                     if collector_name not in collector_classes:
-                        self.log.error('Can not find collector %s',
-                                       collector_name)
+                        self.log.error("Can not find collector %s", collector_name)
                         continue
 
                     collector = initialize_collector(
                         collector_classes[collector_name],
                         name=process_name,
                         configfile=self.configfile,
-                        handlers=[self.handler_queue])
+                        handlers=[self.handler_queue],
+                    )
 
                     if collector is None:
-                        self.log.error('Failed to load collector %s',
-                                       process_name)
+                        self.log.error("Failed to load collector %s", process_name)
                         continue
 
                     # Splay the loads
                     time.sleep(float(load_delay))
 
                     process = multiprocessing.Process(
                         name=process_name,
                         target=collector_process,
-                        args=(collector, self.metric_queue, self.log)
+                        args=(collector, self.metric_queue, self.log),
                     )
                     process.daemon = True
                     process.start()
 
+                if not handlers_process.is_alive():
+                    self.log.error("Handlers process exited")
+
+                    if str_to_bool(
+                        self.config["server"].get(
+                            "abort_on_handlers_process_exit", "False"
+                        )
+                    ):
+                        raise Exception("Handlers process exited")
+
                 ##############################################################
 
                 time.sleep(1)
 
             except SIGHUPException:
                 # ignore further SIGHUPs for now
                 original_sighup_handler = signal.getsignal(signal.SIGHUP)
                 signal.signal(signal.SIGHUP, signal.SIG_IGN)
 
-                self.log.info('Reloading state due to HUP')
+                self.log.info("Reloading state due to HUP")
                 self.config = load_config(self.configfile)
-                collectors = load_collectors(
-                    self.config['server']['collectors_path'])
+                collectors = load_collectors(self.config["server"]["collectors_path"])
+
                 # restore SIGHUP handler
                 signal.signal(signal.SIGHUP, original_sighup_handler)
```

### Comparing `diamond-next-4.0.515/src/diamond/metric.py` & `diamond-next-5.0.0/src/diamond/metric.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,86 +1,103 @@
 # coding=utf-8
 
-import time
-import re
 import logging
-from error import DiamondException
+import re
+import time
+
+from diamond.error import DiamondException
 
 
 class Metric(object):
     # This saves a significant amount of memory per object. This only matters
     # due to the queue system that moves objects between processes and can end
     # up storing a large number of objects in the queue waiting for the
     # handlers to flush.
     __slots__ = [
-        'path', 'value', 'raw_value', 'timestamp', 'precision',
-        'host', 'metric_type', 'ttl'
-        ]
-
-    def __init__(self, path, value, raw_value=None, timestamp=None, precision=0,
-                 host=None, metric_type='COUNTER', ttl=None):
+        "path",
+        "value",
+        "raw_value",
+        "timestamp",
+        "precision",
+        "host",
+        "metric_type",
+        "ttl",
+    ]
+
+    def __init__(
+        self,
+        path,
+        value,
+        raw_value=None,
+        timestamp=None,
+        precision=0,
+        host=None,
+        metric_type="COUNTER",
+        ttl=None,
+    ):
         """
         Create new instance of the Metric class
 
         Takes:
             path=string: string the specifies the path of the metric
             value=[float|int]: the value to be submitted
             timestamp=[float|int]: the timestamp, in seconds since the epoch
             (as from time.time()) precision=int: the precision to apply.
             Generally the default (2) should work fine.
         """
 
         # Validate the path, value and metric_type submitted
-        if (None in [path, value] or metric_type not in ('COUNTER', 'GAUGE')):
-            raise DiamondException(("Invalid parameter when creating new "
-                                    "Metric with path: %r value: %r "
-                                    "metric_type: %r")
-                                   % (path, value, metric_type))
+        if None in [path, value] or metric_type not in ("COUNTER", "GAUGE"):
+            raise DiamondException(
+                "Invalid parameter when creating new Metric with path: %r value: %r metric_type: %r"
+                % (path, value, metric_type)
+            )
 
         # If no timestamp was passed in, set it to the current time
         if timestamp is None:
             timestamp = int(time.time())
         else:
             # If the timestamp isn't an int, then make it one
             if not isinstance(timestamp, int):
                 try:
                     timestamp = int(timestamp)
                 except ValueError as e:
-                    raise DiamondException(("Invalid timestamp when "
-                                            "creating new Metric %r: %s")
-                                           % (path, e))
+                    raise DiamondException(
+                        "Invalid timestamp when creating new Metric %r: %s" % (path, e)
+                    )
 
         # The value needs to be a float or an int.  If it is, great.  If not,
         # try to cast it to one of those.
         if not isinstance(value, (int, float)):
             try:
                 if precision == 0:
                     value = round(float(value))
                 else:
                     value = float(value)
             except ValueError as e:
-                raise DiamondException(("Invalid value when creating new "
-                                        "Metric %r: %s") % (path, e))
+                raise DiamondException(
+                    "Invalid value when creating new Metric %r: %s" % (path, e)
+                )
 
         self.path = path
         self.value = value
         self.raw_value = raw_value
         self.timestamp = timestamp
         self.precision = precision
         self.host = host
         self.metric_type = metric_type
         self.ttl = ttl
 
     def __repr__(self):
         """
         Return the Metric as a string
         """
-        if not isinstance(self.precision, (int, long)):
-            log = logging.getLogger('diamond')
-            log.warn('Metric %s does not have a valid precision', self.path)
+        if not isinstance(self.precision, int):
+            log = logging.getLogger("diamond")
+            log.warning("Metric %s does not have a valid precision", self.path)
             self.precision = 0
 
         # Set the format string
         fstring = "%%s %%0.%if %%i\n" % self.precision
 
         # Return formated string
         return fstring % (self.path, self.value, self.timestamp)
@@ -97,68 +114,71 @@
             setattr(self, slot, value)
 
     @classmethod
     def parse(cls, string):
         """
         Parse a string and create a metric
         """
-        match = re.match(r'^(?P<name>[A-Za-z0-9\.\-_]+)\s+' +
-                         '(?P<value>[0-9\.]+)\s+' +
-                         '(?P<timestamp>[0-9\.]+)(\n?)$',
-                         string)
+        match = re.match(
+            r"^(?P<name>[A-Za-z0-9\.\-_]+)\s+(?P<value>[0-9\.]+)\s+(?P<timestamp>[0-9\.]+)(\n?)$",
+            string,
+        )
+
         try:
             groups = match.groupdict()
+
             # TODO: get precision from value string
-            return Metric(groups['name'],
-                          groups['value'],
-                          float(groups['timestamp']))
+            return Metric(groups["name"], groups["value"], float(groups["timestamp"]))
         except:
             raise DiamondException(
-                "Metric could not be parsed from string: %s." % string)
+                "Metric could not be parsed from string: %s." % string
+            )
 
     def getPathPrefix(self):
         """
-            Returns the path prefix path
-            servers.host.cpu.total.idle
-            return "servers"
+        Returns the path prefix path
+        servers.host.cpu.total.idle
+        return "servers"
         """
-        # If we don't have a host name, assume it's just the first part of the
-        # metric path
+
+        # If we don't have a host name, assume it's just the first part of the metric path
         if self.host is None:
-            return self.path.split('.')[0]
+            return self.path.split(".")[0]
 
         offset = self.path.index(self.host) - 1
+
         return self.path[0:offset]
 
     def getCollectorPath(self):
         """
-            Returns collector path
-            servers.host.cpu.total.idle
-            return "cpu"
+        Returns collector path
+        servers.host.cpu.total.idle
+        return "cpu"
         """
-        # If we don't have a host name, assume it's just the third part of the
-        # metric path
+
+        # If we don't have a host name, assume it's just the third part of the metric path
         if self.host is None:
-            return self.path.split('.')[2]
+            return self.path.split(".")[2]
 
         offset = self.path.index(self.host)
         offset += len(self.host) + 1
-        endoffset = self.path.index('.', offset)
+        endoffset = self.path.index(".", offset)
+
         return self.path[offset:endoffset]
 
     def getMetricPath(self):
         """
-            Returns the metric path after the collector name
-            servers.host.cpu.total.idle
-            return "total.idle"
+        Returns the metric path after the collector name
+        servers.host.cpu.total.idle
+        return "total.idle"
         """
-        # If we don't have a host name, assume it's just the fourth+ part of the
-        # metric path
+
+        # If we don't have a host name, assume it's just the fourth+ part of the metric path
         if self.host is None:
-            path = self.path.split('.')[3:]
-            return '.'.join(path)
+            path = self.path.split(".")[3:]
 
-        prefix = '.'.join([self.getPathPrefix(), self.host,
-                           self.getCollectorPath()])
+            return ".".join(path)
 
+        prefix = ".".join([self.getPathPrefix(), self.host, self.getCollectorPath()])
         offset = len(prefix) + 1
+
         return self.path[offset:]
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/datadog.py` & `diamond-next-5.0.0/src/diamond/handler/datadog.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,75 +6,80 @@
 
 #### Dependencies
 
   * [dogapi](https://github.com/DataDog/datadogpy)
 
 """
 
-from Handler import Handler
 import logging
 from collections import deque
 
+from diamond.handler.Handler import Handler
+
 try:
     import dogapi
 except ImportError:
     dogapi = None
 
 
 class DatadogHandler(Handler):
-
     def __init__(self, config=None):
         """
         New instance of DatadogHandler class
         """
 
         Handler.__init__(self, config)
         logging.debug("Initialized Datadog handler.")
 
         if dogapi is None:
             logging.error("Failed to load dogapi module.")
             return
 
         self.api = dogapi.dog_http_api
-        self.api.api_key = self.config.get('api_key', '')
-        self.queue_size = self.config.get('queue_size', 1)
+        self.api.api_key = self.config.get("api_key", "")
+        self.queue_size = self.config.get("queue_size", 1)
         self.queue = deque([])
 
     def get_default_config_help(self):
         """
         Help text
         """
         config = super(DatadogHandler, self).get_default_config_help()
 
-        config.update({
-            'api_key': 'Datadog API key',
-            'queue_size': 'Number of metrics to queue before send',
-        })
+        config.update(
+            {
+                "api_key": "Datadog API key",
+                "queue_size": "Number of metrics to queue before send",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return default config for the handler
         """
         config = super(DatadogHandler, self).get_default_config()
 
-        config.update({
-            'api_key': '',
-            'queue_size': '',
-        })
+        config.update(
+            {
+                "api_key": "",
+                "queue_size": "",
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Process metric by sending it to datadog api
         """
 
         self.queue.append(metric)
+
         if len(self.queue) >= self.queue_size:
             self._send()
 
     def flush(self):
         """
         Flush metrics
         """
@@ -85,22 +90,19 @@
         """
         Take metrics from queue and send it to Datadog API
         """
 
         while len(self.queue) > 0:
             metric = self.queue.popleft()
 
-            path = '%s.%s.%s' % (
+            path = "%s.%s.%s" % (
                 metric.getPathPrefix(),
                 metric.getCollectorPath(),
-                metric.getMetricPath()
+                metric.getMetricPath(),
             )
 
             topic, value, timestamp = str(metric).split()
             logging.debug(
-                "Sending.. topic[%s], value[%s], timestamp[%s]",
-                path,
-                value,
-                timestamp
+                "Sending.. topic[%s], value[%s], timestamp[%s]", path, value, timestamp
             )
 
             self.api.metric(path, (timestamp, value), host=metric.host)
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/hostedgraphite.py` & `diamond-next-5.0.0/src/diamond/handler/hostedgraphite.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,81 +12,84 @@
 
  * handlers = diamond.handler.hostedgraphite.HostedGraphiteHandler,
 
  * apikey = API_KEY
 
 """
 
-from Handler import Handler
-from graphite import GraphiteHandler
+from diamond.handler.Handler import Handler
+from diamond.handler.graphite import GraphiteHandler
 
 
 class HostedGraphiteHandler(Handler):
-
     def __init__(self, config=None):
         """
         Create a new instance of the HostedGraphiteHandler class
         """
         # Initialize Handler
         Handler.__init__(self, config)
 
-        self.key = self.config['apikey'].lower().strip()
+        self.key = self.config["apikey"].lower().strip()
 
         self.graphite = GraphiteHandler(self.config)
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(HostedGraphiteHandler, self).get_default_config_help()
 
-        config.update({
-            'apikey': 'Api key to use',
-            'host': 'Hostname',
-            'port': 'Port',
-            'proto': 'udp or tcp',
-            'timeout': '',
-            'batch': 'How many to store before sending to the graphite server',
-            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
-            'trim_backlog_multiplier': 'Trim down how many batches',
-        })
+        config.update(
+            {
+                "apikey": "Api key to use",
+                "host": "Hostname",
+                "port": "Port",
+                "proto": "udp or tcp",
+                "timeout": "",
+                "batch": "How many to store before sending to the graphite server",
+                "max_backlog_multiplier": "how many batches to store before trimming",  # NOQA
+                "trim_backlog_multiplier": "Trim down how many batches",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(HostedGraphiteHandler, self).get_default_config()
 
-        config.update({
-            'apikey': '',
-            'host': 'carbon.hostedgraphite.com',
-            'port': 2003,
-            'proto': 'tcp',
-            'timeout': 15,
-            'batch': 1,
-            'max_backlog_multiplier': 5,
-            'trim_backlog_multiplier': 4,
-        })
+        config.update(
+            {
+                "apikey": "",
+                "host": "carbon.hostedgraphite.com",
+                "port": 2003,
+                "proto": "tcp",
+                "timeout": 15,
+                "batch": 1,
+                "max_backlog_multiplier": 5,
+                "trim_backlog_multiplier": 4,
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Process a metric by sending it to graphite
         """
-        metric = self.key + '.' + str(metric)
+        metric = self.key + "." + str(metric)
         self.graphite.process(metric)
 
     def _process(self, metric):
         """
         Process a metric by sending it to graphite
         """
-        metric = self.key + '.' + str(metric)
+        metric = self.key + "." + str(metric)
         self.graphite._process(metric)
 
     def _flush(self):
         self.graphite._flush()
 
     def flush(self):
         self.graphite.flush()
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/libratohandler.py` & `diamond-next-5.0.0/src/diamond/handler/libratohandler.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,126 +8,135 @@
 #### Dependencies
 
  * [librato-metrics](https://github.com/librato/python-librato)
 
 
 """
 
-from Handler import Handler
 import logging
-import time
 import re
+import time
+
+from diamond.handler.Handler import Handler
 
 try:
     import librato
 except ImportError:
     librato = None
 
 
 class LibratoHandler(Handler):
-
     def __init__(self, config=None):
         """
         Create a new instance of the LibratoHandler class
         """
+
         # Initialize Handler
         Handler.__init__(self, config)
         logging.debug("Initialized Librato handler.")
 
         if librato is None:
             logging.error("Failed to load librato module")
             return
 
         # Initialize Options
-        api = librato.connect(self.config['user'],
-                              self.config['apikey'])
+        api = librato.connect(self.config["user"], self.config["apikey"])
         self.queue = api.new_queue()
-        self.queue_max_size = int(self.config['queue_max_size'])
-        self.queue_max_interval = int(self.config['queue_max_interval'])
+        self.queue_max_size = int(self.config["queue_max_size"])
+        self.queue_max_interval = int(self.config["queue_max_interval"])
         self.queue_max_timestamp = int(time.time() + self.queue_max_interval)
         self.current_n_measurements = 0
 
         # If a user leaves off the ending comma, cast to a array for them
-        include_filters = self.config['include_filters']
-        if isinstance(include_filters, basestring):
+        include_filters = self.config["include_filters"]
+
+        if isinstance(include_filters, str):
             include_filters = [include_filters]
 
-        self.include_reg = re.compile(r'(?:%s)' % '|'.join(include_filters))
+        self.include_reg = re.compile(r"(?:%s)" % "|".join(include_filters))
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(LibratoHandler, self).get_default_config_help()
 
-        config.update({
-            'user': 'Librato username',
-            'apikey': 'Librato API key',
-            'apply_metric_prefix': 'Allow diamond to apply metric prefix',
-            'queue_max_size': 'Max measurements to queue before submitting',
-            'queue_max_interval':
-                'Max seconds to wait before submitting. For best behavior, '
-                'be sure your highest collector poll interval is lower than '
-                'or equal to the queue_max_interval setting.',
-            'include_filters':
-                'A list of regex patterns. Only measurements whose path '
-                'matches a filter will be submitted. Useful for limiting '
-                'usage to *only* desired measurements, e.g. '
+        config.update(
+            {
+                "user": "Librato username",
+                "apikey": "Librato API key",
+                "apply_metric_prefix": "Allow diamond to apply metric prefix",
+                "queue_max_size": "Max measurements to queue before submitting",
+                "queue_max_interval": "Max seconds to wait before submitting. For best behavior, "
+                "be sure your highest collector poll interval is lower than "
+                "or equal to the queue_max_interval setting.",
+                "include_filters": "A list of regex patterns. Only measurements whose path "
+                "matches a filter will be submitted. Useful for limiting "
+                "usage to *only* desired measurements, e.g. "
                 '`"^diskspace\..*\.byte_avail$", "^loadavg\.01"` or '
                 '`"^sockets\.",` (note trailing comma to indicate a list)',
-        })
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(LibratoHandler, self).get_default_config()
 
-        config.update({
-            'user': '',
-            'apikey': '',
-            'apply_metric_prefix': False,
-            'queue_max_size': 300,
-            'queue_max_interval': 60,
-            'include_filters': ['^.*'],
-        })
+        config.update(
+            {
+                "user": "",
+                "apikey": "",
+                "apply_metric_prefix": False,
+                "queue_max_size": 300,
+                "queue_max_interval": 60,
+                "include_filters": ["^.*"],
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Process a metric by sending it to Librato
         """
         path = metric.getCollectorPath()
-        path += '.'
+        path += "."
         path += metric.getMetricPath()
-        if self.config['apply_metric_prefix']:
-            path = metric.getPathPrefix() + '.' + path
+
+        if self.config["apply_metric_prefix"]:
+            path = metric.getPathPrefix() + "." + path
 
         if self.include_reg.match(path):
-            if metric.metric_type == 'GAUGE':
-                m_type = 'gauge'
+            if metric.metric_type == "GAUGE":
+                m_type = "gauge"
             else:
-                m_type = 'counter'
-            self.queue.add(path,                # name
-                           float(metric.value),  # value
-                           type=m_type,
-                           source=metric.host,
-                           measure_time=metric.timestamp)
+                m_type = "counter"
+
+            self.queue.add(
+                path,  # name
+                float(metric.value),  # value
+                type=m_type,
+                source=metric.host,
+                measure_time=metric.timestamp,
+            )
             self.current_n_measurements += 1
         else:
-            self.log.debug("LibratoHandler: Skip %s, no include_filters match",
-                           path)
+            self.log.debug("LibratoHandler: Skip %s, no include_filters match", path)
 
-        if (self.current_n_measurements >= self.queue_max_size or
-                time.time() >= self.queue_max_timestamp):
-            self.log.debug("LibratoHandler: Sending batch size: %d",
-                           self.current_n_measurements)
+        if (
+            self.current_n_measurements >= self.queue_max_size
+            or time.time() >= self.queue_max_timestamp
+        ):
+            self.log.debug(
+                "LibratoHandler: Sending batch size: %d", self.current_n_measurements
+            )
             self._send()
 
     def flush(self):
         """Flush metrics in queue"""
         self._send()
 
     def _send(self):
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/null.py` & `diamond-next-5.0.0/src/diamond/handler/null.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,41 +1,39 @@
 # coding=utf-8
 
 """
 Output the collected values to the debug log channel.
 """
 
-from Handler import Handler
+from diamond.handler.Handler import Handler
 
 
 class NullHandler(Handler):
     """
     Implements the abstract Handler class, doing nothing except log
     """
 
     def process(self, metric):
         """
         Process a metric by doing nothing
         """
-        self.log.debug("Metric: %s", str(metric).rstrip().replace(' ', '\t'))
+        self.log.debug("Metric: %s", str(metric).rstrip().replace(" ", "\t"))
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(NullHandler, self).get_default_config_help()
 
-        config.update({
-        })
+        config.update({})
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(NullHandler, self).get_default_config()
 
-        config.update({
-        })
+        config.update({})
 
         return config
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/mqtt.py` & `diamond-next-5.0.0/src/diamond/handler/mqtt.py`

 * *Files 5% similar despite different names*

```diff
@@ -55,145 +55,150 @@
 * This handler sets a last will and testament, so that the broker
   publishes its death at a topic called clients/diamond/<hostname>
 * Support for reconnecting to a broker is implemented and ought to
   work.
 
 """
 
-from Handler import Handler
-from diamond.collector import get_hostname
 import os
+
+from diamond.collector import get_hostname
+from diamond.handler.Handler import Handler
+
 HAVE_SSL = True
+
 try:
     import ssl
 except ImportError:
     HAVE_SSL = False
 
 try:
     import mosquitto
 except ImportError:
     mosquitto = None
 
-__author__ = 'Jan-Piet Mens'
-__email__ = 'jpmens@gmail.com'
+__author__ = "Jan-Piet Mens"
+__email__ = "jpmens@gmail.com"
 
 
 class MQTTHandler(Handler):
-    """
-    """
-
     def __init__(self, config=None):
         """
         Create a new instance of the MQTTHandler class
         """
         # Initialize Handler
         Handler.__init__(self, config)
 
         # Initialize Data
         self.mqttc = None
         self.hostname = get_hostname(self.config)
         self.client_id = "%s_%s" % (self.hostname, os.getpid())
 
         # Initialize Options
-        self.host = self.config.get('host', 'localhost')
+        self.host = self.config.get("host", "localhost")
         self.port = 0
-        self.qos = int(self.config.get('qos', 0))
-        self.prefix = self.config.get('prefix', "")
-        self.tls = self.config.get('tls', False)
+        self.qos = int(self.config.get("qos", 0))
+        self.prefix = self.config.get("prefix", "")
+        self.tls = self.config.get("tls", False)
         self.timestamp = 0
+
         try:
-            self.timestamp = self.config['timestamp']
+            self.timestamp = self.config["timestamp"]
             if not self.timestamp:
                 self.timestamp = 1
             else:
                 self.timestamp = 0
         except:
             self.timestamp = 1
 
         if not mosquitto:
-            self.log.error('mosquitto import failed. Handler disabled')
+            self.log.error("mosquitto import failed. Handler disabled")
             self.enabled = False
+
             return
 
         # Initialize
         self.mqttc = mosquitto.Mosquitto(self.client_id, clean_session=True)
 
         if not self.tls:
-            self.port = int(self.config.get('port', 1883))
+            self.port = int(self.config.get("port", 1883))
         else:
             # Set up TLS if requested
 
-            self.port = int(self.config.get('port', 8883))
+            self.port = int(self.config.get("port", 8883))
 
-            self.cafile = self.config.get('cafile', None)
-            self.certfile = self.config.get('certfile', None)
-            self.keyfile = self.config.get('keyfile', None)
+            self.cafile = self.config.get("cafile", None)
+            self.certfile = self.config.get("certfile", None)
+            self.keyfile = self.config.get("keyfile", None)
 
             if None in [self.cafile, self.certfile, self.keyfile]:
                 self.log.error("MQTTHandler: TLS configuration missing.")
                 return
 
             try:
                 self.mqttc.tls_set(
                     self.cafile,
                     certfile=self.certfile,
                     keyfile=self.keyfile,
                     cert_reqs=ssl.CERT_REQUIRED,
                     tls_version=3,
-                    ciphers=None)
+                    ciphers=None,
+                )
             except:
-                self.log.error("MQTTHandler: Cannot set up TLS " +
-                               "configuration. Files missing?")
-
-        self.mqttc.will_set("clients/diamond/%s" % (self.hostname),
-                            payload="Adios!", qos=0, retain=False)
+                self.log.error(
+                    "MQTTHandler: Cannot set up TLS configuration. Files missing?"
+                )
+
+        self.mqttc.will_set(
+            "clients/diamond/%s" % self.hostname, payload="Adios!", qos=0, retain=False
+        )
         self.mqttc.connect(self.host, self.port, 60)
 
         self.mqttc.on_disconnect = self._disconnect
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(MQTTHandler, self).get_default_config_help()
 
-        config.update({
-        })
+        config.update({})
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(MQTTHandler, self).get_default_config()
 
-        config.update({
-        })
+        config.update({})
 
         return config
 
     def process(self, metric):
         """
         Process a metric by converting metric name to MQTT topic name;
         the payload is metric and timestamp.
         """
 
         if not mosquitto:
             return
 
         line = str(metric)
         topic, value, timestamp = line.split()
+
         if len(self.prefix):
             topic = "%s/%s" % (self.prefix, topic)
-        topic = topic.replace('.', '/')
-        topic = topic.replace('#', '&')     # Topic must not contain wildcards
+
+        topic = topic.replace(".", "/")
+        topic = topic.replace("#", "&")  # Topic must not contain wildcards
 
         if self.timestamp == 0:
-            self.mqttc.publish(topic, "%s" % (value), self.qos)
+            self.mqttc.publish(topic, "%s" % value, self.qos)
         else:
             self.mqttc.publish(topic, "%s %s" % (value, timestamp), self.qos)
 
     def _disconnect(self, mosq, obj, rc):
 
         self.log.debug("MQTTHandler: reconnecting to broker...")
         mosq.reconnect()
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/rabbitmq_pubsub.py` & `diamond-next-5.0.0/src/diamond/handler/rabbitmq_pubsub.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,209 +1,224 @@
 # coding=utf-8
 
 """
 Output the collected values to RabitMQ pub/sub channel
 """
 
-from Handler import Handler
 import time
 
+from diamond.handler.Handler import Handler
+
 try:
     import pika
 except ImportError:
     pika = None
 
 
-class rmqHandler (Handler):
+class rmqHandler(Handler):
     """
-      Implements the abstract Handler class
-      Sending data to a RabbitMQ pub/sub channel
+    Implements the abstract Handler class
+    Sending data to a RabbitMQ pub/sub channel
     """
 
     def __init__(self, config=None):
         """
-          Create a new instance of rmqHandler class
+        Create a new instance of rmqHandler class
         """
 
         # Initialize Handler
         Handler.__init__(self, config)
 
         if pika is None:
-            self.log.error('pika import failed. Handler disabled')
+            self.log.error("pika import failed. Handler disabled")
             self.enabled = False
             return
 
         # Initialize Data
         self.connections = {}
         self.channels = {}
         self.reconnect_interval = 1
 
         # Initialize Options
-        tmp_rmq_server = self.config['rmq_server']
+        tmp_rmq_server = self.config["rmq_server"]
+
         if type(tmp_rmq_server) is list:
             self.rmq_server = tmp_rmq_server
         else:
             self.rmq_server = [tmp_rmq_server]
 
         self.rmq_port = 5672
-        self.rmq_exchange = self.config['rmq_exchange']
+        self.rmq_exchange = self.config["rmq_exchange"]
         self.rmq_user = None
         self.rmq_password = None
-        self.rmq_vhost = '/'
-        self.rmq_exchange_type = 'fanout'
+        self.rmq_vhost = "/"
+        self.rmq_exchange_type = "fanout"
         self.rmq_durable = True
         self.rmq_heartbeat_interval = 300
 
         self.get_config()
+
         # Create rabbitMQ pub socket and bind
         try:
             self._bind_all()
         except pika.exceptions.AMQPConnectionError:
-            self.log.error('Failed to bind to rabbitMQ pub socket')
+            self.log.error("Failed to bind to rabbitMQ pub socket")
 
     def get_config(self):
-        """ Get and set config options from config file """
-        if 'rmq_port' in self.config:
-            self.rmq_port = int(self.config['rmq_port'])
+        """Get and set config options from config file"""
+        if "rmq_port" in self.config:
+            self.rmq_port = int(self.config["rmq_port"])
 
-        if 'rmq_user' in self.config:
-            self.rmq_user = self.config['rmq_user']
+        if "rmq_user" in self.config:
+            self.rmq_user = self.config["rmq_user"]
 
-        if 'rmq_password' in self.config:
-            self.rmq_password = self.config['rmq_password']
+        if "rmq_password" in self.config:
+            self.rmq_password = self.config["rmq_password"]
 
-        if 'rmq_vhost' in self.config:
-            self.rmq_vhost = self.config['rmq_vhost']
+        if "rmq_vhost" in self.config:
+            self.rmq_vhost = self.config["rmq_vhost"]
 
-        if 'rmq_exchange_type' in self.config:
-            self.rmq_exchange_type = self.config['rmq_exchange_type']
+        if "rmq_exchange_type" in self.config:
+            self.rmq_exchange_type = self.config["rmq_exchange_type"]
 
-        if 'rmq_durable' in self.config:
-            self.rmq_durable = bool(self.config['rmq_durable'])
+        if "rmq_durable" in self.config:
+            self.rmq_durable = bool(self.config["rmq_durable"])
 
-        if 'rmq_heartbeat_interval' in self.config:
-            self.rmq_heartbeat_interval = int(
-                self.config['rmq_heartbeat_interval'])
+        if "rmq_heartbeat_interval" in self.config:
+            self.rmq_heartbeat_interval = int(self.config["rmq_heartbeat_interval"])
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(rmqHandler, self).get_default_config_help()
 
-        config.update({
-            'server': '',
-            'rmq_exchange': '',
-        })
+        config.update(
+            {
+                "server": "",
+                "rmq_exchange": "",
+            }
+        )
+
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(rmqHandler, self).get_default_config()
 
-        config.update({
-            'server': '127.0.0.1',
-            'rmq_exchange': 'diamond',
-        })
+        config.update(
+            {
+                "server": "127.0.0.1",
+                "rmq_exchange": "diamond",
+            }
+        )
 
         return config
 
     def _bind_all(self):
         """
         Bind all RMQ servers defined in config
         """
         for rmq_server in self.rmq_server:
             self._bind(rmq_server)
 
     def _bind(self, rmq_server):
         """
-           Create PUB socket and bind
+        Create PUB socket and bind
         """
-        if ((rmq_server in self.connections.keys() and
-             self.connections[rmq_server] is not None and
-             self.connections[rmq_server].is_open)):
-            # It seems we already have this server, so let's try _unbind just
-            # to be safe.
+        if (
+            rmq_server in self.connections.keys()
+            and self.connections[rmq_server] is not None
+            and self.connections[rmq_server].is_open
+        ):
+            # It seems we already have this server, so let's try _unbind just to be safe.
             self._unbind(rmq_server)
 
         credentials = None
+
         if self.rmq_user and self.rmq_password:
-            credentials = pika.PlainCredentials(
-                self.rmq_user,
-                self.rmq_password)
+            credentials = pika.PlainCredentials(self.rmq_user, self.rmq_password)
 
         parameters = pika.ConnectionParameters(
             host=rmq_server,
             port=self.rmq_port,
             virtual_host=self.rmq_vhost,
             credentials=credentials,
             heartbeat_interval=self.rmq_heartbeat_interval,
             retry_delay=5,
-            connection_attempts=3)
+            connection_attempts=3,
+        )
 
         self.connections[rmq_server] = None
-        while (self.connections[rmq_server] is None or
-               self.connections[rmq_server].is_open is False):
+
+        while (
+            self.connections[rmq_server] is None
+            or self.connections[rmq_server].is_open is False
+        ):
             try:
-                self.connections[rmq_server] = pika.BlockingConnection(
-                    parameters)
-                self.channels[rmq_server] = self.connections[
-                    rmq_server].channel()
+                self.connections[rmq_server] = pika.BlockingConnection(parameters)
+                self.channels[rmq_server] = self.connections[rmq_server].channel()
                 self.channels[rmq_server].exchange_declare(
                     exchange=self.rmq_exchange,
                     type=self.rmq_exchange_type,
-                    durable=self.rmq_durable)
+                    durable=self.rmq_durable,
+                )
+
                 # Reset reconnect_interval after a successful connection
                 self.reconnect_interval = 1
-            except Exception, exception:
+            except Exception as exception:
                 self.log.debug("Caught exception in _bind: %s", exception)
+
                 if rmq_server in self.connections.keys():
                     self._unbind(rmq_server)
 
                 if self.reconnect_interval >= 16:
                     break
 
                 if self.reconnect_interval < 16:
                     self.reconnect_interval = self.reconnect_interval * 2
 
                 time.sleep(self.reconnect_interval)
 
     def _unbind(self, rmq_server=None):
-        """ Close AMQP connection and unset channel """
+        """Close AMQP connection and unset channel"""
         try:
             self.connections[rmq_server].close()
         except AttributeError:
             pass
 
         self.connections[rmq_server] = None
         self.channels[rmq_server] = None
 
     def __del__(self):
         """
-          Destroy instance of the rmqHandler class
+        Destroy instance of the rmqHandler class
         """
-        if hasattr(self, 'connections'):
+        if hasattr(self, "connections"):
             for rmq_server in self.connections.keys():
                 self._unbind(rmq_server)
 
     def process(self, metric):
         """
-          Process a metric and send it to RMQ pub socket
+        Process a metric and send it to RMQ pub socket
         """
         for rmq_server in self.connections.keys():
             try:
-                if ((self.connections[rmq_server] is None or
-                     self.connections[rmq_server].is_open is False)):
+                if (
+                    self.connections[rmq_server] is None
+                    or self.connections[rmq_server].is_open is False
+                ):
                     self._bind(rmq_server)
 
                 channel = self.channels[rmq_server]
-                channel.basic_publish(exchange=self.rmq_exchange,
-                                      routing_key='', body="%s" % metric)
-            except Exception, exception:
+                channel.basic_publish(
+                    exchange=self.rmq_exchange, routing_key="", body="%s" % metric
+                )
+            except Exception as exception:
                 self.log.error(
-                    "Failed publishing to %s, attempting reconnect",
-                    rmq_server)
+                    "Failed publishing to %s, attempting reconnect", rmq_server
+                )
                 self.log.debug("Caught exception: %s", exception)
                 self._unbind(rmq_server)
                 self._bind(rmq_server)
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/rabbitmq_topic.py` & `diamond-next-5.0.0/src/diamond/handler/rabbitmq_topic.py`

 * *Files 15% similar despite different names*

```diff
@@ -2,154 +2,159 @@
 
 """
 Output the collected values to RabitMQ Topic Exchange
 This allows for 'subscribing' to messages based on the routing key, which is
 the metric path
 """
 
-from Handler import Handler
+from diamond.handler.Handler import Handler
 
 try:
     import pika
 except ImportError:
     pika = None
 
 
-class rmqHandler (Handler):
+class rmqHandler(Handler):
     """
-      Implements the abstract Handler class
-      Sending data to a RabbitMQ topic exchange.
-      The routing key will be the full name of the metric being sent.
+    Implements the abstract Handler class
+    Sending data to a RabbitMQ topic exchange.
+    The routing key will be the full name of the metric being sent.
 
-      Based on the rmqHandler and zmpqHandler code.
+    Based on the rmqHandler and zmpqHandler code.
     """
 
     def __init__(self, config=None):
         """
-          Create a new instance of rmqHandler class
+        Create a new instance of rmqHandler class
         """
 
         # Initialize Handler
         Handler.__init__(self, config)
 
         # Initialize Data
         self.connection = None
         self.channel = None
 
         # Initialize Options
-        self.server = self.config.get('server', '127.0.0.1')
-        self.port = int(self.config.get('port', 5672))
-        self.topic_exchange = self.config.get('topic_exchange', 'diamond')
-        self.vhost = self.config.get('vhost', '')
-        self.user = self.config.get('user', 'guest')
-        self.password = self.config.get('password', 'guest')
-        self.routing_key = self.config.get('routing_key', 'metric')
-        self.custom_routing_key = self.config.get(
-            'custom_routing_key', 'diamond')
+        self.server = self.config.get("server", "127.0.0.1")
+        self.port = int(self.config.get("port", 5672))
+        self.topic_exchange = self.config.get("topic_exchange", "diamond")
+        self.vhost = self.config.get("vhost", "")
+        self.user = self.config.get("user", "guest")
+        self.password = self.config.get("password", "guest")
+        self.routing_key = self.config.get("routing_key", "metric")
+        self.custom_routing_key = self.config.get("custom_routing_key", "diamond")
 
         if not pika:
-            self.log.error('pika import failed. Handler disabled')
+            self.log.error("pika import failed. Handler disabled")
             self.enabled = False
             return
 
         # Create rabbitMQ topic exchange and bind
         try:
             self._bind()
         except pika.exceptions.AMQPConnectionError:
-            self.log.error('Failed to bind to rabbitMQ topic exchange')
+            self.log.error("Failed to bind to rabbitMQ topic exchange")
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(rmqHandler, self).get_default_config_help()
 
-        config.update({
-            'server': '',
-            'topic_exchange': '',
-            'vhost': '',
-            'user': '',
-            'password': '',
-            'routing_key': '',
-            'custom_routing_key': '',
-        })
+        config.update(
+            {
+                "server": "",
+                "topic_exchange": "",
+                "vhost": "",
+                "user": "",
+                "password": "",
+                "routing_key": "",
+                "custom_routing_key": "",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(rmqHandler, self).get_default_config()
 
-        config.update({
-            'server': '127.0.0.1',
-            'topic_exchange': 'diamond',
-            'vhost': '/',
-            'user': 'guest',
-            'password': 'guest',
-            'port': '5672',
-        })
+        config.update(
+            {
+                "server": "127.0.0.1",
+                "topic_exchange": "diamond",
+                "vhost": "/",
+                "user": "guest",
+                "password": "guest",
+                "port": "5672",
+            }
+        )
 
         return config
 
     def _bind(self):
         """
-           Create  socket and bind
+        Create  socket and bind
         """
 
         credentials = pika.PlainCredentials(self.user, self.password)
-        params = pika.ConnectionParameters(credentials=credentials,
-                                           host=self.server,
-                                           virtual_host=self.vhost,
-                                           port=self.port)
+        params = pika.ConnectionParameters(
+            credentials=credentials,
+            host=self.server,
+            virtual_host=self.vhost,
+            port=self.port,
+        )
 
         self.connection = pika.BlockingConnection(params)
         self.channel = self.connection.channel()
 
         # NOTE : PIKA version uses 'exchange_type' instead of 'type'
 
-        self.channel.exchange_declare(exchange=self.topic_exchange,
-                                      exchange_type="topic")
+        self.channel.exchange_declare(
+            exchange=self.topic_exchange, exchange_type="topic"
+        )
 
     def __del__(self):
         """
-          Destroy instance of the rmqHandler class
+        Destroy instance of the rmqHandler class
         """
         try:
             self.connection.close()
         except AttributeError:
             pass
 
     def process(self, metric):
         """
-          Process a metric and send it to RabbitMQ topic exchange
+        Process a metric and send it to RabbitMQ topic exchange
         """
+
         # Send the data as ......
         if not pika:
             return
 
-        routingKeyDic = {
-            'metric': lambda: metric.path,
-            'custom': lambda: self.custom_routing_key,
-
+        routing_key_dic = {
+            "metric": lambda: metric.path,
+            "custom": lambda: self.custom_routing_key,
             # These option and the below are really not needed because
             #  with Rabbitmq you can use regular expressions to indicate
             #  what routing_keys to subscribe to. But I figure this is
             #  a good example of how to allow more routing keys
-
-            'host': lambda: metric.host,
-            'metric.path': metric.getMetricPath,
-            'path.prefix': metric.getPathPrefix,
-            'collector.path': metric.getCollectorPath,
+            "host": lambda: metric.host,
+            "metric.path": metric.getMetricPath,
+            "path.prefix": metric.getPathPrefix,
+            "collector.path": metric.getCollectorPath,
         }
 
         try:
             self.channel.basic_publish(
                 exchange=self.topic_exchange,
-                routing_key=routingKeyDic[self.routing_key](),
-                body="%s" % metric)
+                routing_key=routing_key_dic[self.routing_key](),
+                body="%s" % metric,
+            )
 
         except Exception:  # Rough connection re-try logic.
-            self.log.info(
-                "Failed publishing to rabbitMQ. Attempting reconnect")
+            self.log.info("Failed publishing to rabbitMQ. Attempting reconnect")
             self._bind()
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/tsdb.py` & `diamond-next-5.0.0/src/diamond/handler/statsite.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,191 +1,230 @@
 # coding=utf-8
 
 """
-Send metrics to a [OpenTSDB](http://opentsdb.net/) server.
+Send metrics to a [Statsite](https://github.com/armon/statsite/)
+using the default interface.
 
-[OpenTSDB](http://opentsdb.net/) is a distributed, scalable Time Series
-Database (TSDB) written on top of [HBase](http://hbase.org/). OpenTSDB was
-written to address a common need: store, index and serve metrics collected from
-computer systems (network gear, operating systems, applications) at a large
-scale, and make this data easily accessible and graphable.
-
-Thanks to HBase's scalability, OpenTSDB allows you to collect many thousands of
-metrics from thousands of hosts and applications, at a high rate (every few
-seconds). OpenTSDB will never delete or downsample data and can easily store
-billions of data points. As a matter of fact, StumbleUpon uses it to keep track
-of hundred of thousands of time series and collects over 1 billion data points
-per day in their main production datacenter.
-
-Imagine having the ability to quickly plot a graph showing the number of DELETE
-statements going to your MySQL database along with the number of slow queries
-and temporary files created, and correlate this with the 99th percentile of
-your service's latency. OpenTSDB makes generating such graphs on the fly a
-trivial operation, while manipulating millions of data point for very fine
-grained, real-time monitoring.
-
-==== Notes
-
-We don't automatically make the metrics via mkmetric, so we recommand you run
-with the null handler and log the output and extract the key values to mkmetric
-yourself.
+Statsite
+========
 
-- enable it in `diamond.conf` :
-
-`    handlers = diamond.handler.tsdb.TSDBHandler
-`
+This is a stats aggregation server. Statsite is based heavily
+on Etsy's [StatsD](https://github.com/etsy/statsd). This is
+a re-implementation of the Python version of
+[statsite](https://github.com/kiip/statsite).
+
+Features
+--------
+
+* Basic key/value metrics
+* Send timer data, statsite will calculate:
+  - Mean
+  - Min/Max
+  - Standard deviation
+  - Median, Percentile 95, Percentile 99
+* Send counters that statsite will aggregate
+
+
+Architecture
+-------------
+
+Statsite is designed to be both highly performant,
+and very flexible. To achieve this, it implements the stats
+collection and aggregation in pure C, using libev to be
+extremely fast. This allows it to handle hundreds of connections,
+and millions of metrics. After each flush interval expires,
+statsite performs a fork/exec to start a new stream handler
+invoking a specified application. Statsite then streams the
+aggregated metrics over stdin to the application, which is
+free to handle the metrics as it sees fit.
+
+This allows statsite to aggregate metrics and then ship metrics
+to any number of sinks (Graphite, SQL databases, etc). There
+is an included Python script that ships metrics to graphite.
+
+Additionally, statsite tries to minimize memory usage by not
+storing all the metrics that are received. Counter values are
+aggregated as they are received, and timer values are stored
+and aggregated using the Cormode-Muthurkrishnan algorithm from
+"Effective Computation of Biased Quantiles over Data Streams".
+This means that the percentile values are not perfectly accurate,
+and are subject to a specifiable error epsilon. This allows us to
+store only a fraction of the samples.
 
 """
 
-from Handler import Handler
 import socket
 
+from diamond.handler.Handler import Handler
+
 
-class TSDBHandler(Handler):
+class StatsiteHandler(Handler):
     """
-    Implements the abstract Handler class, sending data to graphite
+    Implements the abstract Handler class, sending data to statsite
     """
+
     RETRY = 3
 
     def __init__(self, config=None):
         """
-        Create a new instance of the TSDBHandler class
+        Create a new instance of the StatsiteHandler class
         """
+
         # Initialize Handler
         Handler.__init__(self, config)
 
         # Initialize Data
         self.socket = None
 
         # Initialize Options
-        self.host = self.config['host']
-        self.port = int(self.config['port'])
-        self.timeout = int(self.config['timeout'])
-        self.metric_format = str(self.config['format'])
-        self.tags = str(self.config['tags'])
+        self.host = self.config["host"]
+        self.tcpport = int(self.config["tcpport"])
+        self.udpport = int(self.config["udpport"])
+        self.timeout = int(self.config["timeout"])
 
         # Connect
         self._connect()
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
-        config = super(TSDBHandler, self).get_default_config_help()
+        config = super(StatsiteHandler, self).get_default_config_help()
 
-        config.update({
-            'host': '',
-            'port': '',
-            'timeout': '',
-            'format': '',
-            'tags': '',
-        })
+        config.update(
+            {
+                "host": "",
+                "tcpport": "",
+                "udpport": "",
+                "timeout": "",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
-        config = super(TSDBHandler, self).get_default_config()
+        config = super(StatsiteHandler, self).get_default_config()
 
-        config.update({
-            'host': '',
-            'port': 1234,
-            'timeout': 5,
-            'format': '{Collector}.{Metric} {timestamp} {value} hostname={host}'
-                      '{tags}',
-            'tags': '',
-        })
+        config.update(
+            {
+                "host": "",
+                "tcpport": 1234,
+                "udpport": 1234,
+                "timeout": 5,
+            }
+        )
 
         return config
 
     def __del__(self):
         """
-        Destroy instance of the TSDBHandler class
+        Destroy instance of the StatsiteHandler class
         """
         self._close()
 
     def process(self, metric):
         """
-        Process a metric by sending it to TSDB
+        Process a metric by sending it to statsite
         """
 
-        metric_str = self.metric_format.format(
-            Collector=metric.getCollectorPath(),
-            Path=metric.path,
-            Metric=metric.getMetricPath(),
-            host=metric.host,
-            timestamp=metric.timestamp,
-            value=metric.value,
-            tags=self.tags
-        )
         # Just send the data as a string
-        self._send("put " + str(metric_str) + "\n")
+        self._send(str(metric))
 
     def _send(self, data):
         """
-        Send data to TSDB. Data that can not be sent will be queued.
+        Send data to statsite. Data that can not be sent will be queued.
         """
         retry = self.RETRY
+
         # Attempt to send any data in the queue
         while retry > 0:
             # Check socket
             if not self.socket:
                 # Log Error
-                self.log.error("TSDBHandler: Socket unavailable.")
+                self.log.error("StatsiteHandler: Socket unavailable.")
+
                 # Attempt to restablish connection
                 self._connect()
+
                 # Decrement retry
                 retry -= 1
+
                 # Try again
                 continue
             try:
                 # Send data to socket
+                data = data.split()
+                data = data[0] + ":" + data[1] + "|kv\n"
                 self.socket.sendall(data)
+
                 # Done
                 break
-            except socket.error, e:
+            except socket.error as e:
                 # Log Error
-                self.log.error("TSDBHandler: Failed sending data. %s.", e)
+                self.log.error("StatsiteHandler: Failed sending data. %s.", e)
+
                 # Attempt to restablish connection
                 self._close()
+
                 # Decrement retry
                 retry -= 1
+
                 # try again
                 continue
 
     def _connect(self):
         """
-        Connect to the TSDB server
+        Connect to the statsite server
         """
         # Create socket
-        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        if self.udpport > 0:
+            self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+            self.port = self.udpport
+        elif self.tcpport > 0:
+            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+            self.port = self.tcpport
+
         if socket is None:
             # Log Error
-            self.log.error("TSDBHandler: Unable to create socket.")
+            self.log.error("StatsiteHandler: Unable to create socket.")
+
             # Close Socket
             self._close()
+
             return
+
         # Set socket timeout
         self.socket.settimeout(self.timeout)
-        # Connect to graphite server
+
+        # Connect to statsite server
         try:
             self.socket.connect((self.host, self.port))
+
             # Log
-            self.log.debug("Established connection to TSDB server %s:%d",
-                           self.host, self.port)
-        except Exception, ex:
+            self.log.debug(
+                "Established connection to statsite server %s:%d", self.host, self.port
+            )
+        except Exception as ex:
             # Log Error
-            self.log.error("TSDBHandler: Failed to connect to %s:%i. %s",
-                           self.host, self.port, ex)
+            self.log.error(
+                "StatsiteHandler: Failed to connect to %s:%i. %s",
+                self.host,
+                self.port,
+                ex,
+            )
+
             # Close Socket
             self._close()
+
             return
 
     def _close(self):
         """
         Close the socket
         """
         if self.socket is not None:
             self.socket.close()
+
         self.socket = None
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/multigraphite.py` & `diamond-next-5.0.0/src/diamond/handler/multigraphite.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,18 +2,19 @@
 
 """
 Send metrics to a [graphite](http://graphite.wikidot.com/) using the default
 interface. Unlike GraphiteHandler, this one supports multiple graphite servers.
 Specify them as a list of hosts divided by comma.
 """
 
-from Handler import Handler
-from graphite import GraphiteHandler
 from copy import deepcopy
 
+from diamond.handler.Handler import Handler
+from diamond.handler.graphite import GraphiteHandler
+
 
 class MultiGraphiteHandler(Handler):
     """
     Implements the abstract Handler class, sending data to multiple
     graphite servers by using two instances of GraphiteHandler
     """
 
@@ -23,53 +24,58 @@
         """
         # Initialize Handler
         Handler.__init__(self, config)
 
         self.handlers = []
 
         # Initialize Options
-        hosts = self.config['host']
+        hosts = self.config["host"]
+
         for host in hosts:
             config = deepcopy(self.config)
-            config['host'] = host
+            config["host"] = host
             self.handlers.append(GraphiteHandler(config))
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(MultiGraphiteHandler, self).get_default_config_help()
 
-        config.update({
-            'host': 'Hostname, Hostname, Hostname',
-            'port': 'Port',
-            'proto': 'udp or tcp',
-            'timeout': '',
-            'batch': 'How many to store before sending to the graphite server',
-            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
-            'trim_backlog_multiplier': 'Trim down how many batches',
-        })
+        config.update(
+            {
+                "host": "Hostname, Hostname, Hostname",
+                "port": "Port",
+                "proto": "udp or tcp",
+                "timeout": "",
+                "batch": "How many to store before sending to the graphite server",
+                "max_backlog_multiplier": "how many batches to store before trimming",  # NOQA
+                "trim_backlog_multiplier": "Trim down how many batches",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(MultiGraphiteHandler, self).get_default_config()
 
-        config.update({
-            'host': ['localhost'],
-            'port': 2003,
-            'proto': 'tcp',
-            'timeout': 15,
-            'batch': 1,
-            'max_backlog_multiplier': 5,
-            'trim_backlog_multiplier': 4,
-        })
+        config.update(
+            {
+                "host": ["localhost"],
+                "port": 2003,
+                "proto": "tcp",
+                "timeout": 15,
+                "batch": 1,
+                "max_backlog_multiplier": 5,
+                "trim_backlog_multiplier": 4,
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Process a metric by passing it to GraphiteHandler
         instances
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/riemann.py` & `diamond-next-5.0.0/src/diamond/handler/riemann.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,119 +1,133 @@
 # coding=utf-8
 
 """
 Send metrics to [Riemann](http://aphyr.github.com/riemann/).
 
 #### Dependencies
 
- * [Bernhard](https://github.com/banjiewen/bernhard).
+ * [riemann-client](https://github.com/borntyping/python-riemann-client).
 
 #### Configuration
 
 Add `diamond.handler.riemann.RiemannHandler` to your handlers.
 It has these options:
 
  * `host` - The Riemann host to connect to.
  * `port` - The port it's on.
  * `transport` - Either `tcp` or `udp`. (default: `tcp`)
 
 """
 
-from Handler import Handler
 import logging
+
+from diamond.handler.Handler import Handler
+
 try:
-    import bernhard
+    from riemann_client.transport import TCPTransport, UDPTransport
+    from riemann_client.client import Client
+
+    riemann_client = True
 except ImportError:
-    bernhard = None
+    riemann_client = None
 
 
 class RiemannHandler(Handler):
-
     def __init__(self, config=None):
         # Initialize Handler
         Handler.__init__(self, config)
 
-        if bernhard is None:
-            logging.error("Failed to load bernhard module")
+        if riemann_client is None:
+            logging.error("Failed to load riemann_client module")
             return
 
         # Initialize options
-        self.host = self.config['host']
-        self.port = int(self.config['port'])
-        self.transport = self.config['transport']
+        self.host = self.config["host"]
+        self.port = int(self.config["port"])
+        self.transport = self.config["transport"]
 
         # Initialize client
-        if self.transport == 'tcp':
-            transportCls = bernhard.TCPTransport
+        if self.transport == "tcp":
+            self.transport = TCPTransport(self.host, self.port)
         else:
-            transportCls = bernhard.UDPTransport
-        self.client = bernhard.Client(self.host, self.port, transportCls)
+            self.transport = UDPTransport(self.host, self.port)
+
+        self.client = Client(self.transport)
+        self._connect()
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(RiemannHandler, self).get_default_config_help()
 
-        config.update({
-            'host': '',
-            'port': '',
-            'transport': 'tcp or udp',
-        })
+        config.update(
+            {
+                "host": "",
+                "port": "",
+                "transport": "tcp or udp",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(RiemannHandler, self).get_default_config()
 
-        config.update({
-            'host': '',
-            'port': 123,
-            'transport': 'tcp',
-        })
+        config.update(
+            {
+                "host": "",
+                "port": 123,
+                "transport": "tcp",
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Send a metric to Riemann.
         """
         event = self._metric_to_riemann_event(metric)
+
         try:
-            self.client.send(event)
-        except Exception, e:
-            self.log.error("RiemannHandler: Error sending event to Riemann: %s",
-                           e)
+            self.client.send_event(event)
+        except Exception as e:
+            self.log.error("RiemannHandler: Error sending event to Riemann: %s", e)
 
     def _metric_to_riemann_event(self, metric):
         """
         Convert a metric to a dictionary representing a Riemann event.
         """
+
         # Riemann has a separate "host" field, so remove from the path.
-        path = '%s.%s.%s' % (
+        path = "%s.%s.%s" % (
             metric.getPathPrefix(),
             metric.getCollectorPath(),
-            metric.getMetricPath()
+            metric.getMetricPath(),
         )
 
-        return {
-            'host': metric.host,
-            'service': path,
-            'time': metric.timestamp,
-            'metric': float(metric.value),
-            'ttl': metric.ttl,
-        }
+        return self.client.create_event(
+            {
+                "host": metric.host,
+                "service": path,
+                "time": metric.timestamp,
+                "metric_f": float(metric.value),
+                "ttl": metric.ttl,
+            }
+        )
+
+    def _connect(self):
+        self.transport.connect()
 
     def _close(self):
         """
         Disconnect from Riemann.
         """
-        try:
-            self.client.disconnect()
-        except AttributeError:
-            pass
+        if hasattr(self, "transport"):
+            self.transport.disconnect()
 
     def __del__(self):
         self._close()
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/archive.py` & `diamond-next-5.0.0/src/diamond/handler/archive.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,76 +1,87 @@
 # coding=utf-8
 
 """
 Write the collected stats to a locally stored log file. Rotate the log file
 every night and remove after 7 days.
 """
 
-from Handler import Handler
 import logging
 import logging.handlers
 
+from diamond.handler.Handler import Handler
+
 
 class ArchiveHandler(Handler):
     """
     Implements the Handler abstract class, archiving data to a log file
     """
 
     def __init__(self, config):
         """
         Create a new instance of the ArchiveHandler class
         """
         # Initialize Handler
         Handler.__init__(self, config)
 
         # Create Archive Logger
-        self.archive = logging.getLogger('archive')
+        self.archive = logging.getLogger("archive")
         self.archive.setLevel(logging.DEBUG)
-        self.archive.propagate = self.config['propagate']
+        self.archive.propagate = self.config["propagate"]
+
         # Create Archive Log Formatter
-        formatter = logging.Formatter('%(message)s')
+        formatter = logging.Formatter("%(message)s")
+
         # Create Archive Log Handler
         handler = logging.handlers.TimedRotatingFileHandler(
-            filename=self.config['log_file'],
-            when='midnight',
-            interval=1,
-            backupCount=int(self.config['days']),
-            encoding=self.config['encoding']
+            filename=self.config["log_file"],
+            when=self.config["when"],
+            interval=int(self.config["rollover_interval"]),
+            backupCount=int(self.config["days"]),
+            encoding=self.config["encoding"],
         )
         handler.setFormatter(formatter)
         handler.setLevel(logging.DEBUG)
         self.archive.addHandler(handler)
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(ArchiveHandler, self).get_default_config_help()
 
-        config.update({
-            'log_file': 'Path to the logfile',
-            'days': 'How many days to store',
-            'encoding': '',
-            'propagate': 'Pass handled metrics to configured root logger',
-        })
+        config.update(
+            {
+                "log_file": "Path to the logfile",
+                "when": "type of interval; S, M, H, D, Weekday, midnight",
+                "days": "How many days to store",
+                "rollover_interval": "rollover interval length",
+                "encoding": "",
+                "propagate": "Pass handled metrics to configured root logger",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(ArchiveHandler, self).get_default_config()
 
-        config.update({
-            'log_file': '',
-            'days': 7,
-            'encoding': None,
-            'propagate': False,
-        })
+        config.update(
+            {
+                "log_file": "",
+                "when": "midnight",
+                "days": 7,
+                "rollover_interval": 1,
+                "encoding": None,
+                "propagate": False,
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Send a Metric to the Archive.
         """
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/signalfx.py` & `diamond-next-5.0.0/src/collectors/postfix/postfix.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,139 +1,126 @@
 # coding=utf-8
 
 """
-Send metrics to signalfx
+Collect stats from postfix-stats. postfix-stats is a simple threaded stats
+aggregator for Postfix. When running as a syslog destination, it can be used to
+get realtime cumulative stats.
 
 #### Dependencies
 
- * urllib2
+ * socket
+ * json (or simplejson)
+ * [postfix-stats](https://github.com/disqus/postfix-stats)
 
+"""
 
-#### Configuration
-Enable this handler
+import socket
 
- * handers = diamond.handler.httpHandler.SignalfxHandler
+import diamond.collector
 
- * auth_token = SIGNALFX_AUTH_TOKEN
- * batch_size = [optional | 300 ] will wait for this many requests before
-     posting
-"""
+try:
+    import json
+except ImportError:
+    import simplejson as json
 
-from Handler import Handler
-from diamond.util import get_diamond_version
-import json
-import logging
-import time
-import urllib2
-
-
-class SignalfxHandler(Handler):
-
-    # Inititalize Handler with url and batch size
-    def __init__(self, config=None):
-        Handler.__init__(self, config)
-        self.metrics = []
-        self.batch_size = int(self.config['batch'])
-        self.url = self.config['url']
-        self.auth_token = self.config['auth_token']
-        self.batch_max_interval = self.config['batch_max_interval']
-        self.resetBatchTimeout()
-        if self.auth_token == "":
-            logging.error("Failed to load Signalfx module")
-            return
+DOTS_TO_UNDERS = {ord("."): "_"}
 
-    def resetBatchTimeout(self):
-        self.batch_max_timestamp = int(time.time() + self.batch_max_interval)
 
+class PostfixCollector(diamond.collector.Collector):
     def get_default_config_help(self):
-        """
-        Returns the help text for the configuration options for this handler
-        """
-        config = super(SignalfxHandler, self).get_default_config_help()
-
-        config.update({
-            'url': 'Where to send metrics',
-            'batch': 'How many to store before sending',
-            'auth_token': 'Org API token to use when sending metrics',
-        })
-
-        return config
+        config_help = super(PostfixCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "Hostname to connect to",
+                "port": "Port to connect to",
+                "include_clients": "Include client connection stats",
+            }
+        )
+        return config_help
 
     def get_default_config(self):
         """
-        Return the default config for the handler
+        Returns the default collector settings
         """
-        config = super(SignalfxHandler, self).get_default_config()
-
-        config.update({
-            'url': 'https://ingest.signalfx.com/v2/datapoint',
-            'batch': 300,
-            # Don't wait more than 10 sec between pushes
-            'batch_max_interval': 10,
-            'auth_token': '',
-        })
-
+        config = super(PostfixCollector, self).get_default_config()
+        config.update(
+            {
+                "path": "postfix",
+                "host": "localhost",
+                "port": 7777,
+                "include_clients": True,
+            }
+        )
         return config
 
-    def process(self, metric):
-        """
-        Queue a metric.  Flushing queue if batch size reached
-        """
-        self.metrics.append(metric)
-        if self.should_flush():
-            self._send()
-
-    def should_flush(self):
-        return len(self.metrics) >= self.batch_size or \
-            time.time() >= self.batch_max_timestamp
+    def get_json(self):
+        json_string = ""
+        address = (self.config["host"], int(self.config["port"]))
+        s = None
 
-    def into_signalfx_point(self, metric):
-        """
-        Convert diamond metric into something signalfx can understand
-        """
-        dims = {
-            "collector": metric.getCollectorPath(),
-            "prefix": metric.getPathPrefix(),
-        }
-        if metric.host is not None and metric.host != "":
-            dims["host"] = metric.host
-
-        return {
-            "metric": metric.getMetricPath(),
-            "value": metric.value,
-            "dimensions": dims,
-            # We expect ms timestamps
-            "timestamp": metric.timestamp * 1000,
-        }
-
-    def flush(self):
-        """Flush metrics in queue"""
-        self._send()
+        try:
+            try:
+                s = socket.create_connection(address, timeout=1)
+                s.sendall("stats\n")
+
+                while 1:
+                    data = s.recv(4096)
+
+                    if not data:
+                        break
+
+                    json_string += data
+            except socket.error:
+                self.log.exception("Error talking to postfix-stats")
+                return "{}"
+        finally:
+            if s:
+                s.close()
 
-    def user_agent(self):
-        """
-        HTTP user agent
-        """
-        return "Diamond: %s" % get_diamond_version()
+        return json_string or "{}"
+
+    def get_data(self):
+        json_string = self.get_json()
 
-    def _send(self):
-        # Potentially use protobufs in the future
-        postDictionary = {}
-        for metric in self.metrics:
-            t = metric.metric_type.lower()
-            if t not in postDictionary:
-                postDictionary[t] = []
-            postDictionary[t].append(self.into_signalfx_point(metric))
-
-        self.metrics = []
-        postBody = json.dumps(postDictionary)
-        logging.debug("Body is %s", postBody)
-        req = urllib2.Request(self.url, postBody,
-                              {"Content-type": "application/json",
-                               "X-SF-TOKEN": self.auth_token,
-                               "User-Agent": self.user_agent()})
-        self.resetBatchTimeout()
         try:
-            urllib2.urlopen(req)
-        except urllib2.URLError:
-            logging.exception("Unable to post signalfx metrics")
+            data = json.loads(json_string)
+        except (ValueError, TypeError):
+            self.log.exception("Error parsing json from postfix-stats")
+            return None
+
+        return data
+
+    def collect(self):
+        data = self.get_data()
+
+        if not data:
             return
+
+        if (
+            diamond.collector.str_to_bool(self.config["include_clients"])
+            and "clients" in data
+        ):
+            for client, value in iter(data["clients"].items()):
+                # translate dots to underscores in client names
+                metric = ".".join(["clients", client.translate(DOTS_TO_UNDERS)])
+
+                dvalue = self.derivative(metric, value)
+
+                self.publish(metric, dvalue, precision=4)
+
+        for action in ("in", "recv", "send"):
+            if action not in data:
+                continue
+
+            for sect, stats in iter(data[action].items()):
+                for status, value in iter(stats.items()):
+                    metric = ".".join([action, sect, status.translate(DOTS_TO_UNDERS)])
+                    dvalue = self.derivative(metric, value)
+
+                    self.publish(metric, dvalue, precision=4)
+
+        if "local" in data:
+            for key, value in iter(data["local"].items()):
+                metric = ".".join(["local", key])
+                dvalue = self.derivative(metric, value)
+
+                self.publish(metric, dvalue, precision=4)
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/multigraphitepickle.py` & `diamond-next-5.0.0/src/diamond/handler/multigraphitepickle.py`

 * *Files 24% similar despite different names*

```diff
@@ -2,18 +2,19 @@
 
 """
 Send metrics to a [graphite](http://graphite.wikidot.com/) using the pickle
 interface. Unlike GraphitePickleHandler, this one supports multiple graphite
 servers. Specify them as a list of hosts divided by comma.
 """
 
-from Handler import Handler
-from graphitepickle import GraphitePickleHandler
 from copy import deepcopy
 
+from diamond.handler.Handler import Handler
+from diamond.handler.graphitepickle import GraphitePickleHandler
+
 
 class MultiGraphitePickleHandler(Handler):
     """
     Implements the abstract Handler class, sending data to multiple
     graphite servers by using two instances of GraphitePickleHandler
     """
 
@@ -23,54 +24,58 @@
         """
         # Initialize Handler
         Handler.__init__(self, config)
 
         self.handlers = []
 
         # Initialize Options
-        hosts = self.config['host']
+        hosts = self.config["host"]
+
         for host in hosts:
             config = deepcopy(self.config)
-            config['host'] = host
+            config["host"] = host
             self.handlers.append(GraphitePickleHandler(config))
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
-        config = super(MultiGraphitePickleHandler,
-                       self).get_default_config_help()
+        config = super(MultiGraphitePickleHandler, self).get_default_config_help()
 
-        config.update({
-            'host': 'Hostname, Hostname, Hostname',
-            'port': 'Port',
-            'proto': 'udp or tcp',
-            'timeout': '',
-            'batch': 'How many to store before sending to the graphite server',
-            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
-            'trim_backlog_multiplier': 'Trim down how many batches',
-        })
+        config.update(
+            {
+                "host": "Hostname, Hostname, Hostname",
+                "port": "Port",
+                "proto": "udp or tcp",
+                "timeout": "",
+                "batch": "How many to store before sending to the graphite server",
+                "max_backlog_multiplier": "how many batches to store before trimming",  # NOQA
+                "trim_backlog_multiplier": "Trim down how many batches",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(MultiGraphitePickleHandler, self).get_default_config()
 
-        config.update({
-            'host': ['localhost'],
-            'port': 2003,
-            'proto': 'tcp',
-            'timeout': 15,
-            'batch': 1,
-            'max_backlog_multiplier': 5,
-            'trim_backlog_multiplier': 4,
-        })
+        config.update(
+            {
+                "host": ["localhost"],
+                "port": 2003,
+                "proto": "tcp",
+                "timeout": 15,
+                "batch": 1,
+                "max_backlog_multiplier": 5,
+                "trim_backlog_multiplier": 4,
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Process a metric by passing it to GraphitePickleHandler
         instances
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/httpHandler.py` & `diamond-next-5.0.0/src/diamond/handler/httpHandler.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,61 +1,66 @@
 #!/usr/bin/env python
 # coding=utf-8
 
 """
 Send metrics to a http endpoint via POST
 """
 
-from Handler import Handler
-import urllib2
+import urllib.request
 
+import diamond.handler.Handler
 
-class HttpPostHandler(Handler):
 
+class HttpPostHandler(diamond.handler.Handler.Handler):
     # Inititalize Handler with url and batch size
     def __init__(self, config=None):
-        Handler.__init__(self, config)
+        diamond.handler.Handler.Handler.__init__(self, config)
         self.metrics = []
-        self.batch_size = int(self.config['batch'])
-        self.url = self.config.get('url')
+        self.batch_size = int(self.config["batch"])
+        self.url = self.config.get("url")
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(HttpPostHandler, self).get_default_config_help()
 
-        config.update({
-            'url': 'Fully qualified url to send metrics to',
-            'batch': 'How many to store before sending to the graphite server',
-        })
+        config.update(
+            {
+                "url": "Fully qualified url to send metrics to",
+                "batch": "How many to store before sending to the graphite server",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(HttpPostHandler, self).get_default_config()
 
-        config.update({
-            'url': 'http://localhost/blah/blah/blah',
-            'batch': 100,
-        })
+        config.update(
+            {
+                "url": "http://localhost/blah/blah/blah",
+                "batch": 100,
+            }
+        )
 
         return config
 
     # Join batched metrics and push to url mentioned in config
     def process(self, metric):
         self.metrics.append(str(metric))
+
         if len(self.metrics) >= self.batch_size:
             self.post()
 
     # Overriding flush to post metrics for every collector.
     def flush(self):
         """Flush metrics in queue"""
         self.post()
 
     def post(self):
-        req = urllib2.Request(self.url, "\n".join(self.metrics))
-        urllib2.urlopen(req)
+        req = urllib.request.Request(self.url, b"\n".join(self.metrics))
+        urllib.request.urlopen(req)
         self.metrics = []
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/logentries_diamond.py` & `diamond-next-5.0.0/src/diamond/handler/logentries_diamond.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,83 +1,86 @@
 # coding=utf-8
+
 """
 [Logentries: Log Management & Analytics Made Easy ](https://logentries.com/).
 Send Diamond stats to your Logentries Account where you can monitor and alert
 based on data in real time.
 """
 
-from Handler import Handler
-import logging
-import urllib2
 import json
-from collections import deque
+import logging
+import collections
+import urllib.error
+import urllib.request
+
+import diamond.handler.Handler
 
 
-class LogentriesDiamondHandler(Handler):
+class LogentriesDiamondHandler(diamond.handler.Handler.Handler):
     """
-      Implements the abstract Handler class
+    Implements the abstract Handler class
     """
 
     def __init__(self, config=None):
         """
         New instance of LogentriesDiamondHandler class
         """
 
-        Handler.__init__(self, config)
-        self.log_token = self.config.get('log_token', None)
-        self.queue_size = int(self.config['queue_size'])
-        self.queue = deque([])
+        diamond.handler.Handler.Handler.__init__(self, config)
+        self.log_token = self.config.get("log_token", None)
+        self.queue_size = int(self.config["queue_size"])
+        self.queue = collections.deque([])
+
         if self.log_token is None:
             raise Exception
 
     def get_default_config_help(self):
         """
         Help text
         """
-        config = super(LogentriesDiamondHandler,
-                       self).get_default_config_help()
+        config = super(LogentriesDiamondHandler, self).get_default_config_help()
 
-        config.update({
-            'log_token':
-                '[Your log token](https://logentries.com/doc/input-token/)',
-            'queue_size': ''
-        })
+        config.update(
+            {
+                "log_token": "[Your log token](https://logentries.com/doc/input-token/)",
+                "queue_size": "",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return default config for the handler
         """
         config = super(LogentriesDiamondHandler, self).get_default_config()
 
-        config.update({
-            'log_token': '',
-            'queue_size': 100
-        })
+        config.update({"log_token": "", "queue_size": 100})
 
         return config
 
     def process(self, metric):
         """
         Process metric by sending it to datadog api
         """
-
         self.queue.append(metric)
+
         if len(self.queue) >= self.queue_size:
             logging.debug("Queue is full, sending logs to Logentries")
             self._send()
 
     def _send(self):
         """
         Convert message to a json object and send to Lognetries
         """
         while len(self.queue) > 0:
             metric = self.queue.popleft()
             topic, value, timestamp = str(metric).split()
             msg = json.dumps({"event": {topic: value}})
-            req = urllib2.Request("https://js.logentries.com/v1/logs/" +
-                                  self.log_token, msg)
+            req = urllib.request.Request(
+                "https://js.logentries.com/v1/logs/" + self.log_token, msg
+            )
+
             try:
-                urllib2.urlopen(req)
-            except urllib2.URLError, e:
+                urllib.request.urlopen(req)
+            except urllib.error.URLError as e:
                 logging.error("Can't send log message to Logentries %s", e)
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/cloudwatch.py` & `diamond-next-5.0.0/src/diamond/handler/cloudwatch.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 # coding=utf-8
+
 """
 Output the collected values to AWS CloudWatch
 
 Automatically adds the InstanceId Dimension
 
 #### Dependencies
 
@@ -16,201 +17,265 @@
 
 Example Config:
 
 [[cloudwatchHandler]]
 region = us-east-1
 
 [[[LoadAvg01]]]
+collect_by_instance = True
+collect_without_dimension = False
 collector = loadavg
 metric = 01
-namespace = MachineLoad
 name = Avg01
+namespace = MachineLoad
 unit = None
 
 [[[LoadAvg05]]]
+collect_by_instance = True
+collect_without_dimension = False
 collector = loadavg
 metric = 05
-namespace = MachineLoad
 name = Avg05
+namespace = MachineLoad
 unit = None
 """
 
-import sys
 import datetime
+import sys
 
-from Handler import Handler
 from configobj import Section
 
+from diamond.handler.Handler import Handler
+
 try:
     import boto
     import boto.ec2.cloudwatch
     import boto.utils
 except ImportError:
     boto = None
 
 
 class cloudwatchHandler(Handler):
     """
-      Implements the abstract Handler class
-      Sending data to a AWS CloudWatch
+    Implements the abstract Handler class
+    Sending data to a AWS CloudWatch
     """
 
     def __init__(self, config=None):
         """
-          Create a new instance of cloudwatchHandler class
+        Create a new instance of cloudwatchHandler class
         """
 
         # Initialize Handler
         Handler.__init__(self, config)
 
         if not boto:
-            self.log.error(
-                "CloudWatch: Boto is not installed, please install boto.")
+            self.log.error("CloudWatch: Boto is not installed, please install boto.")
             return
 
         # Initialize Data
         self.connection = None
 
         # Initialize Options
-        self.region = self.config['region']
-        instances = boto.utils.get_instance_metadata()
-        if 'instance-id' not in instances:
-            self.log.error('CloudWatch: Failed to load instance metadata')
-            return
-        self.instance_id = instances['instance-id']
-        self.log.debug("Setting InstanceId: " + self.instance_id)
+        self.region = self.config["region"]
 
-        self.valid_config = ('region', 'collector', 'metric', 'namespace',
-                             'name', 'unit')
+        instance_metadata = boto.utils.get_instance_metadata()
 
+        if "instance-id" in instance_metadata:
+            self.instance_id = instance_metadata["instance-id"]
+            self.log.debug("Setting InstanceId: " + self.instance_id)
+        else:
+            self.instance_id = None
+            self.log.error("CloudWatch: Failed to load instance metadata")
+
+        self.valid_config = (
+            "region",
+            "collector",
+            "metric",
+            "namespace",
+            "name",
+            "unit",
+            "collect_by_instance",
+            "collect_without_dimension",
+        )
         self.rules = []
+
         for key_name, section in self.config.items():
             if section.__class__ is Section:
                 keys = section.keys()
-                rules = {}
+                rules = self.get_default_rule_config()
+
                 for key in keys:
                     if key not in self.valid_config:
-                        self.log.warning("invalid key %s in section %s",
-                                         key, section.name)
+                        self.log.warning(
+                            "invalid key %s in section %s", key, section.name
+                        )
                     else:
                         rules[key] = section[key]
 
                 self.rules.append(rules)
 
         # Create CloudWatch Connection
         self._bind()
 
+    def get_default_rule_config(self):
+        """
+        Return the default config for a rule
+        """
+        config = {}
+        config.update(
+            {
+                "collector": "",
+                "metric": "",
+                "namespace": "",
+                "name": "",
+                "unit": "None",
+                "collect_by_instance": True,
+                "collect_without_dimension": False,
+            }
+        )
+
+        return config
+
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(cloudwatchHandler, self).get_default_config_help()
 
-        config.update({
-            'region': '',
-            'metric': '',
-            'namespace': '',
-            'name': '',
-            'unit': '',
-            'collector': '',
-        })
+        config.update(
+            {
+                "region": "AWS region",
+                "metric": "Diamond metric name",
+                "namespace": "CloudWatch metric namespace",
+                "name": "CloudWatch metric name",
+                "unit": "CloudWatch metric unit",
+                "collector": "Diamond collector name",
+                "collect_by_instance": "Collect metrics for instances separately",
+                "collect_without_dimension": "Collect metrics without dimension",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(cloudwatchHandler, self).get_default_config()
 
-        config.update({
-            'region': 'us-east-1',
-            'collector': 'loadavg',
-            'metric': '01',
-            'namespace': 'MachineLoad',
-            'name': 'Avg01',
-            'unit': 'None',
-        })
+        config.update(
+            {
+                "region": "us-east-1",
+                "collector": "loadavg",
+                "metric": "01",
+                "namespace": "MachineLoad",
+                "name": "Avg01",
+                "unit": "None",
+                "collect_by_instance": True,
+                "collect_without_dimension": False,
+            }
+        )
 
         return config
 
     def _bind(self):
         """
-           Create CloudWatch Connection
+        Create CloudWatch Connection
         """
 
         self.log.debug(
-            "CloudWatch: Attempting to connect to CloudWatch at Region: %s",
-            self.region)
+            "CloudWatch: Attempting to connect to CloudWatch at Region: %s", self.region
+        )
+
         try:
-            self.connection = boto.ec2.cloudwatch.connect_to_region(
-                self.region)
+            self.connection = boto.ec2.cloudwatch.connect_to_region(self.region)
             self.log.debug(
                 "CloudWatch: Succesfully Connected to CloudWatch at Region: %s",
-                self.region)
+                self.region,
+            )
         except boto.exception.EC2ResponseError:
-            self.log.error('CloudWatch: CloudWatch Exception Handler: ')
+            self.log.error("CloudWatch: CloudWatch Exception Handler: ")
 
     def __del__(self):
         """
-          Destroy instance of the cloudWatchHandler class
+        Destroy instance of the cloudWatchHandler class
         """
         try:
             self.connection = None
         except AttributeError:
             pass
 
     def process(self, metric):
         """
-          Process a metric and send it to CloudWatch
+        Process a metric and send it to CloudWatch
         """
         if not boto:
             return
 
         collector = str(metric.getCollectorPath())
         metricname = str(metric.getMetricPath())
-        timestamp = datetime.datetime.fromtimestamp(metric.timestamp)
 
         # Send the data as ......
 
         for rule in self.rules:
             self.log.debug(
-                "Comparing Collector: [%s] with (%s) "
-                "and Metric: [%s] with (%s)",
-                str(rule['collector']),
+                "Comparing Collector: [%s] with (%s) " "and Metric: [%s] with (%s)",
+                str(rule["collector"]),
                 collector,
-                str(rule['metric']),
-                metricname
+                str(rule["metric"]),
+                metricname,
             )
 
-            if ((str(rule['collector']) == collector and
-                 str(rule['metric']) == metricname)):
-                self.log.debug(
-                    "CloudWatch: Attempting to publish metric: %s to %s "
-                    "with value (%s) @%s",
-                    rule['name'],
-                    rule['namespace'],
-                    str(metric.value),
-                    str(metric.timestamp)
-                )
-                try:
-                    self.connection.put_metric_data(
-                        str(rule['namespace']),
-                        str(rule['name']),
-                        str(metric.value),
-                        timestamp, str(rule['unit']),
-                        {'InstanceId': self.instance_id})
-                    self.log.debug(
-                        "CloudWatch: Successfully published metric: %s to"
-                        " %s with value (%s)",
-                        rule['name'],
-                        rule['namespace'],
-                        str(metric.value)
+            if (
+                str(rule["collector"]) == collector
+                and str(rule["metric"]) == metricname
+            ):
+                if rule["collect_by_instance"] and self.instance_id:
+                    self.send_metrics_to_cloudwatch(
+                        rule, metric, {"InstanceId": self.instance_id}
                     )
-                except AttributeError, e:
-                    self.log.error(
-                        "CloudWatch: Failed publishing - %s ", str(e))
-                except Exception:  # Rough connection re-try logic.
-                    self.log.error(
-                        "CloudWatch: Failed publishing - %s ",
-                        str(sys.exc_info()[0]))
-                    self._bind()
+
+                if rule["collect_without_dimension"]:
+                    self.send_metrics_to_cloudwatch(rule, metric, {})
+
+    def send_metrics_to_cloudwatch(self, rule, metric, dimensions):
+        """
+        Send metrics to CloudWatch for the given dimensions
+        """
+
+        timestamp = datetime.datetime.utcfromtimestamp(metric.timestamp)
+
+        self.log.debug(
+            "CloudWatch: Attempting to publish metric: %s to %s with value (%s) for dimensions %s @%s",
+            rule["name"],
+            rule["namespace"],
+            str(metric.value),
+            str(dimensions),
+            str(metric.timestamp),
+        )
+
+        try:
+            self.connection.put_metric_data(
+                str(rule["namespace"]),
+                str(rule["name"]),
+                str(metric.value),
+                timestamp,
+                str(rule["unit"]),
+                dimensions,
+            )
+            self.log.debug(
+                "CloudWatch: Successfully published metric: %s to %s with value (%s) for dimensions %s",
+                rule["name"],
+                rule["namespace"],
+                str(metric.value),
+                str(dimensions),
+            )
+        except AttributeError as e:
+            self.log.error("CloudWatch: Failed publishing - %s ", str(e))
+        except Exception as e:  # Rough connection re-try logic.
+            self.log.error(
+                "CloudWatch: Failed publishing - %s\n%s ",
+                str(e),
+                str(sys.exc_info()[0]),
+            )
+            self._bind()
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/queue.py` & `diamond-next-5.0.0/src/diamond/handler/queue.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # coding=utf-8
 
 """
 This is a meta handler to act as a shim for the new threading model. Please
 do not try to use it as a normal handler
 """
 
-from Handler import Handler
-import Queue
+from queue import Full
 
+from diamond.handler.Handler import Handler
 
-class QueueHandler(Handler):
 
+class QueueHandler(Handler):
     def __init__(self, config=None, queue=None, log=None):
         # Initialize Handler
         Handler.__init__(self, config=config, log=log)
 
         self.queue = queue
 
     def __del__(self):
@@ -30,23 +30,24 @@
     def _process(self, metric):
         """
         We skip any locking code due to the fact that this is now a single
         process per collector
         """
         try:
             self.queue.put(metric, block=False)
-        except Queue.Full:
-            self._throttle_error('Queue full, check handlers for delays')
+        except Full:
+            self._throttle_error("Queue full, check handlers for delays")
 
     def flush(self):
         return self._flush()
 
     def _flush(self):
         """
         We skip any locking code due to the fact that this is now a single
         process per collector
         """
+
         # Send a None down the queue to indicate a flush
         try:
             self.queue.put(None, block=False)
-        except Queue.Full:
-            self._throttle_error('Queue full, check handlers for delays')
+        except Full:
+            self._throttle_error("Queue full, check handlers for delays")
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/mysql.py` & `diamond-next-5.0.0/src/diamond/handler/mysql.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,108 +1,123 @@
 # coding=utf-8
 
 """
 Insert the collected values into a mysql table
+
+#### Dependencies
+
+ * mysqlclient
+
 """
 
-from Handler import Handler
-import MySQLdb
+from diamond.handler.Handler import Handler
+
+try:
+    import mysqlclient
+except ImportError:
+    mysqlclient = None
 
 
 class MySQLHandler(Handler):
     """
     Implements the abstract Handler class, sending data to a mysql table
     """
+
     conn = None
 
     def __init__(self, config=None):
         """
         Create a new instance of the MySQLHandler class
         """
+
         # Initialize Handler
         Handler.__init__(self, config)
 
         # Initialize Options
-        self.hostname = self.config['hostname']
-        self.port = int(self.config['port'])
-        self.username = self.config['username']
-        self.password = self.config['password']
-        self.database = self.config['database']
-        self.table = self.config['table']
-        self.col_time = self.config['col_time']
-        self.col_metric = self.config['col_metric']
-        self.col_value = self.config['col_value']
+        self.hostname = self.config["hostname"]
+        self.port = int(self.config["port"])
+        self.username = self.config["username"]
+        self.password = self.config["password"]
+        self.database = self.config["database"]
+        self.table = self.config["table"]
+        self.col_time = self.config["col_time"]
+        self.col_metric = self.config["col_metric"]
+        self.col_value = self.config["col_value"]
 
         # Connect
         self._connect()
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(MySQLHandler, self).get_default_config_help()
 
-        config.update({
-        })
+        config.update({})
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(MySQLHandler, self).get_default_config()
 
-        config.update({
-        })
+        config.update({})
 
         return config
 
     def __del__(self):
         """
         Destroy instance of the MySQLHandler class
         """
         self._close()
 
     def process(self, metric):
         """
         Process a metric
         """
+
         # Just send the data
         self._send(str(metric))
 
     def _send(self, data):
         """
         Insert the data
         """
-        data = data.strip().split(' ')
+        data = data.strip().split(" ")
+
         try:
             cursor = self.conn.cursor()
-            cursor.execute("INSERT INTO %s (%s, %s, %s) VALUES(%%s, %%s, %%s)"
-                           % (self.table, self.col_metric,
-                              self.col_time, self.col_value),
-                           (data[0], data[2], data[1]))
+            cursor.execute(
+                "INSERT INTO %s (%s, %s, %s) VALUES(%%s, %%s, %%s)"
+                % (self.table, self.col_metric, self.col_time, self.col_value),
+                (data[0], data[2], data[1]),
+            )
             cursor.close()
             self.conn.commit()
-        except BaseException, e:
+        except BaseException as e:
             # Log Error
             self.log.error("MySQLHandler: Failed sending data. %s.", e)
+
             # Attempt to restablish connection
             self._connect()
 
     def _connect(self):
         """
         Connect to the MySQL server
         """
         self._close()
-        self.conn = MySQLdb.Connect(host=self.hostname,
-                                    port=self.port,
-                                    user=self.username,
-                                    passwd=self.password,
-                                    db=self.database)
+        self.conn = mysqlclient.Connect(
+            host=self.hostname,
+            port=self.port,
+            user=self.username,
+            passwd=self.password,
+            db=self.database,
+        )
 
     def _close(self):
         """
         Close the connection
         """
         if self.conn:
             self.conn.commit()
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/graphitepickle.py` & `diamond-next-5.0.0/src/diamond/handler/graphitepickle.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,80 +12,87 @@
 deployed it at other companies including Sears, where it serves as a pillar of
 the e-commerce monitoring system. Today many
 [large companies](http://graphite.readthedocs.org/en/latest/who-is-using.html)
 use it.
 
 """
 
+import pickle
 import struct
 
-from graphite import GraphiteHandler
-
-try:
-    import cPickle as pickle
-except ImportError:
-    import pickle as pickle
+from diamond.handler.graphite import GraphiteHandler
 
 
 class GraphitePickleHandler(GraphiteHandler):
     """
     Overrides the GraphiteHandler class
     Sending data to graphite using batched pickle format
     """
 
     def __init__(self, config=None):
         """
         Create a new instance of the GraphitePickleHandler
         """
+
         # Initialize GraphiteHandler
         GraphiteHandler.__init__(self, config)
+
         # Initialize Data
         self.batch = []
+
         # Initialize Options
-        self.batch_size = int(self.config['batch'])
+        self.batch_size = int(self.config["batch"])
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(GraphitePickleHandler, self).get_default_config_help()
 
-        config.update({
-        })
+        config.update({})
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(GraphitePickleHandler, self).get_default_config()
 
-        config.update({
-            'port': 2004,
-        })
+        config.update(
+            {
+                "port": 2004,
+            }
+        )
 
         return config
 
     def process(self, metric):
         # Convert metric to pickle format
         m = (metric.path, (metric.timestamp, metric.value))
+
         # Add the metric to the match
         self.batch.append(m)
+
         # If there are sufficient metrics, then pickle and send
         if len(self.batch) >= self.batch_size:
             # Log
-            self.log.debug("GraphitePickleHandler: Sending batch size: %d",
-                           self.batch_size)
+            self.log.debug(
+                "GraphitePickleHandler: Sending batch size: %d", self.batch_size
+            )
+
             # Pickle the batch of metrics
             self.metrics = [self._pickle_batch()]
+
             # Send pickled batch
             self._send()
+
             # Flush the metric pack down the wire
             self.flush()
+
             # Clear Batch
             self.batch = []
 
     def _pickle_batch(self):
         """
         Pickle the metrics into a form that can be understood
         by the graphite pickle connector.
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/g_metric.py` & `diamond-next-5.0.0/src/diamond/handler/g_metric.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 # coding=utf-8
 
 """
 Emulate a gmetric client for usage with
 [Ganglia Monitoring System](http://ganglia.sourceforge.net/)
 """
 
-from Handler import Handler
 import logging
+
+from diamond.handler.Handler import Handler
+
 try:
     import gmetric
 except ImportError:
     gmetric = None
 
 
 class GmetricHandler(Handler):
@@ -19,59 +21,66 @@
     gmetric does.
     """
 
     def __init__(self, config=None):
         """
         Create a new instance of the GmetricHandler class
         """
+
         # Initialize Handler
         Handler.__init__(self, config)
 
         if gmetric is None:
             logging.error("Failed to load gmetric module")
+
             return
 
         # Initialize Data
         self.socket = None
 
         # Initialize Options
-        self.host = self.config['host']
-        self.port = int(self.config['port'])
-        self.protocol = self.config['protocol']
+        self.host = self.config["host"]
+        self.port = int(self.config["port"])
+        self.protocol = self.config["protocol"]
+
         if not self.protocol:
-            self.protocol = 'udp'
+            self.protocol = "udp"
 
         # Initialize
         self.gmetric = gmetric.Gmetric(self.host, self.port, self.protocol)
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(GmetricHandler, self).get_default_config_help()
 
-        config.update({
-            'host': 'Hostname',
-            'port': 'Port',
-            'protocol': 'udp or tcp',
-        })
+        config.update(
+            {
+                "host": "Hostname",
+                "port": "Port",
+                "protocol": "udp or tcp",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(GmetricHandler, self).get_default_config()
 
-        config.update({
-            'host': 'localhost',
-            'port': 8651,
-            'protocol': 'udp',
-        })
+        config.update(
+            {
+                "host": "localhost",
+                "port": 8651,
+                "protocol": "udp",
+            }
+        )
 
         return config
 
     def __del__(self):
         """
         Destroy instance of the GmetricHandler class
         """
@@ -88,25 +97,21 @@
         """
         Send data to gmond.
         """
         metric_name = self.get_name_from_path(metric.path)
         tmax = "60"
         dmax = "0"
         slope = "both"
+
         # FIXME: Badness, shouldn't *assume* double type
         metric_type = "double"
         units = ""
         group = ""
-        self.gmetric.send(metric_name,
-                          metric.value,
-                          metric_type,
-                          units,
-                          slope,
-                          tmax,
-                          dmax,
-                          group)
+        self.gmetric.send(
+            metric_name, metric.value, metric_type, units, slope, tmax, dmax, group
+        )
 
     def _close(self):
         """
         Close the connection
         """
         self.gmetric = None
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/stats_d.py` & `diamond-next-5.0.0/src/diamond/handler/stats_d.py`

 * *Files 15% similar despite different names*

```diff
@@ -23,77 +23,84 @@
 The handler file is named an odd stats_d.py because of an import issue with
 having the python library called statsd and this handler's module being called
 statsd, so we use an odd name for this handler. This doesn't affect the usage
 of this handler.
 
 """
 
-from Handler import Handler
 import logging
+
+from diamond.handler.Handler import Handler
+
 try:
     import statsd
 except ImportError:
     statsd = None
 
 
 class StatsdHandler(Handler):
-
     def __init__(self, config=None):
         """
         Create a new instance of the StatsdHandler class
         """
+
         # Initialize Handler
         Handler.__init__(self, config)
         logging.debug("Initialized statsd handler.")
 
         if not statsd:
-            self.log.error('statsd import failed. Handler disabled')
+            self.log.error("statsd import failed. Handler disabled")
             self.enabled = False
+
             return
 
-        if not hasattr(statsd, 'StatsClient'):
-            self.log.warn('python-statsd support is deprecated '
-                          'and will be removed in the future. '
-                          'Please use https://pypi.python.org/pypi/statsd/')
+        if not hasattr(statsd, "StatsClient"):
+            self.log.warn(
+                "python-statsd support is deprecated and will be removed in the future. Please use https://pypi.python.org/pypi/statsd/"
+            )
 
         # Initialize Options
-        self.host = self.config['host']
-        self.port = int(self.config['port'])
-        self.batch_size = int(self.config['batch'])
+        self.host = self.config["host"]
+        self.port = int(self.config["port"])
+        self.batch_size = int(self.config["batch"])
         self.metrics = []
         self.old_values = {}
 
         # Connect
         self._connect()
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(StatsdHandler, self).get_default_config_help()
 
-        config.update({
-            'host': '',
-            'port': '',
-            'batch': '',
-        })
+        config.update(
+            {
+                "host": "",
+                "port": "",
+                "batch": "",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(StatsdHandler, self).get_default_config()
 
-        config.update({
-            'host': '',
-            'port': 1234,
-            'batch': 1,
-        })
+        config.update(
+            {
+                "host": "",
+                "port": 1234,
+                "batch": 1,
+            }
+        )
 
         return config
 
     def process(self, metric):
         """
         Process a metric by sending it to statsd
         """
@@ -105,65 +112,63 @@
 
     def _send(self):
         """
         Send data to statsd. Fire and forget.  Cross fingers and it'll arrive.
         """
         if not statsd:
             return
-        for metric in self.metrics:
 
+        for metric in self.metrics:
             # Split the path into a prefix and a name
             # to work with the statsd module's view of the world.
             # It will get re-joined by the python-statsd module.
             #
             # For the statsd module, you specify prefix in the constructor
             # so we just use the full metric path.
             (prefix, name) = metric.path.rsplit(".", 1)
             logging.debug("Sending %s %s|g", name, metric.value)
 
-            if metric.metric_type == 'GAUGE':
-                if hasattr(statsd, 'StatsClient'):
+            if metric.metric_type == "GAUGE":
+                if hasattr(statsd, "StatsClient"):
                     self.connection.gauge(metric.path, metric.value)
                 else:
-                    statsd.Gauge(prefix, self.connection).send(
-                        name, metric.value)
+                    statsd.Gauge(prefix, self.connection).send(name, metric.value)
             else:
                 # To send a counter, we need to just send the delta
                 # but without any time delta changes
                 value = metric.raw_value
+
                 if metric.path in self.old_values:
                     value = value - self.old_values[metric.path]
+
                 self.old_values[metric.path] = metric.raw_value
 
-                if hasattr(statsd, 'StatsClient'):
+                if hasattr(statsd, "StatsClient"):
                     self.connection.incr(metric.path, value)
                 else:
-                    statsd.Counter(prefix, self.connection).increment(
-                        name, value)
+                    statsd.Counter(prefix, self.connection).increment(name, value)
 
-        if hasattr(statsd, 'StatsClient'):
+        if hasattr(statsd, "StatsClient"):
             self.connection.send()
+
         self.metrics = []
 
     def flush(self):
         """Flush metrics in queue"""
         self._send()
 
     def _connect(self):
         """
         Connect to the statsd server
         """
         if not statsd:
             return
 
-        if hasattr(statsd, 'StatsClient'):
+        if hasattr(statsd, "StatsClient"):
             self.connection = statsd.StatsClient(
-                host=self.host,
-                port=self.port
+                host=self.host, port=self.port
             ).pipeline()
         else:
             # Create socket
             self.connection = statsd.Connection(
-                host=self.host,
-                port=self.port,
-                sample_rate=1.0
+                host=self.host, port=self.port, sample_rate=1.0
             )
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/sentry.py` & `diamond-next-5.0.0/src/diamond/handler/sentry.py`

 * *Files 9% similar despite different names*

```diff
@@ -27,107 +27,114 @@
 path = memory.MemFree
 min = 66020000
 """
 
 import logging
 import re
 
-from Handler import Handler
-from diamond.collector import get_hostname
 from configobj import Section
 
+from diamond.collector import get_hostname
+from diamond.handler.Handler import Handler
+
 try:
     import raven.handlers.logging
 except ImportError:
     raven = None
 
-__author__ = 'Bruno Clermont'
-__email__ = 'bruno.clermont@gmail.com'
+__author__ = "Bruno Clermont"
+__email__ = "bruno.clermont@gmail.com"
 
 
 class InvalidRule(ValueError):
     """
     invalid rule
     """
+
     pass
 
 
 class BaseResult(object):
     """
     Base class for a Rule minimum/maximum check result
     """
+
     adjective = None
 
     def __init__(self, value, threshold):
         """
         @type value: float
         @param value: metric value
         @type threshold: float
         @param threshold: value that trigger a warning
         """
         self.value = value
         self.threshold = threshold
 
         if not raven:
-            self.log.error('raven.handlers.logging import failed. '
-                           'Handler disabled')
+            self.log.error("raven.handlers.logging import failed. Handler disabled")
             self.enabled = False
             return
 
     @property
     def verbose_message(self):
         """return more complete message"""
         if self.threshold is None:
-            return 'No threshold'
-        return '%.1f is %s than %.1f' % (self.value,
-                                         self.adjective,
-                                         self.threshold)
+            return "No threshold"
+
+        return "%.1f is %s than %.1f" % (self.value, self.adjective, self.threshold)
 
     @property
     def _is_error(self):
-        raise NotImplementedError('_is_error')
+        raise NotImplementedError("_is_error")
 
     @property
     def is_error(self):
         """
         for some reason python do this:
         >>> 1.0 > None
         True
         >>> 1.0 < None
         False
         so we just check if min/max is not None before return _is_error
         """
+
         if self.threshold is None:
             return False
+
         return self._is_error
 
     def __str__(self):
         name = self.__class__.__name__.lower()
+
         if self.threshold is None:
-            return '%s: %.1f no threshold' % (name, self.value)
-        return '%.1f (%s: %.1f)' % (self.value, name, self.threshold)
+            return "%s: %.1f no threshold" % (name, self.value)
+
+        return "%.1f (%s: %.1f)" % (self.value, name, self.threshold)
 
 
 class Minimum(BaseResult):
     """
     Minimum result
     """
-    adjective = 'lower'
+
+    adjective = "lower"
 
     @property
     def _is_error(self):
         """if it's too low"""
         return self.value < self.threshold
 
 
 class Maximum(BaseResult):
     """
     Maximum result
     """
-    adjective = 'higher'
+
+    adjective = "higher"
 
     @property
     def _is_error(self):
         """if it's too high"""
         return self.value > self.threshold
 
 
@@ -146,149 +153,173 @@
         @param min: optional minimal value that if value goes below it send
             an alert to Sentry
         @type max: string of float/int, int or float. will be convert to float
         @param max: optional maximal value that if value goes over it send
             an alert to Sentry
         """
         self.name = name
+
         # counters that can be used to debug rule
         self.counter_errors = 0
         self.counter_pass = 0
 
         # force min and max to be float
         try:
             self.min = float(min)
         except TypeError:
             self.min = None
+
         try:
             self.max = float(max)
         except TypeError:
             self.max = None
 
         if self.min is None and self.max is None:
-            raise InvalidRule("%s: %s: both min and max are unset or invalid"
-                              % (name, path))
+            raise InvalidRule(
+                "%s: %s: both min and max are unset or invalid" % (name, path)
+            )
 
         if self.min is not None and self.max is not None:
             if self.min > self.max:
-                raise InvalidRule("min %.1f is larger than max %.1f" % (
-                    self.min, self.max))
+                raise InvalidRule(
+                    "min %.1f is larger than max %.1f" % (self.min, self.max)
+                )
 
         # compile path regular expression
-        self.regexp = re.compile(r'(?P<prefix>.*)\.(?P<path>%s)$' % path)
+        self.regexp = re.compile(r"(?P<prefix>.*)\.(?P<path>%s)$" % path)
 
     def process(self, metric, handler):
         """
         process a single diamond metric
         @type metric: diamond.metric.Metric
         @param metric: metric to process
         @type handler: diamond.handler.sentry.SentryHandler
         @param handler: configured Sentry graphite handler
         @rtype None
         """
         match = self.regexp.match(metric.path)
+
         if match:
             minimum = Minimum(metric.value, self.min)
             maximum = Maximum(metric.value, self.max)
 
             if minimum.is_error or maximum.is_error:
                 self.counter_errors += 1
-                message = "%s Warning on %s: %.1f" % (self.name,
-                                                      handler.hostname,
-                                                      metric.value)
-                culprit = "%s %s" % (handler.hostname, match.group('path'))
-                handler.raven_logger.error(message, extra={
-                    'culprit': culprit,
-                    'data': {
-                        'metric prefix': match.group('prefix'),
-                        'metric path': match.group('path'),
-                        'minimum check': minimum.verbose_message,
-                        'maximum check': maximum.verbose_message,
-                        'metric original path': metric.path,
-                        'metric value': metric.value,
-                        'metric precision': metric.precision,
-                        'metric timestamp': metric.timestamp,
-                        'minimum threshold': self.min,
-                        'maximum threshold': self.max,
-                        'path regular expression': self.regexp.pattern,
-                        'total errors': self.counter_errors,
-                        'total pass': self.counter_pass,
-                        'hostname': handler.hostname
-                    }
-                }
+                message = "%s Warning on %s: %.1f" % (
+                    self.name,
+                    handler.hostname,
+                    metric.value,
+                )
+                culprit = "%s %s" % (handler.hostname, match.group("path"))
+                handler.raven_logger.error(
+                    message,
+                    extra={
+                        "culprit": culprit,
+                        "data": {
+                            "metric prefix": match.group("prefix"),
+                            "metric path": match.group("path"),
+                            "minimum check": minimum.verbose_message,
+                            "maximum check": maximum.verbose_message,
+                            "metric original path": metric.path,
+                            "metric value": metric.value,
+                            "metric precision": metric.precision,
+                            "metric timestamp": metric.timestamp,
+                            "minimum threshold": self.min,
+                            "maximum threshold": self.max,
+                            "path regular expression": self.regexp.pattern,
+                            "total errors": self.counter_errors,
+                            "total pass": self.counter_pass,
+                            "hostname": handler.hostname,
+                        },
+                    },
                 )
             else:
                 self.counter_pass += 1
 
     def __repr__(self):
-        return '%s: min:%s max:%s %s' % (self.name, self.min, self.max,
-                                         self.regexp.pattern)
+        return "%s: min:%s max:%s %s" % (
+            self.name,
+            self.min,
+            self.max,
+            self.regexp.pattern,
+        )
 
 
 class SentryHandler(Handler):
     """
     Diamond handler that check if a metric goes too low or too high
     """
 
     # valid key name in rules sub-section
-    VALID_RULES_KEYS = ('name', 'path', 'min', 'max')
+    VALID_RULES_KEYS = ("name", "path", "min", "max")
 
     def __init__(self, config=None):
         """
         @type config: configobj.ConfigObj
         """
         Handler.__init__(self, config)
+
         if not raven:
             return
+
         # init sentry/raven
         self.sentry_log_handler = raven.handlers.logging.SentryHandler(
-            self.config['dsn'])
+            self.config["dsn"]
+        )
         self.raven_logger = logging.getLogger(self.__class__.__name__)
         self.raven_logger.addHandler(self.sentry_log_handler)
         self.configure_sentry_errors()
         self.rules = self.compile_rules()
         self.hostname = get_hostname(self.config)
+
         if not len(self.rules):
             self.log.warning("No rules, this graphite handler is unused")
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(SentryHandler, self).get_default_config_help()
 
-        config.update({
-            'dsn': '',
-        })
+        config.update(
+            {
+                "dsn": "",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(SentryHandler, self).get_default_config()
 
-        config.update({
-            'dsn': '',
-        })
+        config.update(
+            {
+                "dsn": "",
+            }
+        )
 
         return config
 
     def compile_rules(self):
         """
         Compile alert rules
         @rtype list of Rules
         """
         output = []
+
         # validate configuration, skip invalid section
         for key_name, section in self.config.items():
             rule = self.compile_section(section)
+
             if rule is not None:
                 output.append(rule)
+
         return output
 
     def compile_section(self, section):
         """
         Validate if a section is a valid rule
         @type section: configobj.Section
         @param section: section to validate
@@ -297,56 +328,53 @@
         """
         if section.__class__ != Section:
             # not a section, just skip
             return
 
         # name and path are mandatory
         keys = section.keys()
-        for key in ('name', 'path'):
+
+        for key in ("name", "path"):
             if key not in keys:
-                self.log.warning("section %s miss key '%s' ignore", key,
-                                 section.name)
+                self.log.warning("section %s miss key '%s' ignore", key, section.name)
                 return
 
         # just warn if invalid key in section
         for key in keys:
             if key not in self.VALID_RULES_KEYS:
-                self.log.warning("invalid key %s in section %s",
-                                 key, section.name)
+                self.log.warning("invalid key %s in section %s", key, section.name)
 
         # need at least a min or a max
-        if 'min' not in keys and 'max' not in keys:
-            self.log.warning("either 'min' or 'max' is defined in %s",
-                             section.name)
+        if "min" not in keys and "max" not in keys:
+            self.log.warning("either 'min' or 'max' is defined in %s", section.name)
             return
 
         # add rule to the list
-        kwargs = {
-            'name': section['name'],
-            'path': section['path']
-        }
-        for argument in ('min', 'max'):
+        kwargs = {"name": section["name"], "path": section["path"]}
+
+        for argument in ("min", "max"):
             try:
                 kwargs[argument] = section[argument]
             except KeyError:
                 pass
 
         # init rule
         try:
             return Rule(**kwargs)
-        except InvalidRule, err:
+        except InvalidRule as err:
             self.log.error(str(err))
 
     def configure_sentry_errors(self):
         """
         Configure sentry.errors to use the same loggers as the root handler
         @rtype: None
         """
-        sentry_errors_logger = logging.getLogger('sentry.errors')
+        sentry_errors_logger = logging.getLogger("sentry.errors")
         root_logger = logging.getLogger()
+
         for handler in root_logger.handlers:
             sentry_errors_logger.addHandler(handler)
 
     def process(self, metric):
         """
         process a single metric
         @type metric: diamond.metric.Metric
@@ -354,8 +382,10 @@
         @rtype None
         """
         for rule in self.rules:
             rule.process(metric, self)
 
     def __repr__(self):
         return "SentryHandler '%s' %d rules" % (
-            self.sentry_log_handler.client.servers, len(self.rules))
+            self.sentry_log_handler.client.servers,
+            len(self.rules),
+        )
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/zmq_pubsub.py` & `diamond-next-5.0.0/src/diamond/handler/zmq_pubsub.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,92 +1,98 @@
 # coding=utf-8
 
 """
 Output the collected values to a Zer0MQ pub/sub channel
 """
 
-from Handler import Handler
+from diamond.handler.Handler import Handler
 
 try:
     import zmq
 except ImportError:
     zmq = None
 
 
-class zmqHandler (Handler):
+class zmqHandler(Handler):
     """
-      Implements the abstract Handler class
-      Sending data to a Zer0MQ pub channel
+    Implements the abstract Handler class
+    Sending data to a Zer0MQ pub channel
     """
 
     def __init__(self, config=None):
         """
-          Create a new instance of zmqHandler class
+        Create a new instance of zmqHandler class
         """
 
         # Initialize Handler
         Handler.__init__(self, config)
 
         if not zmq:
-            self.log.error('zmq import failed. Handler disabled')
+            self.log.error("zmq import failed. Handler disabled")
             self.enabled = False
             return
 
         # Initialize Data
         self.context = None
 
         self.socket = None
 
         # Initialize Options
-        self.port = int(self.config['port'])
+        self.port = int(self.config["port"])
 
         # Create ZMQ pub socket and bind
         self._bind()
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(zmqHandler, self).get_default_config_help()
 
-        config.update({
-            'port': '',
-        })
+        config.update(
+            {
+                "port": "",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(zmqHandler, self).get_default_config()
 
-        config.update({
-            'port': 1234,
-        })
+        config.update(
+            {
+                "port": 1234,
+            }
+        )
 
         return config
 
     def _bind(self):
         """
-           Create PUB socket and bind
+        Create PUB socket and bind
         """
         if not zmq:
             return
+
         self.context = zmq.Context()
         self.socket = self.context.socket(zmq.PUB)
         self.socket.bind("tcp://*:%i" % self.port)
 
     def __del__(self):
         """
-          Destroy instance of the zmqHandler class
+        Destroy instance of the zmqHandler class
         """
         pass
 
     def process(self, metric):
         """
-          Process a metric and send it to zmq pub socket
+        Process a metric and send it to zmq pub socket
         """
         if not zmq:
             return
+
         # Send the data as ......
         self.socket.send("%s" % str(metric))
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/influxdbHandler.py` & `diamond-next-5.0.0/src/diamond/handler/influxdbHandler.py`

 * *Files 18% similar despite different names*

```diff
@@ -25,15 +25,16 @@
 password = root
 database = graphite
 time_precision = s
 ```
 """
 
 import time
-from Handler import Handler
+
+from diamond.handler.Handler import Handler
 
 try:
     from influxdb.client import InfluxDBClient
 except ImportError:
     InfluxDBClient = None
 
 
@@ -42,37 +43,40 @@
     Sending data to Influxdb using batched format
     """
 
     def __init__(self, config=None):
         """
         Create a new instance of the InfluxdbeHandler
         """
+
         # Initialize Handler
         Handler.__init__(self, config)
 
         if not InfluxDBClient:
-            self.log.error('influxdb.client.InfluxDBClient import failed. '
-                           'Handler disabled')
+            self.log.error(
+                "influxdb.client.InfluxDBClient import failed. Handler disabled"
+            )
             self.enabled = False
             return
 
         # Initialize Options
-        if self.config['ssl'] == "True":
+        if self.config["ssl"] == "True":
             self.ssl = True
         else:
             self.ssl = False
-        self.hostname = self.config['hostname']
-        self.port = int(self.config['port'])
-        self.username = self.config['username']
-        self.password = self.config['password']
-        self.database = self.config['database']
-        self.batch_size = int(self.config['batch_size'])
-        self.metric_max_cache = int(self.config['cache_size'])
+
+        self.hostname = self.config["hostname"]
+        self.port = int(self.config["port"])
+        self.username = self.config["username"]
+        self.password = self.config["password"]
+        self.database = self.config["database"]
+        self.batch_size = int(self.config["batch_size"])
+        self.metric_max_cache = int(self.config["cache_size"])
         self.batch_count = 0
-        self.time_precision = self.config['time_precision']
+        self.time_precision = self.config["time_precision"]
 
         # Initialize Data
         self.batch = {}
         self.influx = None
         self.batch_timestamp = time.time()
         self.time_multiplier = 1
 
@@ -81,143 +85,179 @@
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(InfluxdbHandler, self).get_default_config_help()
 
-        config.update({
-            'hostname': 'Hostname',
-            'port': 'Port',
-            'ssl': 'set to True to use HTTPS instead of http',
-            'batch_size': 'How many metrics to store before sending to the'
-            ' influxdb server',
-            'cache_size': 'How many values to store in cache in case of'
-            ' influxdb failure',
-            'username': 'Username for connection',
-            'password': 'Password for connection',
-            'database': 'Database name',
-            'time_precision': 'time precision in second(s), milisecond(ms) or '
-            'microsecond (u)',
-        })
+        config.update(
+            {
+                "hostname": "Hostname",
+                "port": "Port",
+                "ssl": "set to True to use HTTPS instead of http",
+                "batch_size": "How many metrics to store before sending to the influxdb server",
+                "cache_size": "How many values to store in cache in case of influxdb failure",
+                "username": "Username for connection",
+                "password": "Password for connection",
+                "database": "Database name",
+                "time_precision": "time precision in second(s), milisecond(ms) or "
+                "microsecond (u)",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(InfluxdbHandler, self).get_default_config()
 
-        config.update({
-            'hostname': 'localhost',
-            'port': 8086,
-            'ssl': False,
-            'username': 'root',
-            'password': 'root',
-            'database': 'graphite',
-            'batch_size': 1,
-            'cache_size': 20000,
-            'time_precision': 's',
-        })
+        config.update(
+            {
+                "hostname": "localhost",
+                "port": 8086,
+                "ssl": False,
+                "username": "root",
+                "password": "root",
+                "database": "graphite",
+                "batch_size": 1,
+                "cache_size": 20000,
+                "time_precision": "s",
+            }
+        )
 
         return config
 
     def __del__(self):
         """
         Destroy instance of the InfluxdbHandler class
         """
         self._close()
 
     def process(self, metric):
         if self.batch_count <= self.metric_max_cache:
             # Add the data to the batch
-            self.batch.setdefault(metric.path, []).append([metric.timestamp,
-                                                           metric.value])
+            self.batch.setdefault(metric.path, []).append(
+                [metric.timestamp, metric.value]
+            )
             self.batch_count += 1
+
         # If there are sufficient metrics, then pickle and send
-        if self.batch_count >= self.batch_size and (
-                time.time() - self.batch_timestamp) > 2**self.time_multiplier:
+        if (
+            self.batch_count >= self.batch_size
+            and (time.time() - self.batch_timestamp) > 2**self.time_multiplier
+        ):
             # Log
             self.log.debug(
                 "InfluxdbHandler: Sending batch sizeof : %d/%d after %fs",
                 self.batch_count,
                 self.batch_size,
-                (time.time() - self.batch_timestamp))
+                (time.time() - self.batch_timestamp),
+            )
+
             # reset the batch timer
             self.batch_timestamp = time.time()
+
             # Send pickled batch
             self._send()
         else:
             self.log.debug(
                 "InfluxdbHandler: not sending batch of %d as timestamp is %f",
                 self.batch_count,
-                (time.time() - self.batch_timestamp))
+                (time.time() - self.batch_timestamp),
+            )
 
     def _send(self):
         """
         Send data to Influxdb. Data that can not be sent will be kept in queued.
         """
+
         # Check to see if we have a valid socket. If not, try to connect.
         try:
             if self.influx is None:
-                self.log.debug("InfluxdbHandler: Socket is not connected. "
-                               "Reconnecting.")
+                self.log.debug(
+                    "InfluxdbHandler: Socket is not connected. Reconnecting."
+                )
                 self._connect()
             if self.influx is None:
                 self.log.debug("InfluxdbHandler: Reconnect failed.")
             else:
                 # build metrics data
                 metrics = []
+
                 for path in self.batch:
-                    metrics.append({
-                        "points": self.batch[path],
-                        "name": path,
-                        "columns": ["time", "value"]})
+                    metrics.append(
+                        {
+                            "points": self.batch[path],
+                            "name": path,
+                            "columns": ["time", "value"],
+                        }
+                    )
+
                 # Send data to influxdb
-                self.log.debug("InfluxdbHandler: writing %d series of data",
-                               len(metrics))
-                self.influx.write_points(metrics,
-                                         time_precision=self.time_precision)
+                self.log.debug(
+                    "InfluxdbHandler: writing %d series of data", len(metrics)
+                )
+                self.influx.write_points(metrics, time_precision=self.time_precision)
 
                 # empty batch buffer
                 self.batch = {}
                 self.batch_count = 0
                 self.time_multiplier = 1
 
         except Exception:
             self._close()
+
             if self.time_multiplier < 5:
                 self.time_multiplier += 1
+
             self._throttle_error(
                 "InfluxdbHandler: Error sending metrics, waiting for %ds.",
-                2**self.time_multiplier)
+                2**self.time_multiplier,
+            )
+
             raise
 
     def _connect(self):
         """
         Connect to the influxdb server
         """
 
         try:
             # Open Connection
-            self.influx = InfluxDBClient(self.hostname, self.port,
-                                         self.username, self.password,
-                                         self.database, self.ssl)
+            self.influx = InfluxDBClient(
+                self.hostname,
+                self.port,
+                self.username,
+                self.password,
+                self.database,
+                self.ssl,
+            )
+
             # Log
-            self.log.debug("InfluxdbHandler: Established connection to "
-                           "%s:%d/%s.",
-                           self.hostname, self.port, self.database)
-        except Exception, ex:
+            self.log.debug(
+                "InfluxdbHandler: Established connection to %s:%d/%s.",
+                self.hostname,
+                self.port,
+                self.database,
+            )
+        except Exception as ex:
             # Log Error
-            self._throttle_error("InfluxdbHandler: Failed to connect to "
-                                 "%s:%d/%s. %s",
-                                 self.hostname, self.port, self.database, ex)
+            self._throttle_error(
+                "InfluxdbHandler: Failed to connect to %s:%d/%s. %s",
+                self.hostname,
+                self.port,
+                self.database,
+                ex,
+            )
+
             # Close Socket
             self._close()
+
             return
 
     def _close(self):
         """
         Close the socket = do nothing for influx which is http stateless
         """
         self.influx = None
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/rrdtool.py` & `diamond-next-5.0.0/src/diamond/handler/rrdtool.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,28 +3,28 @@
 """
 Save stats in RRD files using rrdtool.
 """
 
 import os
 import re
 import subprocess
-import Queue
+from queue import Empty, Queue
 
-from Handler import Handler
+from diamond.handler.Handler import Handler
 
 #
 # Constants for RRD file creation.
 #
 
 # NOTE: We default to the collectd RRD directory
 # simply as a compatibility tool. Users that have
 # tools that look in that location and would like
 # to switch to Diamond need to make zero changes.
 
-BASEDIR = '/var/lib/collectd/rrd'
+BASEDIR = "/var/lib/collectd/rrd"
 
 METRIC_STEP = 10
 
 BATCH_SIZE = 1
 
 # NOTE: We don't really have a rigorous defition
 # for metrics, particularly how often they will be
@@ -54,51 +54,54 @@
     "RRA:AVERAGE:0.1:2635:1200",
     "RRA:MIN:0.1:2635:1200",
     "RRA:MAX:0.1:2635:1200",
 ]
 
 
 class RRDHandler(Handler):
-
     # NOTE: This handler is fairly loose about locking (none),
     # and the reason is because the calls are always protected
     # by locking done in the _process and _flush routines.
     # If this were to change at some point, we would definitely
     # want to be a bit more sensible about how we lock.
     #
     # We would probably also want to restructure this as a
     # consumer and producer so that one thread can continually
     # write out data, but that really depends on the design
     # at the top level.
 
     def __init__(self, *args, **kwargs):
         super(RRDHandler, self).__init__(*args, **kwargs)
         self._exists_cache = dict()
-        self._basedir = self.config['basedir']
-        self._batch = self.config['batch']
-        self._step = self.config['step']
+        self._basedir = self.config["basedir"]
+        self._batch = self.config["batch"]
+        self._step = self.config["step"]
         self._queues = {}
         self._last_update = {}
 
     def get_default_config_help(self):
         config = super(RRDHandler, self).get_default_config_help()
-        config.update({
-            'basedir': 'The base directory for all RRD files.',
-            'batch': 'Wait for this many updates before saving to the RRD file',
-            'step': 'The minimum interval represented in generated RRD files.',
-        })
+        config.update(
+            {
+                "basedir": "The base directory for all RRD files.",
+                "batch": "Wait for this many updates before saving to the RRD file",
+                "step": "The minimum interval represented in generated RRD files.",
+            }
+        )
         return config
 
     def get_default_config(self):
         config = super(RRDHandler, self).get_default_config()
-        config.update({
-            'basedir': BASEDIR,
-            'batch': BATCH_SIZE,
-            'step': METRIC_STEP,
-        })
+        config.update(
+            {
+                "basedir": BASEDIR,
+                "batch": BATCH_SIZE,
+                "step": METRIC_STEP,
+            }
+        )
         return config
 
     def _ensure_exists(self, filename, metric_name, metric_type):
         # We're good to go!
         if filename in self._exists_cache:
             return True
 
@@ -125,21 +128,23 @@
         # NOTE: If we aren't successful, the check_call()
         # will fail anyways so we can do this optimistically.
         try:
             os.makedirs(os.path.dirname(filename))
         except OSError:
             pass
 
-        ds_spec = "DS:%s:%s:%d:U:U" % (
-            metric_name, metric_type, self._step * 2)
+        ds_spec = "DS:%s:%s:%d:U:U" % (metric_name, metric_type, self._step * 2)
         rrd_create_cmd = [
-            "rrdtool", "create", filename,
+            "rrdtool",
+            "create",
+            filename,
             "--no-overwrite",
-            "--step", str(self._step),
-            ds_spec
+            "--step",
+            str(self._step),
+            ds_spec,
         ]
         rrd_create_cmd.extend(RRA_SPECS)
         subprocess.check_call(rrd_create_cmd, close_fds=True)
 
     def process(self, metric):
         # Extract the filename given the metric.
         # NOTE: We have to tweak the metric name and limit
@@ -151,73 +156,80 @@
         filename = os.path.join(dirname, metric_name + ".rrd")
 
         # Ensure that there is an RRD file for this metric.
         # This is done inline because it's quickly cached and
         # we would like to have exceptions related to creating
         # the RRD file raised in the main thread.
         self._ensure_exists(filename, metric_name, metric.metric_type)
+
         if self._queue(filename, metric.timestamp, metric.value) >= self._batch:
             self._flush_queue(filename)
 
     def _queue(self, filename, timestamp, value):
         if filename not in self._queues:
-            queue = Queue.Queue()
+            queue = Queue()
             self._queues[filename] = queue
         else:
             queue = self._queues[filename]
+
         queue.put((timestamp, value))
+
         return queue.qsize()
 
     def flush(self):
         # Grab all current queues.
         for filename in self._queues.keys():
             self._flush_queue(filename)
 
     def _flush_queue(self, filename):
         queue = self._queues[filename]
 
         # Collect all pending updates.
         updates = {}
         max_timestamp = 0
+
         while True:
             try:
                 (timestamp, value) = queue.get(block=False)
                 # RRD only supports granularity at a
                 # per-second level (not milliseconds, etc.).
                 timestamp = int(timestamp)
 
                 # Remember the latest update done.
                 last_update = self._last_update.get(filename, 0)
+
                 if last_update >= timestamp:
                     # Yikes. RRDtool won't let us do this.
                     # We need to drop this update and log a warning.
-                    self.log.warning(
-                        "Dropping update to %s. Too frequent!" % filename)
+                    self.log.warning("Dropping update to %s. Too frequent!" % filename)
                     continue
+
                 max_timestamp = max(timestamp, max_timestamp)
 
                 # Add this update.
                 if timestamp not in updates:
                     updates[timestamp] = []
+
                 updates[timestamp].append(value)
-            except Queue.Empty:
+            except Empty:
                 break
 
         # Save the last update time.
         self._last_update[filename] = max_timestamp
 
         if len(updates) > 0:
             # Construct our command line.
             # This will look like <time>:<value1>[:<value2>...]
             # The timestamps must be sorted, and we each of the
             # <time> values must be unique (like a snowflake).
             data_points = map(
-                lambda (timestamp, values): "%d:%s" %
-                (timestamp, ":".join(map(str, values))),
-                sorted(updates.items()))
+                lambda timestamp_values: "%d:%s"
+                % (timestamp_values[0], ":".join(map(str, timestamp_values[1]))),
+                sorted(updates.items()),
+            )
 
             # Optimisticly update.
             # Nothing can really be done if we fail.
             rrd_update_cmd = ["rrdupdate", filename, "--"]
             rrd_update_cmd.extend(data_points)
             self.log.info("update: %s" % str(rrd_update_cmd))
             subprocess.call(rrd_update_cmd)
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/Handler.py` & `diamond-next-5.0.0/src/diamond/handler/Handler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # coding=utf-8
 
 import logging
 import threading
+import time
 import traceback
+
 from configobj import ConfigObj
-import time
 
 
 class Handler(object):
     """
     Handlers process metrics that are collected by Collectors.
     """
 
@@ -18,52 +19,50 @@
         """
 
         # Enabled? Default to yes, but allow handlers to disable themselves
         self.enabled = True
 
         # Initialize Log
         if log is None:
-            self.log = logging.getLogger('diamond')
+            self.log = logging.getLogger("diamond")
         else:
             self.log = log
 
         # Initialize Blank Configs
         self.config = ConfigObj()
 
         # Load default
         self.config.merge(self.get_default_config())
 
         # Load in user
         self.config.merge(config)
 
         # error logging throttling
-        self.server_error_interval = float(
-            self.config['server_error_interval'])
+        self.server_error_interval = float(self.config["server_error_interval"])
         self._errors = {}
 
         # Initialize Lock
         self.lock = threading.Lock()
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         return {
-            'get_default_config_help': 'get_default_config_help',
-            'server_error_interval': ('How frequently to send repeated server '
-                                      'errors'),
+            "get_default_config_help": "get_default_config_help",
+            "server_error_interval": "How frequently to send repeated server errors",
         }
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         return {
-            'get_default_config': 'get_default_config',
-            'server_error_interval': 120,
+            "get_default_config": "get_default_config",
+            "server_error_interval": 120,
         }
 
     def _process(self, metric):
         """
         Decorator for processing handlers with a lock, catching exceptions
         """
         if not self.enabled:
@@ -88,14 +87,15 @@
 
     def _flush(self):
         """
         Decorator for flushing handlers with an lock, catching exceptions
         """
         if not self.enabled:
             return
+
         try:
             try:
                 self.lock.acquire()
                 self.flush()
             except Exception:
                 self.log.error(traceback.format_exc())
         finally:
@@ -120,17 +120,17 @@
         Receives the same parameters as `Logger.error` an passes them on to the
         selected logging function, but ignores all parameters but the main
         message string when checking the last emission time.
 
         :returns: the return value of `Logger.debug` or `Logger.error`
         """
         now = time.time()
+
         if msg in self._errors:
-            if ((now - self._errors[msg]) >=
-                    self.server_error_interval):
+            if (now - self._errors[msg]) >= self.server_error_interval:
                 fn = self.log.error
                 self._errors[msg] = now
             else:
                 fn = self.log.debug
         else:
             self._errors[msg] = now
             fn = self.log.error
```

### Comparing `diamond-next-4.0.515/src/diamond/handler/graphite.py` & `diamond-next-5.0.0/src/diamond/handler/graphite.py`

 * *Files 15% similar despite different names*

```diff
@@ -12,99 +12,102 @@
 deployed it at other companies including Sears, where it serves as a pillar of
 the e-commerce monitoring system. Today many
 [large companies](http://graphite.readthedocs.org/en/latest/who-is-using.html)
 use it.
 
 """
 
-from Handler import Handler
 import socket
 import time
 
+from diamond.handler.Handler import Handler
+
 
 class GraphiteHandler(Handler):
     """
     Implements the abstract Handler class, sending data to graphite
     """
 
     def __init__(self, config=None):
         """
         Create a new instance of the GraphiteHandler class
         """
+
         # Initialize Handler
         Handler.__init__(self, config)
 
         # Initialize Data
         self.socket = None
 
         # Initialize Options
-        self.proto = self.config['proto'].lower().strip()
-        self.host = self.config['host']
-        self.port = int(self.config['port'])
-        self.timeout = float(self.config['timeout'])
-        self.keepalive = bool(self.config['keepalive'])
-        self.keepaliveinterval = int(self.config['keepaliveinterval'])
-        self.batch_size = int(self.config['batch'])
-        self.max_backlog_multiplier = int(
-            self.config['max_backlog_multiplier'])
-        self.trim_backlog_multiplier = int(
-            self.config['trim_backlog_multiplier'])
-        self.flow_info = self.config['flow_info']
-        self.scope_id = self.config['scope_id']
+        self.proto = self.config["proto"].lower().strip()
+        self.host = self.config["host"]
+        self.port = int(self.config["port"])
+        self.timeout = float(self.config["timeout"])
+        self.keepalive = bool(self.config["keepalive"])
+        self.keepaliveinterval = int(self.config["keepaliveinterval"])
+        self.batch_size = int(self.config["batch"])
+        self.max_backlog_multiplier = int(self.config["max_backlog_multiplier"])
+        self.trim_backlog_multiplier = int(self.config["trim_backlog_multiplier"])
+        self.flow_info = self.config["flow_info"]
+        self.scope_id = self.config["scope_id"]
         self.metrics = []
-        self.reconnect_interval = int(self.config['reconnect_interval'])
+        self.reconnect_interval = int(self.config["reconnect_interval"])
         self.last_connect_timestamp = -1
 
         # Connect
         self._connect()
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this handler
         """
         config = super(GraphiteHandler, self).get_default_config_help()
 
-        config.update({
-            'host': 'Hostname',
-            'port': 'Port',
-            'proto': 'udp, udp4, udp6, tcp, tcp4, or tcp6',
-            'timeout': '',
-            'batch': 'How many to store before sending to the graphite server',
-            'max_backlog_multiplier': 'how many batches to store before trimming',  # NOQA
-            'trim_backlog_multiplier': 'Trim down how many batches',
-            'keepalive': 'Enable keepalives for tcp streams',
-            'keepaliveinterval': 'How frequently to send keepalives',
-            'flow_info': 'IPv6 Flow Info',
-            'scope_id': 'IPv6 Scope ID',
-            'reconnect_interval': 'How often (seconds) to reconnect to '
-                                  'graphite. Default (0) is never',
-        })
+        config.update(
+            {
+                "host": "Hostname",
+                "port": "Port",
+                "proto": "udp, udp4, udp6, tcp, tcp4, or tcp6",
+                "timeout": "",
+                "batch": "How many to store before sending to the graphite server",
+                "max_backlog_multiplier": "how many batches to store before trimming",  # NOQA
+                "trim_backlog_multiplier": "Trim down how many batches",
+                "keepalive": "Enable keepalives for tcp streams",
+                "keepaliveinterval": "How frequently to send keepalives",
+                "flow_info": "IPv6 Flow Info",
+                "scope_id": "IPv6 Scope ID",
+                "reconnect_interval": "How often (seconds) to reconnect to graphite. Default (0) is never",
+            }
+        )
 
         return config
 
     def get_default_config(self):
         """
         Return the default config for the handler
         """
         config = super(GraphiteHandler, self).get_default_config()
 
-        config.update({
-            'host': 'localhost',
-            'port': 2003,
-            'proto': 'tcp',
-            'timeout': 15,
-            'batch': 1,
-            'max_backlog_multiplier': 5,
-            'trim_backlog_multiplier': 4,
-            'keepalive': 0,
-            'keepaliveinterval': 10,
-            'flow_info': 0,
-            'scope_id': 0,
-            'reconnect_interval': 0,
-        })
+        config.update(
+            {
+                "host": "localhost",
+                "port": 2003,
+                "proto": "tcp",
+                "timeout": 15,
+                "batch": 1,
+                "max_backlog_multiplier": 5,
+                "trim_backlog_multiplier": 4,
+                "keepalive": 0,
+                "keepaliveinterval": 10,
+                "flow_info": 0,
+                "scope_id": 0,
+                "reconnect_interval": 0,
+            }
+        )
 
         return config
 
     def __del__(self):
         """
         Destroy instance of the GraphiteHandler class
         """
@@ -112,148 +115,178 @@
 
     def process(self, metric):
         """
         Process a metric by sending it to graphite
         """
         # Append the data to the array as a string
         self.metrics.append(str(metric))
+
         if len(self.metrics) >= self.batch_size:
             self._send()
 
     def flush(self):
         """Flush metrics in queue"""
         self._send()
 
     def _send_data(self, data):
         """
         Try to send all data in buffer.
         """
         try:
-            self.socket.sendall(data)
+            self.socket.sendall(str.encode(data))
             self._reset_errors()
         except:
             self._close()
-            self._throttle_error("GraphiteHandler: Socket error, "
-                                 "trying reconnect.")
+            self._throttle_error("GraphiteHandler: Socket error, trying reconnect.")
             self._connect()
+
             try:
                 self.socket.sendall(data)
             except:
                 return
+
             self._reset_errors()
 
     def _time_to_reconnect(self):
         if self.reconnect_interval > 0:
-            if time.time() > (
-                    self.last_connect_timestamp + self.reconnect_interval):
+            if time.time() > (self.last_connect_timestamp + self.reconnect_interval):
                 return True
+
         return False
 
     def _send(self):
         """
         Send data to graphite. Data that can not be sent will be queued.
         """
+
         # Check to see if we have a valid socket. If not, try to connect.
         try:
             try:
                 if self.socket is None:
-                    self.log.debug("GraphiteHandler: Socket is not connected. "
-                                   "Reconnecting.")
+                    self.log.debug(
+                        "GraphiteHandler: Socket is not connected. Reconnecting."
+                    )
                     self._connect()
                 if self.socket is None:
                     self.log.debug("GraphiteHandler: Reconnect failed.")
                 else:
                     # Send data to socket
-                    self._send_data(''.join(self.metrics))
+                    self._send_data("".join(self.metrics))
                     self.metrics = []
+
                     if self._time_to_reconnect():
                         self._close()
             except Exception:
                 self._close()
                 self._throttle_error("GraphiteHandler: Error sending metrics.")
                 raise
         finally:
-            if len(self.metrics) >= (
-                    self.batch_size * self.max_backlog_multiplier):
-                trim_offset = (self.batch_size *
-                               self.trim_backlog_multiplier * -1)
-                self.log.warn('GraphiteHandler: Trimming backlog. Removing' +
-                              ' oldest %d and keeping newest %d metrics',
-                              len(self.metrics) - abs(trim_offset),
-                              abs(trim_offset))
+            if len(self.metrics) >= (self.batch_size * self.max_backlog_multiplier):
+                trim_offset = self.batch_size * self.trim_backlog_multiplier * -1
+                self.log.warn(
+                    "GraphiteHandler: Trimming backlog. Removing oldest %d and keeping newest %d metrics",
+                    len(self.metrics) - abs(trim_offset),
+                    abs(trim_offset),
+                )
                 self.metrics = self.metrics[trim_offset:]
 
     def _connect(self):
         """
         Connect to the graphite server
         """
-        if (self.proto == 'udp'):
+        if self.proto == "udp":
             stream = socket.SOCK_DGRAM
         else:
             stream = socket.SOCK_STREAM
 
-        if (self.proto[-1] == '4'):
+        if self.proto[-1] == "4":
             family = socket.AF_INET
             connection_struct = (self.host, self.port)
-        elif (self.proto[-1] == '6'):
+        elif self.proto[-1] == "6":
             family = socket.AF_INET6
-            connection_struct = (self.host, self.port,
-                                 self.flow_info, self.scope_id)
+            connection_struct = (self.host, self.port, self.flow_info, self.scope_id)
         else:
             connection_struct = (self.host, self.port)
+
             try:
                 addrinfo = socket.getaddrinfo(self.host, self.port, 0, stream)
-            except socket.gaierror, ex:
-                self.log.error("GraphiteHandler: Error looking up graphite host"
-                               " '%s' - %s",
-                               self.host, ex)
+            except socket.gaierror as ex:
+                self.log.error(
+                    "GraphiteHandler: Error looking up graphite host '%s' - %s",
+                    self.host,
+                    ex,
+                )
+
                 return
-            if (len(addrinfo) > 0):
+
+            if len(addrinfo) > 0:
                 family = addrinfo[0][0]
-                if (family == socket.AF_INET6):
-                    connection_struct = (self.host, self.port,
-                                         self.flow_info, self.scope_id)
+
+                if family == socket.AF_INET6:
+                    connection_struct = (
+                        self.host,
+                        self.port,
+                        self.flow_info,
+                        self.scope_id,
+                    )
             else:
                 family = socket.AF_INET
 
         # Create socket
         self.socket = socket.socket(family, stream)
+
         if self.socket is None:
             # Log Error
             self.log.error("GraphiteHandler: Unable to create socket.")
             # Close Socket
             self._close()
+
             return
+
         # Enable keepalives?
-        if self.proto != 'udp' and self.keepalive:
+        if self.proto != "udp" and self.keepalive:
             self.log.error("GraphiteHandler: Setting socket keepalives...")
             self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
-            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE,
-                                   self.keepaliveinterval)
-            self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL,
-                                   self.keepaliveinterval)
+            self.socket.setsockopt(
+                socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, self.keepaliveinterval
+            )
+            self.socket.setsockopt(
+                socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, self.keepaliveinterval
+            )
             self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 3)
+
         # Set socket timeout
         self.socket.settimeout(self.timeout)
+
         # Connect to graphite server
         try:
             self.socket.connect(connection_struct)
+
             # Log
-            self.log.debug("GraphiteHandler: Established connection to "
-                           "graphite server %s:%d.",
-                           self.host, self.port)
+            self.log.debug(
+                "GraphiteHandler: Established connection to graphite server %s:%d.",
+                self.host,
+                self.port,
+            )
             self.last_connect_timestamp = time.time()
-        except Exception, ex:
+        except Exception as ex:
             # Log Error
-            self._throttle_error("GraphiteHandler: Failed to connect to "
-                                 "%s:%i. %s.", self.host, self.port, ex)
+            self._throttle_error(
+                "GraphiteHandler: Failed to connect to %s:%i. %s.",
+                self.host,
+                self.port,
+                ex,
+            )
+
             # Close Socket
             self._close()
+
             return
 
     def _close(self):
         """
         Close the socket
         """
         if self.socket is not None:
             self.socket.close()
+
         self.socket = None
```

### Comparing `diamond-next-4.0.515/src/diamond/collector.py` & `diamond-next-5.0.0/src/diamond/collector.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,157 +1,183 @@
 # coding=utf-8
 
 """
 The Collector class is a base class for all metric collectors.
 """
 
+import logging
 import os
-import socket
 import platform
-import logging
-import configobj
-import time
 import re
+import socket
 import subprocess
+import time
+
+import configobj
 
+from diamond.error import DiamondException
 from diamond.metric import Metric
 from diamond.utils.config import load_config
-from error import DiamondException
 
 # Detect the architecture of the system and set the counters for MAX_VALUES
 # appropriately. Otherwise, rolling over counters will cause incorrect or
 # negative values.
 
-if platform.architecture()[0] == '64bit':
-    MAX_COUNTER = (2 ** 64) - 1
+if platform.architecture()[0] == "64bit":
+    MAX_COUNTER = (2**64) - 1
 else:
-    MAX_COUNTER = (2 ** 32) - 1
+    MAX_COUNTER = (2**32) - 1
 
 
 def get_hostname(config, method=None):
     """
     Returns a hostname as configured by the user
     """
-    method = method or config.get('hostname_method', 'smart')
+    method = method or config.get("hostname_method", "smart")
 
     # case insensitive method
     method = method.lower()
 
-    if 'hostname' in config and method != 'shell':
-        return config['hostname']
+    if "hostname" in config and method != "shell":
+        return config["hostname"]
 
     if method in get_hostname.cached_results:
         return get_hostname.cached_results[method]
 
-    if method == 'shell':
-        if 'hostname' not in config:
+    if method == "shell":
+        if "hostname" not in config:
             raise DiamondException(
-                "hostname must be set to a shell command for"
-                " hostname_method=shell")
+                "hostname must be set to a shell command for hostname_method=shell"
+            )
         else:
-            proc = subprocess.Popen(config['hostname'],
-                                    shell=True,
-                                    stdout=subprocess.PIPE)
+            proc = subprocess.Popen(
+                config["hostname"], shell=True, stdout=subprocess.PIPE
+            )
             hostname = proc.communicate()[0].strip()
+
             if proc.returncode != 0:
-                raise subprocess.CalledProcessError(proc.returncode,
-                                                    config['hostname'])
+                raise subprocess.CalledProcessError(proc.returncode, config["hostname"])
+
             get_hostname.cached_results[method] = hostname
+
             return hostname
 
-    if method == 'smart':
-        hostname = get_hostname(config, 'fqdn_short')
-        if hostname != 'localhost':
+    if method == "smart":
+        hostname = get_hostname(config, "fqdn_short")
+
+        if hostname != "localhost":
             get_hostname.cached_results[method] = hostname
+
             return hostname
-        hostname = get_hostname(config, 'hostname_short')
+
+        hostname = get_hostname(config, "hostname_short")
         get_hostname.cached_results[method] = hostname
+
         return hostname
 
-    if method == 'fqdn_short':
-        hostname = socket.getfqdn().split('.')[0]
+    if method == "fqdn_short":
+        hostname = socket.getfqdn().split(".")[0]
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'fqdn':
-        hostname = socket.getfqdn().replace('.', '_')
+    if method == "fqdn":
+        hostname = socket.getfqdn().replace(".", "_")
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'fqdn_rev':
-        hostname = socket.getfqdn().split('.')
+    if method == "fqdn_rev":
+        hostname = socket.getfqdn().split(".")
         hostname.reverse()
-        hostname = '.'.join(hostname)
+        hostname = ".".join(hostname)
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'uname_short':
-        hostname = os.uname()[1].split('.')[0]
+    if method == "uname_short":
+        hostname = os.uname()[1].split(".")[0]
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'uname_rev':
-        hostname = os.uname()[1].split('.')
+    if method == "uname_rev":
+        hostname = os.uname()[1].split(".")
         hostname.reverse()
-        hostname = '.'.join(hostname)
+        hostname = ".".join(hostname)
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'hostname':
+    if method == "hostname":
         hostname = socket.gethostname()
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'hostname_short':
-        hostname = socket.gethostname().split('.')[0]
+    if method == "hostname_short":
+        hostname = socket.gethostname().split(".")[0]
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'hostname_rev':
-        hostname = socket.gethostname().split('.')
+    if method == "hostname_rev":
+        hostname = socket.gethostname().split(".")
         hostname.reverse()
-        hostname = '.'.join(hostname)
+        hostname = ".".join(hostname)
         get_hostname.cached_results[method] = hostname
-        if hostname == '':
-            raise DiamondException('Hostname is empty?!')
+
+        if hostname == "":
+            raise DiamondException("Hostname is empty?!")
+
         return hostname
 
-    if method == 'none':
+    if method == "none":
         get_hostname.cached_results[method] = None
+
         return None
 
-    raise NotImplementedError(config['hostname_method'])
+    raise NotImplementedError(config["hostname_method"])
+
 
 get_hostname.cached_results = {}
 
 
 def str_to_bool(value):
     """
     Converts string truthy/falsey strings to a bool
     Empty strings are false
     """
-    if isinstance(value, basestring):
+    if isinstance(value, str):
         value = value.strip().lower()
-        if value in ['true', 't', 'yes', 'y']:
+
+        if value in ["true", "t", "yes", "y"]:
             return True
-        elif value in ['false', 'f', 'no', 'n', '']:
+        elif value in ["false", "f", "no", "n", ""]:
             return False
         else:
             raise NotImplementedError("Unknown bool %s" % value)
 
     return value
 
 
@@ -160,16 +186,18 @@
     The Collector class is a base class for all metric collectors.
     """
 
     def __init__(self, config=None, handlers=[], name=None, configfile=None):
         """
         Create a new instance of the Collector class
         """
+
         # Initialize Logger
-        self.log = logging.getLogger('diamond')
+        self.log = logging.getLogger("diamond")
+
         # Initialize Members
         if name is None:
             self.name = self.__class__.__name__
         else:
             self.name = name
 
         self.handlers = handlers
@@ -190,275 +218,314 @@
 
         if configfile is not None:
             self.configfile = os.path.abspath(configfile)
 
         if self.configfile is not None:
             config = load_config(self.configfile)
 
-            if 'collectors' in config:
-                if 'default' in config['collectors']:
-                    self.config.merge(config['collectors']['default'])
+            if "collectors" in config:
+                if "collectorsDefault" in config["collectors"]:
+                    self.config.merge(config["collectors"]["collectorsDefault"])
 
-                if self.name in config['collectors']:
-                    self.config.merge(config['collectors'][self.name])
+                if self.name in config["collectors"]:
+                    self.config.merge(config["collectors"][self.name])
 
         if override_config is not None:
-            if 'collectors' in override_config:
-                if 'default' in override_config['collectors']:
-                    self.config.merge(override_config['collectors']['default'])
+            if "collectors" in override_config:
+                if "collectorsDefault" in override_config["collectors"]:
+                    self.config.merge(
+                        override_config["collectors"]["collectorsDefault"]
+                    )
 
-                if self.name in override_config['collectors']:
-                    self.config.merge(override_config['collectors'][self.name])
+                if self.name in override_config["collectors"]:
+                    self.config.merge(override_config["collectors"][self.name])
 
         self.process_config()
 
     def process_config(self):
         """
         Intended to put any code that should be run after any config reload
         event
         """
-        if 'byte_unit' in self.config:
-            if isinstance(self.config['byte_unit'], basestring):
-                self.config['byte_unit'] = self.config['byte_unit'].split()
-
-        if 'enabled' in self.config:
-            self.config['enabled'] = str_to_bool(self.config['enabled'])
-
-        if 'measure_collector_time' in self.config:
-            self.config['measure_collector_time'] = str_to_bool(
-                self.config['measure_collector_time'])
+        if "byte_unit" in self.config:
+            if isinstance(self.config["byte_unit"], str):
+                self.config["byte_unit"] = self.config["byte_unit"].split()
+
+        if "enabled" in self.config:
+            self.config["enabled"] = str_to_bool(self.config["enabled"])
+
+        if "measure_collector_time" in self.config:
+            self.config["measure_collector_time"] = str_to_bool(
+                self.config["measure_collector_time"]
+            )
 
         # Raise an error if both whitelist and blacklist are specified
-        if ((self.config.get('metrics_whitelist', None) and
-             self.config.get('metrics_blacklist', None))):
+        if self.config.get("metrics_whitelist", None) and self.config.get(
+            "metrics_blacklist", None
+        ):
             raise DiamondException(
-                'Both metrics_whitelist and metrics_blacklist specified ' +
-                'in file %s' % self.configfile)
-
-        if self.config.get('metrics_whitelist', None):
-            self.config['metrics_whitelist'] = re.compile(
-                self.config['metrics_whitelist'])
-        elif self.config.get('metrics_blacklist', None):
-            self.config['metrics_blacklist'] = re.compile(
-                self.config['metrics_blacklist'])
+                "Both metrics_whitelist and metrics_blacklist specified in file %s"
+                % self.configfile
+            )
+
+        if self.config.get("metrics_whitelist", None):
+            self.config["metrics_whitelist"] = re.compile(
+                self.config["metrics_whitelist"]
+            )
+        elif self.config.get("metrics_blacklist", None):
+            self.config["metrics_blacklist"] = re.compile(
+                self.config["metrics_blacklist"]
+            )
 
     def get_default_config_help(self):
         """
         Returns the help text for the configuration options for this collector
         """
         return {
-            'enabled': 'Enable collecting these metrics',
-            'byte_unit': 'Default numeric output(s)',
-            'measure_collector_time': 'Collect the collector run time in ms',
-            'metrics_whitelist': 'Regex to match metrics to transmit. ' +
-                                 'Mutually exclusive with metrics_blacklist',
-            'metrics_blacklist': 'Regex to match metrics to block. ' +
-                                 'Mutually exclusive with metrics_whitelist',
+            "enabled": "Enable collecting these metrics",
+            "byte_unit": "Default numeric output(s)",
+            "measure_collector_time": "Collect the collector run time in ms",
+            "metrics_whitelist": "Regex to match metrics to transmit. Mutually exclusive with metrics_blacklist",
+            "metrics_blacklist": "Regex to match metrics to block. Mutually exclusive with metrics_whitelist",
         }
 
     def get_default_config(self):
         """
         Return the default config for the collector
         """
         return {
             # Defaults options for all Collectors
-
             # Uncomment and set to hardcode a hostname for the collector path
             # Keep in mind, periods are seperators in graphite
             # 'hostname': 'my_custom_hostname',
-
             # If you perfer to just use a different way of calculating the
             # hostname
             # Uncomment and set this to one of these values:
             # fqdn_short  = Default. Similar to hostname -s
             # fqdn        = hostname output
             # fqdn_rev    = hostname in reverse (com.example.www)
             # uname_short = Similar to uname -n, but only the first part
             # uname_rev   = uname -r in reverse (com.example.www)
             # 'hostname_method': 'fqdn_short',
-
             # All collectors are disabled by default
-            'enabled': False,
-
+            "enabled": False,
             # Path Prefix
-            'path_prefix': 'servers',
-
+            "path_prefix": "servers",
             # Path Prefix for Virtual Machine metrics
-            'instance_prefix': 'instances',
-
+            "instance_prefix": "instances",
             # Path Suffix
-            'path_suffix': '',
-
+            "path_suffix": "",
             # Default Poll Interval (seconds)
-            'interval': 300,
-
+            "interval": 300,
             # Default Event TTL (interval multiplier)
-            'ttl_multiplier': 2,
-
+            "ttl_multiplier": 2,
             # Default numeric output
-            'byte_unit': 'byte',
-
+            "byte_unit": "byte",
             # Collect the collector run time in ms
-            'measure_collector_time': False,
-
+            "measure_collector_time": False,
             # Whitelist of metrics to let through
-            'metrics_whitelist': None,
-
+            "metrics_whitelist": None,
             # Blacklist of metrics to let through
-            'metrics_blacklist': None,
+            "metrics_blacklist": None,
         }
 
     def get_metric_path(self, name, instance=None):
         """
         Get metric path.
         Instance indicates that this is a metric for a
             virtual machine and should have a different
             root prefix.
         """
-        if 'path' in self.config:
-            path = self.config['path']
+        if "path" in self.config:
+            path = self.config["path"]
         else:
             path = self.__class__.__name__
 
         if instance is not None:
-            if 'instance_prefix' in self.config:
-                prefix = self.config['instance_prefix']
+            if "instance_prefix" in self.config:
+                prefix = self.config["instance_prefix"]
             else:
-                prefix = 'instances'
-            if path == '.':
-                return '.'.join([prefix, instance, name])
+                prefix = "instances"
+
+            if path == ".":
+                return ".".join([prefix, instance, name])
             else:
-                return '.'.join([prefix, instance, path, name])
+                return ".".join([prefix, instance, path, name])
 
-        if 'path_prefix' in self.config:
-            prefix = self.config['path_prefix']
+        if "path_prefix" in self.config:
+            prefix = self.config["path_prefix"]
         else:
-            prefix = 'systems'
+            prefix = "systems"
 
-        if 'path_suffix' in self.config:
-            suffix = self.config['path_suffix']
+        if "path_suffix" in self.config:
+            suffix = self.config["path_suffix"]
         else:
             suffix = None
 
         hostname = get_hostname(self.config)
+
         if hostname is not None:
             if prefix:
                 prefix = ".".join((prefix, hostname))
             else:
                 prefix = hostname
 
         # if there is a suffix, add after the hostname
         if suffix:
-            prefix = '.'.join((prefix, suffix))
+            prefix = ".".join((prefix, suffix))
 
-        is_path_invalid = path == '.' or not path
+        is_path_invalid = path == "." or not path
 
         if is_path_invalid and prefix:
-            return '.'.join([prefix, name])
+            return ".".join([prefix, name])
         elif prefix:
-            return '.'.join([prefix, path, name])
+            return ".".join([prefix, path, name])
         elif is_path_invalid:
             return name
         else:
-            return '.'.join([path, name])
+            return ".".join([path, name])
 
     def get_hostname(self):
         return get_hostname(self.config)
 
     def collect(self):
         """
         Default collector method
         """
         raise NotImplementedError()
 
-    def publish(self, name, value, raw_value=None, precision=0,
-                metric_type='GAUGE', instance=None):
+    def publish(
+        self,
+        name,
+        value,
+        raw_value=None,
+        precision=0,
+        metric_type="GAUGE",
+        instance=None,
+    ):
         """
         Publish a metric with the given name
         """
         # Check whitelist/blacklist
-        if self.config['metrics_whitelist']:
-            if not self.config['metrics_whitelist'].match(name):
+        if self.config["metrics_whitelist"]:
+            if not self.config["metrics_whitelist"].match(name):
                 return
-        elif self.config['metrics_blacklist']:
-            if self.config['metrics_blacklist'].match(name):
+        elif self.config["metrics_blacklist"]:
+            if self.config["metrics_blacklist"].match(name):
                 return
 
         # Get metric Path
         path = self.get_metric_path(name, instance=instance)
 
         # Get metric TTL
-        ttl = float(self.config['interval']) * float(
-            self.config['ttl_multiplier'])
+        ttl = float(self.config["interval"]) * float(self.config["ttl_multiplier"])
 
         # Create Metric
         try:
-            metric = Metric(path, value, raw_value=raw_value, timestamp=None,
-                            precision=precision, host=self.get_hostname(),
-                            metric_type=metric_type, ttl=ttl)
+            metric = Metric(
+                path,
+                value,
+                raw_value=raw_value,
+                timestamp=None,
+                precision=precision,
+                host=self.get_hostname(),
+                metric_type=metric_type,
+                ttl=ttl,
+            )
         except DiamondException:
-            self.log.error(('Error when creating new Metric: path=%r, '
-                            'value=%r'), path, value)
+            self.log.error(
+                "Error when creating new Metric: path=%r, value=%r", path, value
+            )
             raise
 
         # Publish Metric
         self.publish_metric(metric)
 
     def publish_metric(self, metric):
         """
         Publish a Metric object
         """
+
         # Process Metric
         for handler in self.handlers:
             handler._process(metric)
 
     def publish_gauge(self, name, value, precision=0, instance=None):
-        return self.publish(name, value, precision=precision,
-                            metric_type='GAUGE', instance=instance)
-
-    def publish_counter(self, name, value, precision=0, max_value=0,
-                        time_delta=True, interval=None, allow_negative=False,
-                        instance=None):
+        return self.publish(
+            name, value, precision=precision, metric_type="GAUGE", instance=instance
+        )
+
+    def publish_counter(
+        self,
+        name,
+        value,
+        precision=0,
+        max_value=0,
+        time_delta=True,
+        interval=None,
+        allow_negative=False,
+        instance=None,
+    ):
         raw_value = value
-        value = self.derivative(name, value, max_value=max_value,
-                                time_delta=time_delta, interval=interval,
-                                allow_negative=allow_negative,
-                                instance=instance)
-        return self.publish(name, value, raw_value=raw_value,
-                            precision=precision, metric_type='COUNTER',
-                            instance=instance)
-
-    def derivative(self, name, new, max_value=0,
-                   time_delta=True, interval=None,
-                   allow_negative=False, instance=None):
+        value = self.derivative(
+            name,
+            value,
+            max_value=max_value,
+            time_delta=time_delta,
+            interval=interval,
+            allow_negative=allow_negative,
+            instance=instance,
+        )
+        return self.publish(
+            name,
+            value,
+            raw_value=raw_value,
+            precision=precision,
+            metric_type="COUNTER",
+            instance=instance,
+        )
+
+    def derivative(
+        self,
+        name,
+        new,
+        max_value=0,
+        time_delta=True,
+        interval=None,
+        allow_negative=False,
+        instance=None,
+    ):
         """
         Calculate the derivative of the metric.
         """
         # Format Metric Path
         path = self.get_metric_path(name, instance=instance)
 
         if path in self.last_values:
             old = self.last_values[path]
+
             # Check for rollover
             if new < old:
                 old = old - max_value
+
             # Get Change in X (value)
             derivative_x = new - old
 
             # If we pass in a interval, use it rather then the configured one
             if interval is None:
-                interval = float(self.config['interval'])
+                interval = float(self.config["interval"])
 
             # Get Change in Y (time)
             if time_delta:
                 derivative_y = interval
             else:
                 derivative_y = 1
 
             result = float(derivative_x) / float(derivative_y)
+
             if result < 0 and not allow_negative:
                 result = 0
         else:
             result = 0
 
         # Store Old Value
         self.last_values[path] = new
@@ -475,97 +542,102 @@
 
             # Collect Data
             self.collect()
 
             end_time = time.time()
             collector_time = int((end_time - start_time) * 1000)
 
-            self.log.debug('Collection took %s ms', collector_time)
+            self.log.debug("Collection took %s ms", collector_time)
 
-            if 'measure_collector_time' in self.config:
-                if self.config['measure_collector_time']:
-                    metric_name = 'collector_time_ms'
+            if "measure_collector_time" in self.config:
+                if self.config["measure_collector_time"]:
+                    metric_name = "collector_time_ms"
                     metric_value = collector_time
                     self.publish(metric_name, metric_value)
         finally:
-            # After collector run, invoke a flush
-            # method on each handler.
+            # After collector run, invoke a flush method on each handler.
             for handler in self.handlers:
                 handler._flush()
 
     def find_binary(self, binary):
         """
         Scan and return the first path to a binary that we can find
         """
         if os.path.exists(binary):
             return binary
 
         # Extract out the filename if we were given a full path
         binary_name = os.path.basename(binary)
 
         # Gather $PATH
-        search_paths = os.environ['PATH'].split(':')
+        search_paths = os.environ["PATH"].split(":")
 
         # Extra paths to scan...
         default_paths = [
-            '/usr/bin',
-            '/bin'
-            '/usr/local/bin',
-            '/usr/sbin',
-            '/sbin'
-            '/usr/local/sbin',
+            "/usr/bin",
+            "/bin" "/usr/local/bin",
+            "/usr/sbin",
+            "/sbin" "/usr/local/sbin",
         ]
 
         for path in default_paths:
             if path not in search_paths:
                 search_paths.append(path)
 
         for path in search_paths:
             if os.path.isdir(path):
                 filename = os.path.join(path, binary_name)
+
                 if os.path.exists(filename):
                     return filename
 
         return binary
 
 
 class ProcessCollector(Collector):
     """
     Collector with helpers for handling running commands with/without sudo
     """
 
     def get_default_config_help(self):
         config_help = super(ProcessCollector, self).get_default_config_help()
-        config_help.update({
-            'use_sudo':     'Use sudo?',
-            'sudo_cmd':     'Path to sudo',
-        })
+        config_help.update(
+            {
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ProcessCollector, self).get_default_config()
-        config.update({
-            'use_sudo':     False,
-            'sudo_cmd':     self.find_binary('/usr/bin/sudo'),
-        })
+        config.update(
+            {
+                "use_sudo": False,
+                "sudo_cmd": self.find_binary("/usr/bin/sudo"),
+            }
+        )
+
         return config
 
     def run_command(self, args):
-        if 'bin' not in self.config:
-            raise Exception('config does not have any binary configured')
-        if not os.access(self.config['bin'], os.X_OK):
-            raise Exception('%s is not executable' % self.config['bin'])
+        if "bin" not in self.config:
+            raise Exception("config does not have any binary configured")
+
+        if not os.access(self.config["bin"], os.X_OK):
+            raise Exception("%s is not executable" % self.config["bin"])
+
         try:
             command = args
-            command.insert(0, self.config['bin'])
+            command.insert(0, self.config["bin"])
 
-            if str_to_bool(self.config['use_sudo']):
-                command.insert(0, self.config['sudo_cmd'])
+            if str_to_bool(self.config["use_sudo"]):
+                command.insert(0, self.config["sudo_cmd"])
 
-            return subprocess.Popen(command,
-                                    stdout=subprocess.PIPE).communicate()
+            return subprocess.Popen(command, stdout=subprocess.PIPE).communicate()
         except OSError:
             self.log.exception("Unable to run %s", command)
             return None
```

### Comparing `diamond-next-4.0.515/src/collectors/kvm/kvm.py` & `diamond-next-5.0.0/src/collectors/kvm/kvm.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,44 +5,46 @@
 
 #### Dependencies
 
  * /sys/kernel/debug/kvm
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
 
-class KVMCollector(diamond.collector.Collector):
 
-    PROC = '/sys/kernel/debug/kvm'
+class KVMCollector(diamond.collector.Collector):
+    PROC = "/sys/kernel/debug/kvm"
 
     def get_default_config_help(self):
         config_help = super(KVMCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(KVMCollector, self).get_default_config()
-        config.update({
-            'path': 'kvm',
-        })
+        config.update(
+            {
+                "path": "kvm",
+            }
+        )
+
         return config
 
     def collect(self):
         if not os.path.isdir(self.PROC):
-            self.log.error('/sys/kernel/debug/kvm is missing. Did you' +
-                           ' "mount -t debugfs debugfs /sys/kernel/debug"?')
+            self.log.error(
+                '/sys/kernel/debug/kvm is missing. Did you "mount -t debugfs debugfs /sys/kernel/debug"?'
+            )
             return {}
 
         for filename in os.listdir(self.PROC):
             filepath = os.path.abspath(os.path.join(self.PROC, filename))
-            fh = open(filepath, 'r')
-            metric_value = self.derivative(filename,
-                                           float(fh.readline()),
-                                           4294967295)
+            fh = open(filepath, "r")
+            metric_value = self.derivative(filename, float(fh.readline()), 4294967295)
             self.publish(filename, metric_value)
```

### Comparing `diamond-next-4.0.515/src/collectors/tcp/tcp.py` & `diamond-next-5.0.0/src/collectors/tcp/tcp.py`

 * *Files 6% similar despite different names*

```diff
@@ -172,106 +172,109 @@
 example, bad TCP checksums).</td></tr>
 <tr><td>OutRsts</td><td>The number of TCP segments sent containing the RST
 flag.</td></tr>
 </table>
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
 
-class TCPCollector(diamond.collector.Collector):
 
-    PROC = [
-        '/proc/net/netstat',
-        '/proc/net/snmp'
-    ]
+class TCPCollector(diamond.collector.Collector):
+    PROC = ["/proc/net/netstat", "/proc/net/snmp"]
 
     def process_config(self):
         super(TCPCollector, self).process_config()
-        if self.config['allowed_names'] is None:
-            self.config['allowed_names'] = []
+        if self.config["allowed_names"] is None:
+            self.config["allowed_names"] = []
 
-        if self.config['gauges'] is None:
-            self.config['gauges'] = ['CurrEstab', 'MaxConn']
+        if self.config["gauges"] is None:
+            self.config["gauges"] = ["CurrEstab", "MaxConn"]
 
     def get_default_config_help(self):
         config_help = super(TCPCollector, self).get_default_config_help()
-        config_help.update({
-            'allowed_names': 'list of entries to collect, empty to collect all',
-            'gauges': 'list of metrics to be published as gauges',
-        })
+        config_help.update(
+            {
+                "allowed_names": "list of entries to collect, empty to collect all",
+                "gauges": "list of metrics to be published as gauges",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(TCPCollector, self).get_default_config()
-        config.update({
-            'path': 'tcp',
-            'allowed_names':
-                'ListenOverflows, ListenDrops, TCPLoss, ' +
-                'TCPTimeouts, TCPFastRetrans, TCPLostRetransmit, ' +
-                'TCPForwardRetrans, TCPSlowStartRetrans, CurrEstab, ' +
-                'TCPAbortOnMemory, TCPBacklogDrop, AttemptFails, ' +
-                'EstabResets, InErrs, ActiveOpens, PassiveOpens',
-            'gauges': 'CurrEstab, MaxConn',
-        })
+        config.update(
+            {
+                "path": "tcp",
+                "allowed_names": "ListenOverflows, ListenDrops, TCPLoss, "
+                + "TCPTimeouts, TCPFastRetrans, TCPLostRetransmit, "
+                + "TCPForwardRetrans, TCPSlowStartRetrans, CurrEstab, "
+                + "TCPAbortOnMemory, TCPBacklogDrop, AttemptFails, "
+                + "EstabResets, InErrs, ActiveOpens, PassiveOpens",
+                "gauges": "CurrEstab, MaxConn",
+            }
+        )
         return config
 
     def collect(self):
         metrics = {}
 
         for filepath in self.PROC:
             if not os.access(filepath, os.R_OK):
-                self.log.error('Permission to access %s denied', filepath)
+                self.log.error("Permission to access %s denied", filepath)
                 continue
 
-            header = ''
-            data = ''
+            header = ""
+            data = ""
 
             # Seek the file for the lines that start with Tcp
             file = open(filepath)
 
             if not file:
-                self.log.error('Failed to open %s', filepath)
+                self.log.error("Failed to open %s", filepath)
                 continue
 
             while True:
                 line = file.readline()
 
                 # Reached EOF?
                 if len(line) == 0:
                     break
 
                 # Line has metrics?
                 if line.startswith("Tcp"):
                     header = line
                     data = file.readline()
                     break
+
             file.close()
 
             # No data from the file?
-            if header == '' or data == '':
-                self.log.error('%s has no lines with Tcp', filepath)
+            if header == "" or data == "":
+                self.log.error("%s has no lines with Tcp", filepath)
                 continue
 
             header = header.split()
             data = data.split()
 
-            for i in xrange(1, len(header)):
+            for i in range(1, len(header)):
                 metrics[header[i]] = data[i]
 
         for metric_name in metrics.keys():
-            if ((len(self.config['allowed_names']) > 0 and
-                 metric_name not in self.config['allowed_names'])):
+            if (
+                len(self.config["allowed_names"]) > 0
+                and metric_name not in self.config["allowed_names"]
+            ):
                 continue
 
-            value = long(metrics[metric_name])
+            value = int(metrics[metric_name])
 
             # Publish the metric
-            if metric_name in self.config['gauges']:
+            if metric_name in self.config["gauges"]:
                 self.publish_gauge(metric_name, value, 0)
             else:
                 self.publish_counter(metric_name, value, 0)
```

#### html2text {}

```diff
@@ -167,36 +167,36 @@
                           The total number of segments retransmitted - that is,
 RetransSegs               the number of TCP segments transmitted containing one
                           or more previously transmitted octets.
 InErrs                    The total number of segments received in error (for
                           example, bad TCP checksums).
 OutRsts                   The number of TCP segments sent containing the RST
                           flag.
-""" import diamond.collector import os class TCPCollector
-(diamond.collector.Collector): PROC = [ '/proc/net/netstat', '/proc/net/snmp' ]
+""" import os import diamond.collector class TCPCollector
+(diamond.collector.Collector): PROC = ["/proc/net/netstat", "/proc/net/snmp"]
 def process_config(self): super(TCPCollector, self).process_config() if
-self.config['allowed_names'] is None: self.config['allowed_names'] = [] if
-self.config['gauges'] is None: self.config['gauges'] = ['CurrEstab', 'MaxConn']
+self.config["allowed_names"] is None: self.config["allowed_names"] = [] if
+self.config["gauges"] is None: self.config["gauges"] = ["CurrEstab", "MaxConn"]
 def get_default_config_help(self): config_help = super(TCPCollector,
-self).get_default_config_help() config_help.update({ 'allowed_names': 'list of
-entries to collect, empty to collect all', 'gauges': 'list of metrics to be
-published as gauges', }) return config_help def get_default_config(self): """
+self).get_default_config_help() config_help.update( { "allowed_names": "list of
+entries to collect, empty to collect all", "gauges": "list of metrics to be
+published as gauges", } ) return config_help def get_default_config(self): """
 Returns the default collector settings """ config = super(TCPCollector,
-self).get_default_config() config.update({ 'path': 'tcp', 'allowed_names':
-'ListenOverflows, ListenDrops, TCPLoss, ' + 'TCPTimeouts, TCPFastRetrans,
-TCPLostRetransmit, ' + 'TCPForwardRetrans, TCPSlowStartRetrans, CurrEstab, ' +
-'TCPAbortOnMemory, TCPBacklogDrop, AttemptFails, ' + 'EstabResets, InErrs,
-ActiveOpens, PassiveOpens', 'gauges': 'CurrEstab, MaxConn', }) return config
+self).get_default_config() config.update( { "path": "tcp", "allowed_names":
+"ListenOverflows, ListenDrops, TCPLoss, " + "TCPTimeouts, TCPFastRetrans,
+TCPLostRetransmit, " + "TCPForwardRetrans, TCPSlowStartRetrans, CurrEstab, " +
+"TCPAbortOnMemory, TCPBacklogDrop, AttemptFails, " + "EstabResets, InErrs,
+ActiveOpens, PassiveOpens", "gauges": "CurrEstab, MaxConn", } ) return config
 def collect(self): metrics = {} for filepath in self.PROC: if not os.access
-(filepath, os.R_OK): self.log.error('Permission to access %s denied', filepath)
-continue header = '' data = '' # Seek the file for the lines that start with
-Tcp file = open(filepath) if not file: self.log.error('Failed to open %s',
+(filepath, os.R_OK): self.log.error("Permission to access %s denied", filepath)
+continue header = "" data = "" # Seek the file for the lines that start with
+Tcp file = open(filepath) if not file: self.log.error("Failed to open %s",
 filepath) continue while True: line = file.readline() # Reached EOF? if len
 (line) == 0: break # Line has metrics? if line.startswith("Tcp"): header = line
 data = file.readline() break file.close() # No data from the file? if header ==
-'' or data == '': self.log.error('%s has no lines with Tcp', filepath) continue
-header = header.split() data = data.split() for i in xrange(1, len(header)):
-metrics[header[i]] = data[i] for metric_name in metrics.keys(): if ((len
-(self.config['allowed_names']) > 0 and metric_name not in self.config
-['allowed_names'])): continue value = long(metrics[metric_name]) # Publish the
-metric if metric_name in self.config['gauges']: self.publish_gauge(metric_name,
+"" or data == "": self.log.error("%s has no lines with Tcp", filepath) continue
+header = header.split() data = data.split() for i in range(1, len(header)):
+metrics[header[i]] = data[i] for metric_name in metrics.keys(): if ( len
+(self.config["allowed_names"]) > 0 and metric_name not in self.config
+["allowed_names"] ): continue value = int(metrics[metric_name]) # Publish the
+metric if metric_name in self.config["gauges"]: self.publish_gauge(metric_name,
 value, 0) else: self.publish_counter(metric_name, value, 0)
```

### Comparing `diamond-next-4.0.515/src/collectors/postfix/postfix.py` & `diamond-next-5.0.0/src/collectors/etcdstat/etcdstat.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,133 +1,123 @@
 # coding=utf-8
 
 """
-Collect stats from postfix-stats. postfix-stats is a simple threaded stats
-aggregator for Postfix. When running as a syslog destination, it can be used to
-get realtime cumulative stats.
-
-#### Dependencies
-
- * socket
- * json (or simplejson)
- * [postfix-stats](https://github.com/disqus/postfix-stats)
 
-"""
-
-import socket
-import sys
+Collects metrics from an Etcd instance.
 
-try:
-    import json
-except ImportError:
-    import simplejson as json
+#### Example Configuration
 
-import diamond.collector
+```
+    host = localhost
+    port = 2379
+```
+"""
 
-from diamond.collector import str_to_bool
+import json
+import urllib.error
+import urllib.request
 
-if sys.version_info < (2, 6):
-    from string import maketrans
-    DOTS_TO_UNDERS = maketrans('.', '_')
-else:
-    DOTS_TO_UNDERS = {ord(u'.'): u'_'}
+import diamond.collector
 
+METRICS_KEYS = [
+    "sendPkgRate",
+    "recvPkgRate",
+    "sendAppendRequestCnt",
+    "recvAppendRequestCnt",
+    "sendBandwidthRate",
+    "recvBandwidthRate",
+]
 
-class PostfixCollector(diamond.collector.Collector):
 
+class EtcdCollector(diamond.collector.Collector):
     def get_default_config_help(self):
-        config_help = super(PostfixCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host':             'Hostname to connect to',
-            'port':             'Port to connect to',
-            'include_clients':  'Include client connection stats',
-        })
+        config_help = super(EtcdCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "Hostname",
+                "port": "Port (default is 2379)",
+                "timeout": "Timeout per HTTP(s) call",
+                "use_tls": "Use TLS/SSL or just unsecure (default is unsecure)",
+                "ca_file": "Only applies when use_tls=true. Path to CA certificate file to use for server identity verification",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
-        """
-        Returns the default collector settings
-        """
-        config = super(PostfixCollector, self).get_default_config()
-        config.update({
-            'path':             'postfix',
-            'host':             'localhost',
-            'port':             7777,
-            'include_clients':  True,
-        })
-        return config
-
-    def get_json(self):
-        json_string = ''
-
-        address = (self.config['host'], int(self.config['port']))
-
-        s = None
-        try:
-            try:
-                s = socket.create_connection(address, timeout=1)
-
-                s.sendall('stats\n')
-
-                while 1:
-                    data = s.recv(4096)
-                    if not data:
-                        break
-                    json_string += data
-            except socket.error:
-                self.log.exception("Error talking to postfix-stats")
-                return '{}'
-        finally:
-            if s:
-                s.close()
-
-        return json_string or '{}'
-
-    def get_data(self):
-        json_string = self.get_json()
+        config = super(EtcdCollector, self).get_default_config()
+        config.update(
+            {
+                "host": "localhost",
+                "port": 2379,
+                "path": "etcd",
+                "timeout": 5,
+                "use_tls": False,
+                "ca_file": "",
+            }
+        )
 
-        try:
-            data = json.loads(json_string)
-        except (ValueError, TypeError):
-            self.log.exception("Error parsing json from postfix-stats")
-            return None
+        return config
 
-        return data
+    def __init__(self, *args, **kwargs):
+        super(EtcdCollector, self).__init__(*args, **kwargs)
 
     def collect(self):
-        data = self.get_data()
+        self.collect_self_metrics()
+        self.collect_store_metrics()
 
-        if not data:
-            return
+    def collect_self_metrics(self):
+        metrics = self.get_self_metrics()
 
-        if str_to_bool(self.config['include_clients']) and u'clients' in data:
-            for client, value in data['clients'].iteritems():
-                # translate dots to underscores in client names
-                metric = u'.'.join(['clients',
-                                    client.translate(DOTS_TO_UNDERS)])
+        if "state" in metrics and metrics["state"] == "StateLeader":
+            self.publish("self.is_leader", 1)
+        else:
+            self.publish("self.is_leader", 0)
 
-                dvalue = self.derivative(metric, value)
+        for k in METRICS_KEYS:
+            if k not in metrics:
+                continue
 
-                self.publish(metric, dvalue, precision=4)
+            v = metrics[k]
+            key = self.clean_up(k)
+            self.publish("self.%s" % key, v)
 
-        for action in (u'in', u'recv', u'send'):
-            if action not in data:
-                continue
+    def collect_store_metrics(self):
+        metrics = self.get_store_metrics()
 
-            for sect, stats in data[action].iteritems():
-                for status, value in stats.iteritems():
-                    metric = '.'.join([action,
-                                       sect,
-                                       status.translate(DOTS_TO_UNDERS)])
+        for k, v in iter(metrics.items()):
+            key = self.clean_up(k)
+            self.publish("store.%s" % key, v)
 
-                    dvalue = self.derivative(metric, value)
+    def get_self_metrics(self):
+        return self.get_metrics("self")
 
-                    self.publish(metric, dvalue, precision=4)
+    def get_store_metrics(self):
+        return self.get_metrics("store")
 
-        if u'local' in data:
-            for key, value in data[u'local'].iteritems():
-                metric = '.'.join(['local', key])
+    def get_metrics(self, category):
+        try:
+            opts = {
+                "timeout": int(self.config["timeout"]),
+            }
+
+            if self.config["use_tls"]:
+                protocol = "https"
+                opts["cafile"] = self.config["ca_file"]
+            else:
+                protocol = "http"
+
+            url = "%s://%s:%s/v2/stats/%s" % (
+                protocol,
+                self.config["host"],
+                self.config["port"],
+                category,
+            )
+
+            return json.load(urllib.request.urlopen(url, **opts))
+        except (urllib.error.HTTPError, ValueError) as err:
+            self.log.error("Unable to read JSON response: %s" % err)
 
-                dvalue = self.derivative(metric, value)
+            return {}
 
-                self.publish(metric, dvalue, precision=4)
+    def clean_up(self, text):
+        return text.replace("/", ".")
```

### Comparing `diamond-next-4.0.515/src/collectors/jbossapi/jbossapi.py` & `diamond-next-5.0.0/src/collectors/jbossapi/jbossapi.py`

 * *Files 20% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 
 References:
 https://docs.jboss.org/author/display/AS7/Management+API+reference
 http://middlewaremagic.com/jboss/?p=2476
 
 TODO:
 This code was made to work with the local system 'curl' command, due to
-difficulties getting urllib2 or pycurl to work under the python 2.4 options
+difficulties getting urllib or pycurl to work under the python 2.4 options
 successfully doing SSL Digest Authentication.
 
 Plan is to make this code work with newer versions of python and possibly
 Requests. (http://docs.python-requests.org/en/latest/)
 
 If possible, please make future updates backwards compatible to call the local
 curl as an option.
@@ -32,15 +32,15 @@
  * java
  * jboss
  * curl
  * json
 
 ##### Configuration
 
-# Uses local system curl until can be made to work with either urllib2, pycurl,
+# Uses local system curl until can be made to work with either urllib, pycurl,
 or requests (http://docs.python-requests.org/en/latest/)
 
 
 enabled = True
 path_suffix = ""
 measure_collector_time = False
 interface_regex = ^(.+?)\.
@@ -57,339 +57,395 @@
 jvm_memory_pool_stats = True | False
 jvm_gc_stats = True | False
 jvm_thread_stats = True | False
 
 
 """
 
-import diamond.collector
 import os
 import re
 import subprocess
 
+import diamond.collector
+
 try:
     import json
 except ImportError:
     import simplejson as json
 
 
 # Setup a set of VARs
 # Set this for use in curl request
 header = '"Content-Type: application/json"'
 
 operational_type = [
-    'app',
-    'web',
-    'jvm',
-    'thread_pool',
+    "app",
+    "web",
+    "jvm",
+    "thread_pool",
 ]
 
 web_stats = [
-    'errorCount',
-    'requestCount',
-    'bytesReceived',
-    'bytesSent',
-    'processingTime',
+    "errorCount",
+    "requestCount",
+    "bytesReceived",
+    "bytesSent",
+    "processingTime",
 ]
 
 memory_types = [
-    'init',
-    'used',
-    'committed',
-    'max',
+    "init",
+    "used",
+    "committed",
+    "max",
 ]
 
 buffer_pool_types = [
-    'count',
-    'memory-used',
+    "count",
+    "memory-used",
 ]
 
-thread_types = [
-    'thread-count',
-    'daemon-thread-count'
-]
+thread_types = ["thread-count", "daemon-thread-count"]
 
 memory_topics = [
-    'heap-memory-usage',
-    'non-heap-memory-usage',
+    "heap-memory-usage",
+    "non-heap-memory-usage",
 ]
 
 gc_types = [
-    'collection-count',
-    'collection-time',
+    "collection-count",
+    "collection-time",
 ]
 
 
 class JbossApiCollector(diamond.collector.Collector):
-
     def process_config(self):
         super(JbossApiCollector, self).process_config()
-        if self.config['hosts'].__class__.__name__ != 'list':
-            self.config['hosts'] = [self.config['hosts']]
+
+        if self.config["hosts"].__class__.__name__ != "list":
+            self.config["hosts"] = [self.config["hosts"]]
 
         # get the params for each host
-        if 'host' in self.config:
+        if "host" in self.config:
             hoststr = "%s:%s@%s:%s:%s" % (
-                self.config['user'],
-                self.config['password'],
-                self.config['host'],
-                self.config['port'],
-                self.config['proto'],
+                self.config["user"],
+                self.config["password"],
+                self.config["host"],
+                self.config["port"],
+                self.config["proto"],
             )
-            self.config['hosts'].append(hoststr)
+            self.config["hosts"].append(hoststr)
 
-        if type(self.config['connector_options']) is not list:
-            self.config['connector_options'] = [
-                self.config['connector_options']]
+        if type(self.config["connector_options"]) is not list:
+            self.config["connector_options"] = [self.config["connector_options"]]
 
     def get_default_config_help(self):
         # Need to update this when done to help explain details when running
         # diamond-setup.
         config_help = super(JbossApiCollector, self).get_default_config_help()
-        config_help.update({
-            'curl_bin': 'Path to system curl executable',
-            'hosts': 'List of hosts to collect from. Format is yourusername:yourpassword@host:port:proto',  # NOQA
-            'app_stats': 'Collect application pool stats',
-            'jvm_memory_pool_stats': 'Collect JVM memory-pool stats',
-            'jvm_buffer_pool_stats': 'Collect JVM buffer-pool stats',
-            'jvm_memory_stats': 'Collect JVM basic memory stats',
-            'jvm_gc_stats': 'Collect JVM garbage-collector stats',
-            'jvm_thread_stats': 'Collect JVM thread stats',
-            'thread_pool_stats': 'Collect JBoss thread pool stats',
-            'connector_stats': 'Collect HTTP and AJP Connector stats',
-            'connector_options': 'Types of connectors to collect'
-        })
+        config_help.update(
+            {
+                "curl_bin": "Path to system curl executable",
+                "hosts": "List of hosts to collect from. Format is yourusername:yourpassword@host:port:proto",  # NOQA
+                "app_stats": "Collect application pool stats",
+                "jvm_memory_pool_stats": "Collect JVM memory-pool stats",
+                "jvm_buffer_pool_stats": "Collect JVM buffer-pool stats",
+                "jvm_memory_stats": "Collect JVM basic memory stats",
+                "jvm_gc_stats": "Collect JVM garbage-collector stats",
+                "jvm_thread_stats": "Collect JVM thread stats",
+                "thread_pool_stats": "Collect JBoss thread pool stats",
+                "connector_stats": "Collect HTTP and AJP Connector stats",
+                "connector_options": "Types of connectors to collect",
+            }
+        )
         return config_help
 
     def get_default_config(self):
 
         # Initialize default config
         config = super(JbossApiCollector, self).get_default_config()
-        config.update({
-            'path': 'jboss',
-            'curl_bin': '/usr/bin/curl',
-            'connect_timeout': '4',
-            'ssl_options': '--sslv3 -k',
-            'curl_options': '-s --digest -L ',
-            'interface_regex': '^(.+?)\.',  # matches up to first "."
-            'hosts': [],
-            'app_stats': 'True',
-            'connector_options': ['http', 'ajp'],
-            'jvm_memory_pool_stats': 'True',
-            'jvm_buffer_pool_stats': 'True',
-            'jvm_memory_stats': 'True',
-            'jvm_gc_stats': 'True',
-            'jvm_thread_stats': 'True',
-            'connector_stats': 'True',
-            'thread_pool_stats': 'True'
-        })
+        config.update(
+            {
+                "path": "jboss",
+                "curl_bin": "/usr/bin/curl",
+                "connect_timeout": "4",
+                "ssl_options": "--sslv3 -k",
+                "curl_options": "-s --digest -L ",
+                "interface_regex": "^(.+?)\.",  # matches up to first "."
+                "hosts": [],
+                "app_stats": "True",
+                "connector_options": ["http", "ajp"],
+                "jvm_memory_pool_stats": "True",
+                "jvm_buffer_pool_stats": "True",
+                "jvm_memory_stats": "True",
+                "jvm_gc_stats": "True",
+                "jvm_thread_stats": "True",
+                "connector_stats": "True",
+                "thread_pool_stats": "True",
+            }
+        )
         # Return default config
         return config
 
-    def get_stats(self, current_host, current_port, current_proto, current_user,
-                  current_pword):
-
-        if not os.access(self.config['curl_bin'], os.X_OK):
-            self.log.error("%s is not executable or does not exist.",
-                           self.config['curl_bin'])
+    def get_stats(
+        self, current_host, current_port, current_proto, current_user, current_pword
+    ):
+        if not os.access(self.config["curl_bin"], os.X_OK):
+            self.log.error(
+                "%s is not executable or does not exist.", self.config["curl_bin"]
+            )
 
         # Check if there is a RegEx to perform on the interface names
-        if self.config['interface_regex'] != '':
-            interface = self.string_regex(self.config['interface_regex'],
-                                          current_host)
+        if self.config["interface_regex"] != "":
+            interface = self.string_regex(self.config["interface_regex"], current_host)
 
         else:
             # Clean up any possible extra "."'s in the interface, keeps
             # graphite from creating directories
             interface = self.string_fix(current_host)
 
         for op_type in operational_type:
-            output = self.get_data(op_type, current_host, current_port,
-                                   current_proto, current_user, current_pword)
-            if op_type == 'app' and self.config['app_stats'] == 'True':
+            output = self.get_data(
+                op_type,
+                current_host,
+                current_port,
+                current_proto,
+                current_user,
+                current_pword,
+            )
+
+            if op_type == "app" and self.config["app_stats"] == "True":
                 if output:
                     # Grab the pool stats for each Instance
-                    for instance in output['result']['data-source']:
-                        datasource = output['result']['data-source'][instance]
-                        for metric in datasource['statistics']['pool']:
-                            metricName = '%s.%s.%s.statistics.pool.%s' % (
-                                interface, op_type, instance, metric)
-                            metricValue = datasource[
-                                'statistics']['pool'][metric]
-                            self.publish(metricName, float(metricValue))
+                    for instance in output["result"]["data-source"]:
+                        datasource = output["result"]["data-source"][instance]
 
-            if (op_type == 'thread_pool' and
-                    self.config['thread_pool_stats'] == 'True' and output):
+                        for metric in datasource["statistics"]["pool"]:
+                            metric_name = "%s.%s.%s.statistics.pool.%s" % (
+                                interface,
+                                op_type,
+                                instance,
+                                metric,
+                            )
+                            metric_value = datasource["statistics"]["pool"][metric]
+                            self.publish(metric_name, float(metric_value))
+
+            if (
+                op_type == "thread_pool"
+                and self.config["thread_pool_stats"] == "True"
+                and output
+            ):
                 # Grab the stats from each thread pool type
-                for pool_type in output['result']:
-                    if output['result'][pool_type]:
-                        pool_types = output['result'][pool_type]
+                for pool_type in output["result"]:
+                    if output["result"][pool_type]:
+                        pool_types = output["result"][pool_type]
+
                         for pool in pool_types:
                             for metric in pool_types[pool]:
-                                metricName = '%s.%s.%s.%s.statistics.%s' % (
-                                    interface, op_type, pool_type,
-                                    pool, metric)
-                                metricValue = pool_types[pool][metric]
-                                if self.is_number(metricValue):
-                                    self.publish(metricName, float(metricValue))
+                                metric_name = "%s.%s.%s.%s.statistics.%s" % (
+                                    interface,
+                                    op_type,
+                                    pool_type,
+                                    pool,
+                                    metric,
+                                )
+                                metric_value = pool_types[pool][metric]
+
+                                if self.is_number(metric_value):
+                                    self.publish(metric_name, float(metric_value))
 
-            if op_type == 'web' and self.config['connector_stats'] == 'True':
+            if op_type == "web" and self.config["connector_stats"] == "True":
                 if output:
                     # Grab http and ajp info (make these options)
-                    for c_type in self.config['connector_options']:
+                    for c_type in self.config["connector_options"]:
                         for metric in web_stats:
-                            metricName = ('%s.%s.connector.%s.%s' %
-                                          (interface,
-                                           op_type,
-                                           c_type,
-                                           metric))
-                            connector = output['result']['connector']
-                            metricValue = connector[c_type][metric]
-                            self.publish(metricName, float(metricValue))
+                            metric_name = "%s.%s.connector.%s.%s" % (
+                                interface,
+                                op_type,
+                                c_type,
+                                metric,
+                            )
+                            connector = output["result"]["connector"]
+                            metric_value = connector[c_type][metric]
+                            self.publish(metric_name, float(metric_value))
 
-            if op_type == 'jvm':
+            if op_type == "jvm":
                 if output:
-                    if self.config['jvm_memory_pool_stats'] == 'True':
+                    if self.config["jvm_memory_pool_stats"] == "True":
                         # Grab JVM memory pool stats
-                        mempool = output['result']['type']['memory-pool']
-                        for pool_name in mempool['name']:
+                        mempool = output["result"]["type"]["memory-pool"]
+
+                        for pool_name in mempool["name"]:
                             for metric in memory_types:
-                                metricName = ('%s.%s.%s.%s.%s.%s' %
-                                              (interface,
-                                               op_type,
-                                               'memory-pool',
-                                               pool_name,
-                                               'usage',
-                                               metric))
-                                metricValue = mempool['name'][pool_name][
-                                    'usage'][metric]
-                                self.publish(metricName, float(metricValue))
+                                metric_name = "%s.%s.%s.%s.%s.%s" % (
+                                    interface,
+                                    op_type,
+                                    "memory-pool",
+                                    pool_name,
+                                    "usage",
+                                    metric,
+                                )
+                                metric_value = mempool["name"][pool_name]["usage"][
+                                    metric
+                                ]
+                                self.publish(metric_name, float(metric_value))
 
                     # Grab JVM buffer-pool stats
-                    if self.config['jvm_buffer_pool_stats'] == 'True':
-                        bufferpool = output['result']['type']['buffer-pool']
-                        for pool in bufferpool['name']:
+                    if self.config["jvm_buffer_pool_stats"] == "True":
+                        bufferpool = output["result"]["type"]["buffer-pool"]
+
+                        for pool in bufferpool["name"]:
                             for metric in buffer_pool_types:
-                                metricName = ('%s.%s.%s.%s.%s' %
-                                              (interface,
-                                               op_type,
-                                               'buffer-pool',
-                                               pool,
-                                               metric))
-                                metricValue = bufferpool['name'][pool][metric]
-                                self.publish(metricName, float(metricValue))
+                                metric_name = "%s.%s.%s.%s.%s" % (
+                                    interface,
+                                    op_type,
+                                    "buffer-pool",
+                                    pool,
+                                    metric,
+                                )
+                                metric_value = bufferpool["name"][pool][metric]
+                                self.publish(metric_name, float(metric_value))
 
                     # Grab basic memory stats
-                    if self.config['jvm_memory_stats'] == 'True':
+                    if self.config["jvm_memory_stats"] == "True":
                         for mem_type in memory_topics:
                             for metric in memory_types:
-                                metricName = ('%s.%s.%s.%s.%s' %
-                                              (interface,
-                                               op_type,
-                                               'memory',
-                                               mem_type,
-                                               metric))
-                                memory = output['result']['type']['memory']
-                                metricValue = memory[mem_type][metric]
-                                self.publish(metricName, float(metricValue))
+                                metric_name = "%s.%s.%s.%s.%s" % (
+                                    interface,
+                                    op_type,
+                                    "memory",
+                                    mem_type,
+                                    metric,
+                                )
+                                memory = output["result"]["type"]["memory"]
+                                metric_value = memory[mem_type][metric]
+                                self.publish(metric_name, float(metric_value))
 
                     # Grab Garbage collection stats
-                    if self.config['jvm_gc_stats'] == 'True':
-                        garbage = output['result']['type']['garbage-collector']
-                        for gc_name in garbage['name']:
+                    if self.config["jvm_gc_stats"] == "True":
+                        garbage = output["result"]["type"]["garbage-collector"]
+
+                        for gc_name in garbage["name"]:
                             for metric in gc_types:
-                                metricName = ('%s.%s.%s.%s.%s' %
-                                              (interface,
-                                               op_type,
-                                               'garbage-collector',
-                                               gc_name,
-                                               metric))
-                                metricValue = garbage['name'][gc_name][metric]
-                                self.publish(metricName, float(metricValue))
+                                metric_name = "%s.%s.%s.%s.%s" % (
+                                    interface,
+                                    op_type,
+                                    "garbage-collector",
+                                    gc_name,
+                                    metric,
+                                )
+                                metric_value = garbage["name"][gc_name][metric]
+                                self.publish(metric_name, float(metric_value))
 
                     # Grab threading stats
-                    if self.config['jvm_thread_stats'] == 'True':
+                    if self.config["jvm_thread_stats"] == "True":
                         for metric in thread_types:
-                            metricName = ('%s.%s.%s.%s' %
-                                          (interface,
-                                           op_type,
-                                           'threading',
-                                           metric))
-                            threading = output['result']['type']['threading']
-                            metricValue = threading[metric]
-                            self.publish(metricName, float(metricValue))
+                            metric_name = "%s.%s.%s.%s" % (
+                                interface,
+                                op_type,
+                                "threading",
+                                metric,
+                            )
+                            threading = output["result"]["type"]["threading"]
+                            metric_value = threading[metric]
+                            self.publish(metric_name, float(metric_value))
 
         return True
 
-    def get_data(self, op_type, current_host, current_port, current_proto,
-                 current_user, current_pword):
+    def get_data(
+        self,
+        op_type,
+        current_host,
+        current_port,
+        current_proto,
+        current_user,
+        current_pword,
+    ):
         output = {}
-        if op_type == 'app':
-            data = ('{"operation":"read-resource", ' +
-                    '"include-runtime":"true", ' +
-                    '"recursive":"true", ' +
-                    '"address":["subsystem","datasources"]}')
-
-        if op_type == 'thread_pool':
-            data = ('{"operation":"read-resource", "include-runtime":"true", ' +
-                    '"recursive":"true" , "address":["subsystem","threads"]}')
-
-        if op_type == 'web':
-            data = ('{"operation":"read-resource", ' +
-                    '"include-runtime":"true", ' +
-                    '"recursive":"true", ' +
-                    '"address":["subsystem","web"]}')
-
-        if op_type == 'jvm':
-            data = ('{"operation":"read-resource", ' +
-                    '"include-runtime":"true", ' +
-                    '"recursive":"true", ' +
-                    '"address":["core-service","platform-mbean"]}')
-
-        the_cmd = (("%s --connect-timeout %s %s %s %s://%s:%s/management " +
-                    "--header %s -d '%s' -u %s:%s") % (
-            self.config['curl_bin'], self.config['connect_timeout'],
-            self.config['ssl_options'], self.config['curl_options'],
-            current_proto, current_host, current_port, header, data,
-            current_user, current_pword))
+
+        if op_type == "app":
+            data = (
+                '{"operation":"read-resource", '
+                + '"include-runtime":"true", '
+                + '"recursive":"true", '
+                + '"address":["subsystem","datasources"]}'
+            )
+
+        if op_type == "thread_pool":
+            data = (
+                '{"operation":"read-resource", "include-runtime":"true", '
+                + '"recursive":"true" , "address":["subsystem","threads"]}'
+            )
+
+        if op_type == "web":
+            data = (
+                '{"operation":"read-resource", '
+                + '"include-runtime":"true", '
+                + '"recursive":"true", '
+                + '"address":["subsystem","web"]}'
+            )
+
+        if op_type == "jvm":
+            data = (
+                '{"operation":"read-resource", '
+                + '"include-runtime":"true", '
+                + '"recursive":"true", '
+                + '"address":["core-service","platform-mbean"]}'
+            )
+
+        the_cmd = (
+            "%s --connect-timeout %s %s %s %s://%s:%s/management "
+            + "--header %s -d '%s' -u %s:%s"
+        ) % (
+            self.config["curl_bin"],
+            self.config["connect_timeout"],
+            self.config["ssl_options"],
+            self.config["curl_options"],
+            current_proto,
+            current_host,
+            current_port,
+            header,
+            data,
+            current_user,
+            current_pword,
+        )
 
         try:
-            attributes = subprocess.Popen(the_cmd, shell=True,
-                                          stdout=subprocess.PIPE
-                                          ).communicate()[0]
+            attributes = subprocess.Popen(
+                the_cmd, shell=True, stdout=subprocess.PIPE
+            ).communicate()[0]
             output = json.loads(attributes)
-        except Exception, e:
+        except Exception as e:
             self.log.error("JbossApiCollector: There was an exception %s", e)
-            output = ''
+            output = ""
         return output
 
     def is_number(self, value):
-        return (isinstance(value, (int, long, float)) and
-                not isinstance(value, bool))
+        return isinstance(value, (int, float)) and not isinstance(value, bool)
 
     def string_fix(self, s):
         return re.sub(r"[^a-zA-Z0-9_]", "_", s)
 
     def string_regex(self, pattern, s):
         tmp_result = re.match(pattern, s)
         return tmp_result.group(1)
 
     def collect(self):
 
-        for host in self.config['hosts']:
-            matches = re.search(
-                '^([^:]*):([^@]*)@([^:]*):([^:]*):?(.*)', host)
+        for host in self.config["hosts"]:
+            matches = re.search("^([^:]*):([^@]*)@([^:]*):([^:]*):?(.*)", host)
 
             if not matches:
                 continue
 
             current_host = matches.group(3)
             current_port = int(matches.group(4))
             current_proto = matches.group(5)
             current_user = matches.group(1)
             current_pword = matches.group(2)
 
             # Call get_stats for each instance of jboss
-            self.get_stats(current_host, current_port, current_proto,
-                           current_user, current_pword)
+            self.get_stats(
+                current_host, current_port, current_proto, current_user, current_pword
+            )
 
         return True
```

### Comparing `diamond-next-4.0.515/src/collectors/numa/numa.py` & `diamond-next-5.0.0/src/collectors/cpuacct_cgroup/cpuacct_cgroup.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,52 +1,76 @@
-#!/usr/bin/env python
 # coding=utf-8
+
 """
-This class collects data on NUMA utilization
+The CpuAcctCGroupCollector collects CPU Acct metric for cgroups
 
 #### Dependencies
 
-* numactl
+A mounted cgroup fs. Defaults to /sys/fs/cgroup/cpuacct/
 
 """
+
+import os
+
 import diamond.collector
-from subprocess import Popen, PIPE
-from re import compile as re_compile
-import logging
 
-node_re = re_compile('(?P<node>^node \d+ (free|size)): (?P<size>\d+) \MB')
 
+class CpuAcctCgroupCollector(diamond.collector.Collector):
+    def get_default_config_help(self):
+        config_help = super(CpuAcctCgroupCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "path": """Directory path to where cpuacct is located,
+defaults to /sys/fs/cgroup/cpuacct/. Redhat/CentOS/SL use /cgroup"""
+            }
+        )
 
-class NumaCollector(diamond.collector.Collector):
+        return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
-        config = super(NumaCollector, self).get_default_config()
-        config.update({
-            'path':     'numa',
-            'bin':      self.find_binary('numactl'),
-        })
+        config = super(CpuAcctCgroupCollector, self).get_default_config()
+        config.update({"path": "/sys/fs/cgroup/cpuacct/"})
+
         return config
 
     def collect(self):
-        p = Popen([self.config['bin'], '--hardware'], stdout=PIPE, stderr=PIPE)
+        # find all cpuacct.stat files
+        matches = []
 
-        output, errors = p.communicate()
+        for root, dirnames, filenames in os.walk(self.config["path"]):
+            for filename in filenames:
+                if filename == "cpuacct.stat":
+                    # matches will contain a tuple contain path to cpuacct.stat
+                    # and the parent of the stat
+                    parent = root.replace(self.config["path"], "").replace("/", ".")
+
+                    if parent == "":
+                        parent = "system"
+
+                    # If the parent starts with a dot, remove it
+                    if parent[0] == ".":
+                        parent = parent[1:]
+
+                    matches.append((parent, os.path.join(root, filename)))
+
+        # Read utime and stime from cpuacct files
+        results = {}
+
+        for match in matches:
+            results[match[0]] = {}
+            stat_file = open(match[1])
+            elements = [line.split() for line in stat_file]
+
+            for el in elements:
+                results[match[0]][el[0]] = el[1]
+                stat_file.close()
+
+        # create metrics from collected utimes and stimes for cgroups
+        for parent, cpuacct in iter(results.items()):
+            for key, value in iter(cpuacct.items()):
+                metric_name = ".".join([parent, key])
+                self.publish(metric_name, value, metric_type="GAUGE")
 
-        lines = output.split('\n')
-        for line in lines:
-            try:
-                match = node_re.search(line)
-                if match:
-                    logging.debug("Matched: %s %s" %
-                                  (match.group('node'), match.group('size')))
-                    metric_name = "%s_MB" % match.group('node').replace(' ',
-                                                                        '_')
-                    metric_value = int(match.group('size'))
-                    logging.debug("Publishing %s %s" %
-                                  (metric_name, metric_value))
-                    self.publish(metric_name, metric_value)
-            except Exception as e:
-                logging.error('Failed because: %s' % str(e))
-                continue
+        return True
```

### Comparing `diamond-next-4.0.515/src/collectors/users/users.py` & `diamond-next-5.0.0/src/collectors/resqueweb/resqueweb.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,72 +1,66 @@
 # coding=utf-8
 
 """
-Collects the number of users logged in and shells per user
+Collects data for Resque Web
 
 #### Dependencies
 
- * [pyutmp](http://software.clapper.org/pyutmp/)
-or
- * [utmp] (python-utmp on Debian and derivatives)
+ * urllib
 
 """
 
-import diamond.collector
-
-try:
-    from pyutmp import UtmpFile
-except ImportError:
-    UtmpFile = None
-try:
-    from utmp import UtmpRecord
-    import UTMPCONST
-except ImportError:
-    UtmpRecord = None
+import urllib.request
 
+import diamond.collector
 
-class UsersCollector(diamond.collector.Collector):
 
+class ResqueWebCollector(diamond.collector.Collector):
     def get_default_config_help(self):
-        """
-        Returns the default collector help text
-        """
-        config_help = super(UsersCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(ResqueWebCollector, self).get_default_config_help()
+        config_help.update({})
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
-        config = super(UsersCollector, self).get_default_config()
-        config.update({
-            'path':     'users',
-            'utmp':     None,
-        })
+        config = super(ResqueWebCollector, self).get_default_config()
+        config.update(
+            {
+                "host": "localhost",
+                "port": 5678,
+                "path": "resqueweb",
+            }
+        )
         return config
 
     def collect(self):
-        if UtmpFile is None and UtmpRecord is None:
-            self.log.error('Unable to import either pyutmp or python-utmp')
-            return False
-
-        metrics = {}
-        metrics['total'] = 0
-
-        if UtmpFile:
-            for utmp in UtmpFile(path=self.config['utmp']):
-                if utmp.ut_user_process:
-                    metrics[utmp.ut_user] = metrics.get(utmp.ut_user, 0) + 1
-                    metrics['total'] = metrics['total'] + 1
-
-        if UtmpRecord:
-            for utmp in UtmpRecord(fname=self.config['utmp']):
-                if utmp.ut_type == UTMPCONST.USER_PROCESS:
-                    metrics[utmp.ut_user] = metrics.get(utmp.ut_user, 0) + 1
-                    metrics['total'] = metrics['total'] + 1
-
-        for metric_name in metrics.keys():
-            self.publish(metric_name, metrics[metric_name])
+        try:
+            response = urllib.request.urlopen(
+                "http://%s:%s/stats.txt"
+                % (self.config["host"], int(self.config["port"]))
+            )
+        except Exception as e:
+            self.log.error("Could not connect to resque-web: %s", e)
+            return {}
+
+        for data in response.read().split("\n"):
+            if data == "":
+                continue
+
+            item, count = data.strip().split("=")
+
+            try:
+                count = int(count)
+                (item, queue) = item.split(".")
+
+                if item == "resque":
+                    if queue[-1] == "+":
+                        self.publish("%s.total" % queue.replace("+", ""), count)
+                    else:
+                        self.publish("%s.current" % queue, count)
+                else:
+                    self.publish("queue.%s.current" % queue, count)
 
-        return True
+            except Exception as e:
+                self.log.error("Could not parse the queue: %s", e)
```

### Comparing `diamond-next-4.0.515/src/collectors/lmsensors/lmsensors.py` & `diamond-next-5.0.0/src/collectors/lmsensors/lmsensors.py`

 * *Files 9% similar despite different names*

```diff
@@ -14,58 +14,55 @@
 #### Dependencies
 
  * [PySensors](http://pypi.python.org/pypi/PySensors/)
 
 """
 
 import diamond.collector
-from diamond.collector import str_to_bool
+
 try:
     import sensors
+
     sensors  # workaround for pyflakes issue #13
 except ImportError:
     sensors = None
 
 
 class LMSensorsCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(LMSensorsCollector, self).get_default_config_help()
-        config_help.update({
-            'send_zero': 'Send sensor data even when there is no value'
-        })
+        config_help.update(
+            {"send_zero": "Send sensor data even when there is no value"}
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns default collector settings.
         """
         config = super(LMSensorsCollector, self).get_default_config()
-        config.update({
-            'path': 'sensors',
-            'send_zero': False
-        })
+        config.update({"path": "sensors", "send_zero": False})
         return config
 
     def collect(self):
         if sensors is None:
-            self.log.error('Unable to import module sensors')
+            self.log.error("Unable to import module sensors")
             return {}
 
         sensors.init()
+
         try:
             for chip in sensors.iter_detected_chips():
                 for feature in chip:
-                    label = feature.label.replace(' ', '-')
+                    label = feature.label.replace(" ", "-")
                     value = None
+
                     try:
                         value = feature.get_value()
                     except Exception:
-                        if str_to_bool(self.config['send_zero']):
+                        if diamond.collector.str_to_bool(self.config["send_zero"]):
                             value = 0
 
                     if value is not None:
-                        self.publish(".".join([str(chip), label]),
-                                     value,
-                                     precision=2)
+                        self.publish(".".join([str(chip), label]), value, precision=2)
         finally:
             sensors.cleanup()
```

### Comparing `diamond-next-4.0.515/src/collectors/haproxy/haproxy.py` & `diamond-next-5.0.0/src/collectors/haproxy/haproxy.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,185 +1,210 @@
 # coding=utf-8
 
 """
 Collect HAProxy Stats
 
 #### Dependencies
 
- * urlparse
- * urllib2
+ * urllib
 
 """
 
-import re
-import urllib2
 import base64
 import csv
+import re
 import socket
+import urllib.request
+
 import diamond.collector
 
 
 class HAProxyCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(HAProxyCollector, self).get_default_config_help()
-        config_help.update({
-            'method': "Method to use for data collection. Possible values: " +
-                      "http, unix",
-            'url': "Url to stats in csv format",
-            'user': "Username",
-            'pass': "Password",
-            'sock': "Path to admin UNIX-domain socket",
-            'ignore_servers': "Ignore servers, just collect frontend and " +
-                              "backend stats",
-        })
+        config_help.update(
+            {
+                "method": "Method to use for data collection. Possible values: http, unix",
+                "url": "Url to stats in csv format",
+                "user": "Username",
+                "pass": "Password",
+                "sock": "Path to admin UNIX-domain socket",
+                "ignore_servers": "Ignore servers, just collect frontend and backend stats",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(HAProxyCollector, self).get_default_config()
-        config.update({
-            'method':           'http',
-            'path':             'haproxy',
-            'url':              'http://localhost/haproxy?stats;csv',
-            'user':             'admin',
-            'pass':             'password',
-            'sock':             '/var/run/haproxy.sock',
-            'ignore_servers':   False,
-        })
+        config.update(
+            {
+                "method": "http",
+                "path": "haproxy",
+                "url": "http://localhost/haproxy?stats;csv",
+                "user": "admin",
+                "pass": "password",
+                "sock": "/var/run/haproxy.sock",
+                "ignore_servers": False,
+            }
+        )
+
         return config
 
     def _get_config_value(self, section, key):
         if section:
             if section not in self.config:
                 self.log.error("Error: Config section '%s' not found", section)
                 return None
+
             return self.config[section].get(key, self.config[key])
         else:
             return self.config[key]
 
     def http_get_csv_data(self, section=None):
         """
         Request stats from HAProxy Server
         """
         metrics = []
-        req = urllib2.Request(self._get_config_value(section, 'url'))
+        req = urllib.request.Request(self._get_config_value(section, "url"))
+
         try:
-            handle = urllib2.urlopen(req)
+            handle = urllib.request.urlopen(req)
+
             return handle.readlines()
-        except Exception, e:
-            if not hasattr(e, 'code') or e.code != 401:
+        except Exception as e:
+            if not hasattr(e, "code") or e.code != 401:
                 self.log.error("Error retrieving HAProxy stats. %s", e)
+
                 return metrics
 
         # get the www-authenticate line from the headers
         # which has the authentication scheme and realm in it
-        authline = e.headers['www-authenticate']
+        authline = e.headers["www-authenticate"]
 
         # this regular expression is used to extract scheme and realm
-        authre = (r'''(?:\s*www-authenticate\s*:)?\s*''' +
-                  '''(\w*)\s+realm=['"]([^'"]+)['"]''')
+        authre = r"""(?:\s*www-authenticate\s*:)?\s*(\w*)\s+realm=['"]([^'"]+)['"]"""
         authobj = re.compile(authre, re.IGNORECASE)
         matchobj = authobj.match(authline)
+
         if not matchobj:
             # if the authline isn't matched by the regular expression
             # then something is wrong
-            self.log.error('The authentication header is malformed.')
+            self.log.error("The authentication header is malformed.")
+
             return metrics
 
         scheme = matchobj.group(1)
-        # here we've extracted the scheme
-        # and the realm from the header
-        if scheme.lower() != 'basic':
-            self.log.error('Invalid authentication scheme.')
+
+        # here we've extracted the scheme and the realm from the header
+        if scheme.lower() != "basic":
+            self.log.error("Invalid authentication scheme.")
+
             return metrics
 
-        base64string = base64.encodestring(
-            '%s:%s' % (self._get_config_value(section, 'user'),
-                       self._get_config_value(section, 'pass')))[:-1]
-        authheader = 'Basic %s' % base64string
+        base64string = base64.b64encode(
+            bytes(
+                "%s:%s"
+                % (
+                    self._get_config_value(section, "user"),
+                    self._get_config_value(section, "pass"),
+                ),
+                "utf-8",
+            )
+        )
+        authheader = "Basic %s" % base64string
         req.add_header("Authorization", authheader)
+
         try:
-            handle = urllib2.urlopen(req)
+            handle = urllib.request.urlopen(req)
             metrics = handle.readlines()
+
             return metrics
-        except IOError, e:
+        except IOError as e:
             # here we shouldn't fail if the USER/PASS is right
-            self.log.error("Error retrieving HAProxy stats. " +
-                           "(Invalid username or password?) %s", e)
+            self.log.error(
+                "Error retrieving HAProxy stats. (Invalid username or password?) %s", e
+            )
+
             return metrics
 
     def unix_get_csv_data(self):
         sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
         data = str()
 
         try:
-            sock.connect(self.config['sock'])
-            sock.send('show stat\n')
+            sock.connect(self.config["sock"])
+            sock.send(b"show stat\n")
+
             while 1:
                 buf = sock.recv(4096)
+
                 if not buf:
                     break
+
                 data += buf
-        except socket.error, e:
+        except socket.error as e:
             self.log.error("Error retrieving HAProxy stats. %s", e)
+
             return []
 
-        return data.strip().split('\n')
+        return data.strip().split("\n")
 
     def _generate_headings(self, row):
         headings = {}
+
         for index, heading in enumerate(row):
             headings[index] = self._sanitize(heading)
+
         return headings
 
     def _collect(self, section=None):
         """
         Collect HAProxy Stats
         """
-        if self.config['method'] == 'http':
+        if self.config["method"] == "http":
             csv_data = self.http_get_csv_data(section)
-        elif self.config['method'] == 'unix':
+        elif self.config["method"] == "unix":
             csv_data = self.unix_get_csv_data()
         else:
-            self.log.error("Unknown collection method: %s",
-                           self.config['method'])
+            self.log.error("Unknown collection method: %s", self.config["method"])
             csv_data = []
 
         data = list(csv.reader(csv_data))
         headings = self._generate_headings(data[0])
-        section_name = section and self._sanitize(section.lower()) + '.' or ''
+        section_name = section and self._sanitize(section.lower()) + "." or ""
 
         for row in data:
-            if ((self._get_config_value(section, 'ignore_servers') and
-                 row[1].lower() not in ['frontend', 'backend'])):
+            if self._get_config_value(section, "ignore_servers") and row[
+                1
+            ].lower() not in ["frontend", "backend"]:
                 continue
 
             part_one = self._sanitize(row[0].lower())
             part_two = self._sanitize(row[1].lower())
-            metric_name = '%s%s.%s' % (section_name, part_one, part_two)
+            metric_name = "%s%s.%s" % (section_name, part_one, part_two)
 
             for index, metric_string in enumerate(row):
                 try:
                     metric_value = float(metric_string)
                 except ValueError:
                     continue
 
-                stat_name = '%s.%s' % (metric_name, headings[index])
-                self.publish(stat_name, metric_value, metric_type='GAUGE')
+                stat_name = "%s.%s" % (metric_name, headings[index])
+                self.publish(stat_name, metric_value, metric_type="GAUGE")
 
     def collect(self):
-        if 'servers' in self.config:
-            if isinstance(self.config['servers'], list):
-                for serv in self.config['servers']:
+        if "servers" in self.config:
+            if isinstance(self.config["servers"], list):
+                for serv in self.config["servers"]:
                     self._collect(serv)
             else:
-                self._collect(self.config['servers'])
+                self._collect(self.config["servers"])
         else:
             self._collect()
 
     def _sanitize(self, s):
-        """Sanitize the name of a metric to remove unwanted chars
-        """
-        return re.sub('[^\w-]', '_', s)
+        """Sanitize the name of a metric to remove unwanted chars"""
+        return re.sub("[^w-]", "_", s)
```

### Comparing `diamond-next-4.0.515/src/collectors/kafkastat/kafkastat.py` & `diamond-next-5.0.0/src/collectors/kafkastat/kafkastat.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,175 +1,212 @@
 # coding=utf-8
 
 """
 Collect stats via MX4J from Kafka
 
 #### Dependencies
 
- * urllib2
+ * urllib
  * xml.etree
 """
-import urllib2
 
-from urllib import urlencode
+import urllib.error
+import urllib.parse
+import urllib.request
+
+import diamond.collector
 
 try:
     from xml.etree import ElementTree
 except ImportError:
     ElementTree = None
 
 try:
     from ElementTree import ParseError as ETParseError
 except ImportError:
     ETParseError = Exception
 
-import diamond.collector
-
 
 class KafkaCollector(diamond.collector.Collector):
     ATTRIBUTE_TYPES = {
-        'double': float,
-        'float': float,
-        'int': int,
-        'long': long,
+        "double": float,
+        "float": float,
+        "int": int,
+        "java.lang.Object": float,
     }
 
     def get_default_config_help(self):
         config_help = super(KafkaCollector, self).get_default_config_help()
-        config_help.update({
-            'host': "",
-            'port': "",
-        })
+        config_help.update(
+            {
+                "host": "",
+                "port": "",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(KafkaCollector, self).get_default_config()
-        config.update({
-            'host': '127.0.0.1',
-            'port': 8082,
-            'path': 'kafka',
-        })
+        config.update(
+            {
+                "host": "127.0.0.1",
+                "port": 8082,
+                "path": "kafka",
+            }
+        )
+
         return config
 
     def _get(self, path, query_args=None):
-        if not path.startswith('/'):
-            path = '/' + path
+        if not path.startswith("/"):
+            path = "/" + path
 
-        qargs = {'template': 'identity'}
+        qargs = {"template": "identity"}
 
         if query_args:
             qargs.update(query_args)
 
-        url = 'http://%s:%i%s?%s' % (
-            self.config['host'], int(self.config['port']),
-            path, urlencode(qargs))
+        url = "http://%s:%i%s?%s" % (
+            self.config["host"],
+            int(self.config["port"]),
+            path,
+            urllib.parse.urlencode(qargs),
+        )
 
         try:
-            response = urllib2.urlopen(url)
-        except urllib2.URLError, err:
+            response = urllib.request.urlopen(url)
+        except urllib.error.URLError as err:
             self.log.error("%s: %s", url, err)
+
             return None
 
         try:
             return ElementTree.fromstring(response.read())
         except ETParseError:
             self.log.error("Unable to parse response from mx4j")
+
             return None
 
     def get_mbeans(self, pattern):
-        query_args = {'querynames': pattern}
+        query_args = {"querynames": pattern}
+
+        mbeans = self._get("/serverbydomain", query_args)
 
-        mbeans = self._get('/serverbydomain', query_args)
         if mbeans is None:
             return
 
         found_beans = set()
 
-        for mbean in mbeans.getiterator(tag='MBean'):
-            objectname = mbean.get('objectname')
+        for mbean in list(mbeans.iter(tag="MBean")):
+            objectname = mbean.get("objectname")
 
             if objectname:
                 found_beans.add(objectname)
 
         return found_beans
 
     def query_mbean(self, objectname, key_prefix=None):
         query_args = {
-            'objectname': objectname,
-            'operations': False,
-            'constructors': False,
-            'notifications': False,
+            "objectname": objectname,
+            "operations": False,
+            "constructors": False,
+            "notifications": False,
         }
 
-        attributes = self._get('/mbean', query_args)
+        attributes = self._get("/mbean", query_args)
+
         if attributes is None:
             return
 
         if key_prefix is None:
             # Could be 1 or 2 = in the string
             # java.lang:type=Threading
             # "kafka.controller":type="ControllerStats",
             # name="LeaderElectionRateAndTimeMs"
-            split_num = objectname.count('=')
+            split_num = objectname.count("=")
+
             for i in range(split_num):
                 if i == 0:
-                    key_prefix = objectname.split('=')[1]
+                    key_prefix = objectname.split("=")[1]
+
                     if '"' in key_prefix:
                         key_prefix = key_prefix.split('"')[1]
+
                     if "," in key_prefix:
-                        key_prefix = key_prefix.split(',')[0]
+                        key_prefix = key_prefix.split(",")[0]
                 elif i > 0:
-                    key = objectname.split('=')[2]
+                    key = objectname.split("=")[i + 1]
+
                     if key:
                         if '"' in key:
                             key = key.split('"')[1]
-                        key_prefix = key_prefix + '.' + key
+
+                        key_prefix = key_prefix + "." + key
+                        key_prefix = key_prefix.replace(",", ".")
 
         metrics = {}
 
-        for attrib in attributes.getiterator(tag='Attribute'):
-            atype = attrib.get('type')
+        for attrib in list(attributes.iter(tag="Attribute")):
+            atype = attrib.get("type")
 
             ptype = self.ATTRIBUTE_TYPES.get(atype)
+
             if not ptype:
                 continue
 
-            value = ptype(attrib.get('value'))
+            try:
+                value = ptype(attrib.get("value"))
+            except ValueError:
+                # It will be too busy, so not logging it every time
+                self.log.debug(
+                    "Unable to parse the value for " + atype + " in " + objectname
+                )
+                continue
+
+            name = ".".join([key_prefix, attrib.get("name")])
 
-            name = '.'.join([key_prefix, attrib.get('name')])
-            # Some prefixes and attributes could have spaces, thus we must
-            # sanitize them
-            name = name.replace(' ', '')
+            # Some prefixes and attributes could have spaces, thus we must sanitize them
+            name = name.replace(" ", "")
 
             metrics[name] = value
 
         return metrics
 
     def collect(self):
         if ElementTree is None:
-            self.log.error('Failed to import xml.etree.ElementTree')
+            self.log.error("Failed to import xml.etree.ElementTree")
+
             return
 
         # Get list of gatherable stats
         query_list = [
-            '*kafka*:*',
-            'java.lang:type=GarbageCollector,name=*',
-            'java.lang:type=Threading'
+            "*kafka*:*",
+            "java.lang:type=GarbageCollector,name=*",
+            "java.lang:type=Threading",
         ]
         mbeans = set()
+
         for pattern in query_list:
             match = self.get_mbeans(pattern)
             mbeans.update(match)
 
         metrics = {}
 
         # Query each one for stats
         for mbean in mbeans:
+            if mbean is None:
+                continue
+
             stats = self.query_mbean(mbean)
+
+            if stats is None:
+                self.log.error("Failed to get stats for" + mbean)
+
             metrics.update(stats)
 
         # Publish stats
-        for metric, value in metrics.iteritems():
+        for metric, value in iter(metrics.items()):
             self.publish(metric, value)
```

### Comparing `diamond-next-4.0.515/src/collectors/exim/exim.py` & `diamond-next-5.0.0/src/collectors/exim/exim.py`

 * *Files 20% similar despite different names*

```diff
@@ -5,59 +5,62 @@
 
 #### Dependencies
 
  * /usr/sbin/exim
 
 """
 
-import diamond.collector
-import subprocess
 import os
-from diamond.collector import str_to_bool
+import subprocess
+
+import diamond.collector
 
 
 class EximCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(EximCollector, self).get_default_config_help()
-        config_help.update({
-            'bin':         'The path to the exim binary',
-            'use_sudo':    'Use sudo?',
-            'sudo_cmd':    'Path to sudo',
-            'sudo_user':   'User to sudo as',
-        })
+        config_help.update(
+            {
+                "bin": "The path to the exim binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+                "sudo_user": "User to sudo as",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(EximCollector, self).get_default_config()
-        config.update({
-            'path':            'exim',
-            'bin':              '/usr/sbin/exim',
-            'use_sudo':         False,
-            'sudo_cmd':         '/usr/bin/sudo',
-            'sudo_user':        'root',
-        })
+        config.update(
+            {
+                "path": "exim",
+                "bin": "/usr/sbin/exim",
+                "use_sudo": False,
+                "sudo_cmd": "/usr/bin/sudo",
+                "sudo_user": "root",
+            }
+        )
         return config
 
     def collect(self):
-        if not os.access(self.config['bin'], os.X_OK):
+        if not os.access(self.config["bin"], os.X_OK):
             return
 
-        command = [self.config['bin'], '-bpc']
+        command = [self.config["bin"], "-bpc"]
 
-        if str_to_bool(self.config['use_sudo']):
-            command = [
-                self.config['sudo_cmd'],
-                '-u',
-                self.config['sudo_user']
-            ].extend(command)
-
-        queuesize = subprocess.Popen(
-            command, stdout=subprocess.PIPE).communicate()[0].split()
+        if diamond.collector.str_to_bool(self.config["use_sudo"]):
+            command = [self.config["sudo_cmd"], "-u", self.config["sudo_user"]].extend(
+                command
+            )
+
+        queuesize = (
+            subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0].split()
+        )
 
         if not len(queuesize):
             return
+
         queuesize = queuesize[-1]
-        self.publish('queuesize', queuesize)
+        self.publish("queuesize", queuesize)
```

### Comparing `diamond-next-4.0.515/src/collectors/ping/ping.py` & `diamond-next-5.0.0/src/collectors/ping/ping.py`

 * *Files 17% similar despite different names*

```diff
@@ -25,56 +25,60 @@
 diamond-setup --print -C PingCollector
 
 You should get a response back that indicates 'enabled': True and see entries
 for your targets in pairs like:
 
 'target_1': 'example.org'
 
-We extract out the key after target_ and use it in the graphite node we push.
+The graphite nodes pushed are derived from the pinged hostnames by replacing all
+dots with underscores, i.e. 'www.example.org' becomes 'www_example_org'.
 
 """
 
 import diamond.collector
 
 
 class PingCollector(diamond.collector.ProcessCollector):
-
     def get_default_config_help(self):
         config_help = super(PingCollector, self).get_default_config_help()
-        config_help.update({
-            'bin':         'The path to the ping binary',
-        })
+        config_help.update(
+            {
+                "bin": "The path to the ping binary",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(PingCollector, self).get_default_config()
-        config.update({
-            'path':             'ping',
-            'bin':              '/bin/ping',
-        })
+        config.update(
+            {
+                "path": "ping",
+                "bin": "/bin/ping",
+            }
+        )
         return config
 
     def collect(self):
         for key in self.config.keys():
             if key[:7] == "target_":
                 host = self.config[key]
-                metric_name = host.replace('.', '_')
+                metric_name = host.replace(".", "_")
 
-                ping = self.run_command(['-nq', '-c 1', host])
+                ping = self.run_command(["-nq", "-c 1", host])
                 ping = ping[0].strip().split("\n")[-1]
 
                 # Linux
-                if ping.startswith('rtt'):
-                    ping = ping.split()[3].split('/')[0]
+                if ping.startswith("rtt"):
+                    ping = ping.split()[3].split("/")[0]
                     metric_value = float(ping)
                 # OS X
-                elif ping.startswith('round-trip '):
-                    ping = ping.split()[3].split('/')[0]
+                elif ping.startswith("round-trip "):
+                    ping = ping.split()[3].split("/")[0]
                     metric_value = float(ping)
                 # Unknown
                 else:
                     metric_value = 10000
 
                 self.publish(metric_name, metric_value, precision=3)
```

### Comparing `diamond-next-4.0.515/src/collectors/zookeeper/zookeeper.py` & `diamond-next-5.0.0/src/collectors/zookeeper/zookeeper.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,130 +20,141 @@
 TO use a unix socket, set a host string like this
 
 ```
     hosts = /path/to/blah.sock, app-1@/path/to/bleh.sock,
 ```
 """
 
-import diamond.collector
-import socket
 import re
+import socket
+
+import diamond.collector
 
 
 class ZookeeperCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(ZookeeperCollector, self).get_default_config_help()
-        config_help.update({
-            'publish':
-                "Which rows of 'status' you would like to publish." +
-                " Telnet host port' and type stats and hit enter to see the " +
-                " list of possibilities. Leave unset to publish all.",
-            'hosts':
-                "List of hosts, and ports to collect. Set an alias by " +
-                " prefixing the host:port with alias@",
-        })
+        config_help.update(
+            {
+                "publish": "Which rows of 'status' you would like to publish."
+                + " Telnet host port' and type stats and hit enter to see the "
+                + " list of possibilities. Leave unset to publish all.",
+                "hosts": "List of hosts, and ports to collect. Set an alias by "
+                + " prefixing the host:port with alias@",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ZookeeperCollector, self).get_default_config()
-        config.update({
-            'path':     'zookeeper',
+        config.update(
+            {
+                "path": "zookeeper",
+                # Which rows of 'status' you would like to publish.
+                # 'telnet host port' and type mntr and hit enter to see the list of
+                # possibilities.
+                # Leave unset to publish all
+                # 'publish': ''
+                # Connection settings
+                "hosts": ["localhost:2181"],
+            }
+        )
 
-            # Which rows of 'status' you would like to publish.
-            # 'telnet host port' and type mntr and hit enter to see the list of
-            # possibilities.
-            # Leave unset to publish all
-            # 'publish': ''
-
-            # Connection settings
-            'hosts': ['localhost:2181']
-        })
         return config
 
     def get_raw_stats(self, host, port):
-        data = ''
+        data = ""
+
         # connect
         try:
             if port is None:
                 sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                 sock.connect(host)
             else:
                 sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                 sock.connect((host, int(port)))
+
             # request stats
-            sock.send('mntr\n')
+            sock.send("mntr\n")
+
             # something big enough to get whatever is sent back
             data = sock.recv(4096)
         except socket.error:
-            self.log.exception('Failed to get stats from %s:%s',
-                               host, port)
+            self.log.exception("Failed to get stats from %s:%s", host, port)
+
         return data
 
     def get_stats(self, host, port):
         # stuff that's always ignored, aren't 'stats'
-        ignored = ('zk_version', 'zk_server_state')
+        ignored = ("zk_version", "zk_server_state")
         pid = None
 
         stats = {}
         data = self.get_raw_stats(host, port)
 
         # parse stats
         for line in data.splitlines():
 
             pieces = line.split()
 
             if pieces[0] in ignored:
                 continue
+
             stats[pieces[0]] = pieces[1]
 
         # get max connection limit
-        self.log.debug('pid %s', pid)
+        self.log.debug("pid %s", pid)
+
         try:
             cmdline = "/proc/%s/cmdline" % pid
-            f = open(cmdline, 'r')
+            f = open(cmdline, "r")
             m = re.search("-c\x00(\d+)", f.readline())
+
             if m is not None:
-                self.log.debug('limit connections %s', m.group(1))
-                stats['limit_maxconn'] = m.group(1)
+                self.log.debug("limit connections %s", m.group(1))
+                stats["limit_maxconn"] = m.group(1)
+
             f.close()
         except:
             self.log.debug("Cannot parse command line options for zookeeper")
 
         return stats
 
     def collect(self):
-        hosts = self.config.get('hosts')
+        hosts = self.config.get("hosts")
 
         # Convert a string config value to be an array
-        if isinstance(hosts, basestring):
+        if isinstance(hosts, str):
             hosts = [hosts]
 
         for host in hosts:
-            matches = re.search('((.+)\@)?([^:]+)(:(\d+))?', host)
+            matches = re.search("((.+)\@)?([^:]+)(:(\d+))?", host)
             alias = matches.group(2)
             hostname = matches.group(3)
             port = matches.group(5)
 
             stats = self.get_stats(hostname, port)
 
             # figure out what we're configured to get, defaulting to everything
-            desired = self.config.get('publish', stats.keys())
+            desired = self.config.get("publish", stats.keys())
 
             # for everything we want
             for stat in desired:
                 if stat in stats:
 
                     # we have it
                     if alias is not None:
                         self.publish(alias + "." + stat, stats[stat])
                     else:
                         self.publish(stat, stats[stat])
                 else:
 
                     # we don't, must be somehting configured in publish so we
                     # should log an error about it
-                    self.log.error("No such key '%s' available, issue 'stats' "
-                                   "for a full list", stat)
+                    self.log.error(
+                        "No such key '%s' available, issue 'stats' for a full list",
+                        stat,
+                    )
```

### Comparing `diamond-next-4.0.515/src/collectors/nagiosperfdata/nagiosperfdata.py` & `diamond-next-5.0.0/src/collectors/nagiosperfdata/nagiosperfdata.py`

 * *Files 4% similar despite different names*

```diff
@@ -66,83 +66,81 @@
 import os
 import re
 
 import diamond.collector
 
 
 class NagiosPerfdataCollector(diamond.collector.Collector):
-    """Diamond collector for Nagios performance data
-    """
+    """Diamond collector for Nagios performance data"""
 
-    GENERIC_FIELDS = ['DATATYPE', 'HOSTNAME', 'TIMET']
-    HOST_FIELDS = ['HOSTPERFDATA']
-    SERVICE_FIELDS = ['SERVICEDESC', 'SERVICEPERFDATA']
+    GENERIC_FIELDS = ["DATATYPE", "HOSTNAME", "TIMET"]
+    HOST_FIELDS = ["HOSTPERFDATA"]
+    SERVICE_FIELDS = ["SERVICEDESC", "SERVICEPERFDATA"]
     TOKENIZER_RE = (
-        r"([^\s]+|'[^']+')=([-.\d]+)(c|s|ms|us|B|KB|MB|GB|TB|%)?" +
-        r"(?:;([-.\d]+))?(?:;([-.\d]+))?(?:;([-.\d]+))?(?:;([-.\d]+))?")
+        r"([^\s]+|'[^']+')=([-.\d]+)(c|s|ms|us|B|KB|MB|GB|TB|%)?"
+        + r"(?:;([-.\d]+))?(?:;([-.\d]+))?(?:;([-.\d]+))?(?:;([-.\d]+))?"
+    )
 
     def get_default_config_help(self):
-        config_help = super(NagiosPerfdataCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'perfdata_dir': 'The directory containing Nagios perfdata files'
-        })
+        config_help = super(NagiosPerfdataCollector, self).get_default_config_help()
+        config_help.update(
+            {"perfdata_dir": "The directory containing Nagios perfdata files"}
+        )
         return config_help
 
     def get_default_config(self):
         config = super(NagiosPerfdataCollector, self).get_default_config()
-        config.update({
-            'path': 'nagiosperfdata',
-            'perfdata_dir': '/var/spool/diamond/nagiosperfdata',
-        })
+        config.update(
+            {
+                "path": "nagiosperfdata",
+                "perfdata_dir": "/var/spool/diamond/nagiosperfdata",
+            }
+        )
         return config
 
     def collect(self):
-        """Collect statistics from a Nagios perfdata directory.
-        """
-        perfdata_dir = self.config['perfdata_dir']
+        """Collect statistics from a Nagios perfdata directory."""
+        perfdata_dir = self.config["perfdata_dir"]
 
         try:
             filenames = os.listdir(perfdata_dir)
         except OSError:
-            self.log.error("Cannot read directory `{dir}'".format(
-                dir=perfdata_dir))
+            self.log.error("Cannot read directory `{dir}'".format(dir=perfdata_dir))
             return
 
         for filename in filenames:
             self._process_file(os.path.join(perfdata_dir, filename))
 
     def _extract_fields(self, line):
-        """Extract the key/value fields from a line of performance data
-        """
+        """Extract the key/value fields from a line of performance data"""
         acc = {}
         field_tokens = line.split("\t")
         for field_token in field_tokens:
-            kv_tokens = field_token.split('::')
+            kv_tokens = field_token.split("::")
             if len(kv_tokens) == 2:
                 (key, value) = kv_tokens
                 acc[key] = value
 
         return acc
 
     def _fields_valid(self, d):
         """Verify that all necessary fields are present
 
         Determine whether the fields parsed represent a host or
         service perfdata. If the perfdata is unknown, return False.
         If the perfdata does not contain all fields required for that
         type, return False. Otherwise, return True.
         """
-        if 'DATATYPE' not in d:
+        if "DATATYPE" not in d:
             return False
 
-        datatype = d['DATATYPE']
-        if datatype == 'HOSTPERFDATA':
+        datatype = d["DATATYPE"]
+        if datatype == "HOSTPERFDATA":
             fields = self.GENERIC_FIELDS + self.HOST_FIELDS
-        elif datatype == 'SERVICEPERFDATA':
+        elif datatype == "SERVICEPERFDATA":
             fields = self.GENERIC_FIELDS + self.SERVICE_FIELDS
         else:
             return False
 
         for field in fields:
             if field not in d:
                 return False
@@ -152,98 +150,100 @@
     def _normalize_to_unit(self, value, unit):
         """Normalize the value to the unit returned.
 
         We use base-1000 for second-based units, and base-1024 for
         byte-based units. Sadly, the Nagios-Plugins specification doesn't
         disambiguate base-1000 (KB) and base-1024 (KiB).
         """
-        if unit == 'ms':
+        if unit == "ms":
             return value / 1000.0
-        if unit == 'us':
+        if unit == "us":
             return value / 1000000.0
-        if unit == 'KB':
+        if unit == "KB":
             return value * 1024
-        if unit == 'MB':
+        if unit == "MB":
             return value * 1024 * 1024
-        if unit == 'GB':
+        if unit == "GB":
             return value * 1024 * 1024 * 1024
-        if unit == 'TB':
+        if unit == "TB":
             return value * 1024 * 1024 * 1024 * 1024
 
         return value
 
     def _parse_perfdata(self, s):
-        """Parse performance data from a perfdata string
-        """
+        """Parse performance data from a perfdata string"""
         metrics = []
         counters = re.findall(self.TOKENIZER_RE, s)
+
         if counters is None:
-            self.log.warning("Failed to parse performance data: {s}".format(
-                s=s))
+            self.log.warning("Failed to parse performance data: {s}".format(s=s))
             return metrics
 
         for (key, value, uom, warn, crit, min, max) in counters:
             try:
                 norm_value = self._normalize_to_unit(float(value), uom)
                 metrics.append((key, norm_value))
             except ValueError:
                 self.log.warning(
-                    "Couldn't convert value '{value}' to float".format(
-                        value=value))
+                    "Couldn't convert value '{value}' to float".format(value=value)
+                )
 
         return metrics
 
     def _process_file(self, path):
-        """Parse and submit the metrics from a file
-        """
+        """Parse and submit the metrics from a file"""
         try:
             f = open(path)
             for line in f:
                 self._process_line(line)
 
             os.remove(path)
-        except IOError, ex:
-            self.log.error("Could not open file `{path}': {error}".format(
-                path=path, error=ex.strerror))
+        except IOError as ex:
+            self.log.error(
+                "Could not open file `{path}': {error}".format(
+                    path=path, error=ex.strerror
+                )
+            )
 
     def _process_line(self, line):
-        """Parse and submit the metrics from a line of perfdata output
-        """
+        """Parse and submit the metrics from a line of perfdata output"""
         fields = self._extract_fields(line)
+
         if not self._fields_valid(fields):
-            self.log.warning("Missing required fields for line: {line}".format(
-                line=line))
+            self.log.warning(
+                "Missing required fields for line: {line}".format(line=line)
+            )
 
         metric_path_base = []
-        graphite_prefix = fields.get('GRAPHITEPREFIX')
-        graphite_postfix = fields.get('GRAPHITEPOSTFIX')
+        graphite_prefix = fields.get("GRAPHITEPREFIX")
+        graphite_postfix = fields.get("GRAPHITEPOSTFIX")
 
         if graphite_prefix:
             metric_path_base.append(graphite_prefix)
 
-        hostname = fields['HOSTNAME'].lower()
+        hostname = fields["HOSTNAME"].lower()
         metric_path_base.append(hostname)
+        datatype = fields["DATATYPE"]
+
+        if datatype == "HOSTPERFDATA":
+            metric_path_base.append("host")
+        elif datatype == "SERVICEPERFDATA":
+            service_desc = fields.get("SERVICEDESC")
+            graphite_postfix = fields.get("GRAPHITEPOSTFIX")
 
-        datatype = fields['DATATYPE']
-        if datatype == 'HOSTPERFDATA':
-            metric_path_base.append('host')
-        elif datatype == 'SERVICEPERFDATA':
-            service_desc = fields.get('SERVICEDESC')
-            graphite_postfix = fields.get('GRAPHITEPOSTFIX')
             if graphite_postfix:
                 metric_path_base.append(graphite_postfix)
             else:
                 metric_path_base.append(service_desc)
 
         perfdata = fields[datatype]
         counters = self._parse_perfdata(perfdata)
 
         for (counter, value) in counters:
             metric_path = metric_path_base + [counter]
             metric_path = [self._sanitize(x) for x in metric_path]
-            metric_name = '.'.join(metric_path)
+            metric_name = ".".join(metric_path)
             self.publish(metric_name, value)
 
     def _sanitize(self, s):
-        """Sanitize the name of a metric to remove unwanted chars
-        """
+        """Sanitize the name of a metric to remove unwanted chars"""
         return re.sub("[^\w-]", "_", s)
```

### Comparing `diamond-next-4.0.515/src/collectors/vmsdoms/vmsdoms.py` & `diamond-next-5.0.0/src/collectors/vmsdoms/vmsdoms.py`

 * *Files 27% similar despite different names*

```diff
@@ -14,52 +14,51 @@
 # in /etc/sysconfig/vms. VMS_GROUP may not be defined in which case it
 # defaults to the login group of VMS_USER. If you are running diamond
 # as root, no worries. In most other cases:
 #     usermod -G kvm diamond
 # should suffice
 
 import diamond.collector
+
 try:
     import vms
 except ImportError:
     vms = None
 
 
 class VMSDomsCollector(diamond.collector.Collector):
     PLUGIN_STATS = {
-        'nominal': ('pages', 4096),
-        'current': ('memory.current', 4096),
-        'clean': ('memory.clean', 4096),
-        'dirty': ('memory.dirty', 4096),
-        'limit': ('memory.limit', 4096),
-        'target': ('memory.target', 4096),
-        'evicted': ('eviction.dropped', 4096),
-        'pagedout': ('eviction.pagedout', 4096),
-        'pagedin': ('eviction.pagedin', 4096),
+        "nominal": ("pages", 4096),
+        "current": ("memory.current", 4096),
+        "clean": ("memory.clean", 4096),
+        "dirty": ("memory.dirty", 4096),
+        "limit": ("memory.limit", 4096),
+        "target": ("memory.target", 4096),
+        "evicted": ("eviction.dropped", 4096),
+        "pagedout": ("eviction.pagedout", 4096),
+        "pagedin": ("eviction.pagedin", 4096),
     }
 
     def get_default_config_help(self):
         config_help = super(VMSDomsCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(VMSDomsCollector, self).get_default_config()
-        config.update({
-            'path':     'vms'
-        })
+        config.update({"path": "vms"})
         return config
 
     def collect(self):
         if vms is None:
-            self.log.error('Unable to import vms')
+            self.log.error("Unable to import vms")
             return {}
 
         vms.virt.init()
         hypervisor = vms.virt.AUTO.Hypervisor()
 
         # Get list of domains and iterate.
         domains = hypervisor.domain_list()
@@ -70,44 +69,47 @@
         for d in domains:
             # Skip non-VMS domains.
             if not vms.control.exists(d):
                 continue
 
             # Grab a control connection.
             dom = hypervisor.domain_lookup(d)
+
             if dom is None:
                 continue
+
             ctrl = dom._wait_for_control(wait=False)
+
             if ctrl is None:
                 continue
 
             try:
                 # Skip ghost domains.
-                if ctrl.get('gd.isghost') == '1':
+                if ctrl.get("gd.isghost") == "1":
                     continue
             except vms.control.ControlException:
                 continue
 
             vms_domains.append((dom, ctrl))
             count += 1
 
         # Add the number of domains.
-        self.publish('domains', count)
+        self.publish("domains", count)
 
         # For each stat,
         for stat in self.PLUGIN_STATS:
             key = self.PLUGIN_STATS[stat][0]
             scale = self.PLUGIN_STATS[stat][1]
             total = 0
 
             # For each domain,
             for dom, ctrl in vms_domains:
                 try:
                     # Get value and scale.
-                    value = long(ctrl.get(key)) * scale
+                    value = int(ctrl.get(key)) * scale
                 except vms.control.ControlException:
                     continue
 
                 # Dispatch.
                 self.publish(stat, value, instance=dom.name())
 
                 # Add to total.
```

### Comparing `diamond-next-4.0.515/src/collectors/onewire/onewire.py` & `diamond-next-5.0.0/src/collectors/onewire/onewire.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,68 +18,72 @@
 #### Dependencies
 
  * owfs
 
 """
 
 import os
+
 import diamond.collector
 
 
 class OneWireCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(OneWireCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(OneWireCollector, self).get_default_config()
-        config.update({
-            'path': 'owfs',
-            'owfs': '/mnt/1wire',
-            # 'scan': {'temperature': 't'},
-            # 'id:24.BB000000': {'file_with_value': 'alias'},
-        })
+        config.update(
+            {
+                "path": "owfs",
+                "owfs": "/mnt/1wire",
+                # 'scan': {'temperature': 't'},
+                # 'id:24.BB000000': {'file_with_value': 'alias'},
+            }
+        )
+
         return config
 
     def collect(self):
         """
         Overrides the Collector.collect method
         """
 
         metrics = {}
 
-        if 'scan' in self.config:
-            for ld in os.listdir(self.config['owfs']):
-                if '.' in ld:
-                    self.read_values(ld, self.config['scan'], metrics)
+        if "scan" in self.config:
+            for ld in os.listdir(self.config["owfs"]):
+                if "." in ld:
+                    self.read_values(ld, self.config["scan"], metrics)
 
-        for oid, files in self.config.iteritems():
-            if oid[:3] == 'id:':
+        for oid, files in iter(self.config.items()):
+            if oid[:3] == "id:":
                 self.read_values(oid[3:], files, metrics)
 
-        for fn, fv in metrics.iteritems():
+        for fn, fv in iter(metrics.items()):
             self.publish(fn, fv, 2)
 
     def read_values(self, oid, files, metrics):
         """
         Reads values from owfs/oid/{files} and update
         metrics with format [oid.alias] = value
         """
 
-        oid_path = os.path.join(self.config['owfs'], oid)
-        oid = oid.replace('.', '_')
+        oid_path = os.path.join(self.config["owfs"], oid)
+        oid = oid.replace(".", "_")
 
-        for fn, alias in files.iteritems():
+        for fn, alias in iter(files.items()):
             fv = os.path.join(oid_path, fn)
+
             if os.path.isfile(fv):
                 try:
                     f = open(fv)
                     v = f.read()
                     f.close()
                 except:
                     self.log.error("Unable to read %s", fv)
```

### Comparing `diamond-next-4.0.515/src/collectors/gridengine/gridengine.py` & `diamond-next-5.0.0/src/collectors/gridengine/gridengine.py`

 * *Files 7% similar despite different names*

```diff
@@ -16,135 +16,148 @@
 import sys
 import xml.dom.minidom
 
 import diamond.collector
 
 
 class GridEngineCollector(diamond.collector.Collector):
-    """Diamond collector for Grid Engine performance data
-    """
+    """Diamond collector for Grid Engine performance data"""
 
     class QueueStatsEntry:
-
-        def __init__(self, name=None, load=None, used=None, resv=None,
-                     available=None, total=None, temp_disabled=None,
-                     manual_intervention=None):
+        def __init__(
+            self,
+            name=None,
+            load=None,
+            used=None,
+            resv=None,
+            available=None,
+            total=None,
+            temp_disabled=None,
+            manual_intervention=None,
+        ):
             self.name = name
             self.load = load
             self.used = used
             self.resv = resv
             self.available = available
             self.total = total
             self.temp_disabled = temp_disabled
             self.manual_intervention = manual_intervention
 
     class StatsParser(object):
-
         def __init__(self, document):
             self.dom = xml.dom.minidom.parseString(document.strip())
 
         def get_tag_text(self, node, tag_name):
             el = node.getElementsByTagName(tag_name)[0]
             return self.get_text(el)
 
         def get_text(self, node):
             rc = []
             for node in node.childNodes:
                 if node.nodeType == node.TEXT_NODE:
                     rc.append(node.data)
-            return ''.join(rc)
+            return "".join(rc)
 
     class QueueStatsParser(StatsParser):
-
         def __init__(self, document):
             self.dom = xml.dom.minidom.parseString(document.strip())
 
         def parse(self):
             cluster_queue_summaries = self.dom.getElementsByTagName(
-                "cluster_queue_summary")
+                "cluster_queue_summary"
+            )
             return [
                 self._parse_cluster_stats_entry(node)
-                for node in cluster_queue_summaries]
+                for node in cluster_queue_summaries
+            ]
 
         def _parse_cluster_stats_entry(self, node):
             name = self.get_tag_text(node, "name")
             load = float(self.get_tag_text(node, "load"))
             used = int(self.get_tag_text(node, "used"))
             resv = int(self.get_tag_text(node, "resv"))
             available = int(self.get_tag_text(node, "available"))
             total = int(self.get_tag_text(node, "total"))
             temp_disabled = int(self.get_tag_text(node, "temp_disabled"))
-            manual_intervention = int(self.get_tag_text(
-                node,
-                "manual_intervention"))
+            manual_intervention = int(self.get_tag_text(node, "manual_intervention"))
 
             return GridEngineCollector.QueueStatsEntry(
                 name=name,
                 load=load,
                 used=used,
                 resv=resv,
                 available=available,
                 total=total,
                 temp_disabled=temp_disabled,
-                manual_intervention=manual_intervention)
+                manual_intervention=manual_intervention,
+            )
 
     def process_config(self):
         super(GridEngineCollector, self).process_config()
-        os.environ['SGE_ROOT'] = self.config['sge_root']
+        os.environ["SGE_ROOT"] = self.config["sge_root"]
 
     def get_default_config_help(self):
-        config_help = super(GridEngineCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'bin_path': "The path to Grid Engine's qstat",
-            'sge_root': "The SGE_ROOT value to provide to qstat"
-        })
+        config_help = super(GridEngineCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "bin_path": "The path to Grid Engine's qstat",
+                "sge_root": "The SGE_ROOT value to provide to qstat",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         config = super(GridEngineCollector, self).get_default_config()
-        config.update({
-            'bin_path': '/opt/gridengine/bin/lx-amd64/qstat',
-            'path': 'gridengine',
-            'sge_root': self._sge_root(),
-        })
+        config.update(
+            {
+                "bin_path": "/opt/gridengine/bin/lx-amd64/qstat",
+                "path": "gridengine",
+                "sge_root": self._sge_root(),
+            }
+        )
         return config
 
     def collect(self):
-        """Collect statistics from Grid Engine via qstat.
-        """
+        """Collect statistics from Grid Engine via qstat."""
         self._collect_queue_stats()
 
     def _capture_output(self, cmd):
         p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
         bytestr = p.communicate()[0]
         output = bytestr.decode(sys.getdefaultencoding())
         return output
 
     def _collect_queue_stats(self):
         output = self._queue_stats_xml()
         parser = self.QueueStatsParser(output)
         for cq in parser.parse():
             name = self._sanitize(cq.name)
-            prefix = 'queues.%s' % (name)
-            metrics = ['load', 'used', 'resv', 'available', 'total',
-                       'temp_disabled', 'manual_intervention']
+            prefix = "queues.%s" % name
+            metrics = [
+                "load",
+                "used",
+                "resv",
+                "available",
+                "total",
+                "temp_disabled",
+                "manual_intervention",
+            ]
             for metric in metrics:
-                path = '%s.%s' % (prefix, metric)
+                path = "%s.%s" % (prefix, metric)
                 value = getattr(cq, metric)
                 self.publish(path, value)
 
     def _queue_stats_xml(self):
-        bin_path = self.config['bin_path']
-        return self._capture_output([bin_path, '-g', 'c', '-xml'])
+        bin_path = self.config["bin_path"]
+        return self._capture_output([bin_path, "-g", "c", "-xml"])
 
     def _sanitize(self, s):
-        """Sanitize the name of a metric to remove unwanted chars
-        """
+        """Sanitize the name of a metric to remove unwanted chars"""
         return re.sub("[^\w-]", "_", s)
 
     def _sge_root(self):
-        sge_root = os.environ.get('SGE_ROOT')
+        sge_root = os.environ.get("SGE_ROOT")
         if sge_root:
             return sge_root
         else:
-            return '/opt/gridengine'
+            return "/opt/gridengine"
```

### Comparing `diamond-next-4.0.515/src/collectors/icinga_stats/icinga_stats.py` & `diamond-next-5.0.0/src/collectors/icinga_stats/icinga_stats.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 # coding=utf-8
+
 """
 IcingaStats collector - collect statistics exported by Icinga/Nagios
 via status.dat file.
 """
-import diamond.collector
+
 import re
 import time
 
+import diamond.collector
+
 RE_LSPACES = re.compile("^[\s\t]*")
 RE_TSPACES = re.compile("[\s\t]*$")
 
 
 class IcingaStatsCollector(diamond.collector.Collector):
     """
     Collect Icinga Stats
@@ -27,62 +30,59 @@
             return {}
         elif "programstatus" not in stats.keys():
             return {}
 
         metrics = self.get_icinga_stats(stats["programstatus"])
         if "hoststatus" in stats.keys():
             metrics = dict(
-                metrics.items() + self.get_host_stats(
-                    stats["hoststatus"]).items())
+                metrics.items() + self.get_host_stats(stats["hoststatus"]).items()
+            )
 
         if "servicestatus" in stats.keys():
             metrics = dict(
-                metrics.items() + self.get_svc_stats(
-                    stats["servicestatus"]).items())
+                metrics.items() + self.get_svc_stats(stats["servicestatus"]).items()
+            )
 
         for metric in metrics.keys():
             self.log.debug("Publishing '%s %s'.", metric, metrics[metric])
             self.publish(metric, metrics[metric])
 
     def get_default_config_help(self):
         """
         Return help text
         """
-        config_help = super(IcingaStatsCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            "status_path": "Path to Icinga status.dat file"
-        })
+        config_help = super(IcingaStatsCollector, self).get_default_config_help()
+        config_help.update({"status_path": "Path to Icinga status.dat file"})
         return config_help
 
     def get_default_config(self):
         """
         Returns default settings for collector
         """
         config = super(IcingaStatsCollector, self).get_default_config()
-        config.update({
-            "path": "icinga_stats",
-            "status_path": "/var/lib/icinga/status.dat",
-        })
+        config.update(
+            {
+                "path": "icinga_stats",
+                "status_path": "/var/lib/icinga/status.dat",
+            }
+        )
         return config
 
     def get_icinga_stats(self, app_stats):
-        """ Extract metrics from 'programstatus' """
+        """Extract metrics from 'programstatus'"""
         stats = {}
         stats = dict(stats.items() + self._get_active_stats(app_stats).items())
         stats = dict(stats.items() + self._get_cached_stats(app_stats).items())
-        stats = dict(
-            stats.items() + self._get_command_execution(app_stats).items())
-        stats = dict(
-            stats.items() + self._get_externalcmd_stats(app_stats).items())
+        stats = dict(stats.items() + self._get_command_execution(app_stats).items())
+        stats = dict(stats.items() + self._get_externalcmd_stats(app_stats).items())
         stats["uptime"] = self._get_uptime(app_stats)
         return stats
 
     def parse_stats_file(self, file_name):
-        """ Read and parse given file_name, return config as a dictionary """
+        """Read and parse given file_name, return config as a dictionary"""
         stats = {}
         try:
             with open(file_name, "r") as fhandle:
                 fbuffer = []
                 save_buffer = False
                 for line in fhandle:
                     line = line.rstrip("\n")
@@ -117,15 +117,15 @@
 
         except Exception as exception:
             self.log.info("Caught exception: %s", exception)
 
         return stats
 
     def get_host_stats(self, hosts):
-        """ Get statistics for Hosts, resp. Host entities """
+        """Get statistics for Hosts, resp. Host entities"""
         stats = {
             "hosts.total": 0,
             "hosts.ok": 0,
             "hosts.down": 0,
             "hosts.unreachable": 0,
             "hosts.flapping": 0,
             "hosts.in_downtime": 0,
@@ -137,27 +137,26 @@
         for host in list(hosts):
             if type(host) is not dict:
                 continue
 
             sane = self._sanitize_entity(host)
             stats["hosts.total"] += 1
             stats["hosts.flapping"] += self._trans_binary(sane["flapping"])
-            stats[
-                "hosts.in_downtime"] += self._trans_dtime(sane["in_downtime"])
+            stats["hosts.in_downtime"] += self._trans_dtime(sane["in_downtime"])
             stats["hosts.checked"] += self._trans_binary(sane["checked"])
             stats["hosts.scheduled"] += self._trans_binary(sane["scheduled"])
             stats["hosts.active_checks"] += sane["active_checks"]
             stats["hosts.passive_checks"] += sane["passive_checks"]
             state_key = self._trans_host_state(sane["state"])
-            stats["hosts.%s" % (state_key)] += 1
+            stats["hosts.%s" % state_key] += 1
 
         return stats
 
     def get_svc_stats(self, svcs):
-        """ Get statistics for Services, resp. Service entities """
+        """Get statistics for Services, resp. Service entities"""
         stats = {
             "services.total": 0,
             "services.ok": 0,
             "services.warning": 0,
             "services.critical": 0,
             "services.unknown": 0,
             "services.flapping": 0,
@@ -170,44 +169,42 @@
         for svc in svcs:
             if type(svc) is not dict:
                 continue
 
             sane = self._sanitize_entity(svc)
             stats["services.total"] += 1
             stats["services.flapping"] += self._trans_binary(sane["flapping"])
-            stats["services.in_downtime"] += self._trans_dtime(
-                sane["in_downtime"])
+            stats["services.in_downtime"] += self._trans_dtime(sane["in_downtime"])
             stats["services.checked"] += self._trans_binary(sane["checked"])
-            stats[
-                "services.scheduled"] += self._trans_binary(sane["scheduled"])
+            stats["services.scheduled"] += self._trans_binary(sane["scheduled"])
             stats["services.active_checks"] += sane["active_checks"]
             stats["services.passive_checks"] += sane["passive_checks"]
             state_key = self._trans_svc_state(sane["state"])
-            stats["services.%s" % (state_key)] += 1
+            stats["services.%s" % state_key] += 1
 
         return stats
 
     def _convert_tripplet(self, tripplet):
-        """ Turn '10,178,528' into tuple of integers """
+        """Turn '10,178,528' into tuple of integers"""
         splitted = tripplet.split(",")
         if len(splitted) != 3:
             self.log.debug("Got %i chunks, expected 3.", len(splitted))
-            return (0, 0, 0)
+            return 0, 0, 0
 
         try:
             x01 = int(splitted[0])
             x05 = int(splitted[1])
             x15 = int(splitted[2])
         except Exception as exception:
             self.log.warning("Caught exception: %s", exception)
             x01 = 0
             x05 = 0
             x15 = 0
 
-        return (x01, x05, x15)
+        return x01, x05, x15
 
     def _get_active_stats(self, app_stats):
         """
         Process:
           * active_scheduled_host_check_stats
           * active_scheduled_service_check_stats
           * active_ondemand_host_check_stats
@@ -223,17 +220,17 @@
         for app_key in app_keys:
             if app_key not in app_stats.keys():
                 continue
 
             splitted = app_key.split("_")
             metric = "%ss.%s_%s" % (splitted[2], splitted[0], splitted[1])
             (x01, x05, x15) = self._convert_tripplet(app_stats[app_key])
-            stats["%s.01" % (metric)] = x01
-            stats["%s.05" % (metric)] = x05
-            stats["%s.15" % (metric)] = x15
+            stats["%s.01" % metric] = x01
+            stats["%s.05" % metric] = x05
+            stats["%s.15" % metric] = x15
 
         return stats
 
     def _get_cached_stats(self, app_stats):
         """
         Process:
          * cached_host_check_stats
@@ -246,17 +243,17 @@
         ]
         for app_key in app_keys:
             if app_key not in app_stats.keys():
                 continue
 
             (x01, x05, x15) = self._convert_tripplet(app_stats[app_key])
             scratch = app_key.split("_")[1]
-            stats["%ss.cached.01" % (scratch)] = x01
-            stats["%ss.cached.05" % (scratch)] = x05
-            stats["%ss.cached.15" % (scratch)] = x15
+            stats["%ss.cached.01" % scratch] = x01
+            stats["%ss.cached.05" % scratch] = x05
+            stats["%ss.cached.15" % scratch] = x15
 
         return stats
 
     def _get_command_execution(self, app_stats):
         """
         Process:
          * serial_host_check_stats
@@ -317,29 +314,29 @@
             stats[aliases["x01"]] = x01
             stats[aliases["x05"]] = x05
             stats[aliases["x01"]] = x15
 
         return stats
 
     def _get_uptime(self, app_stats):
-        """ Return Icinga's uptime """
+        """Return Icinga's uptime"""
         if "program_start" not in app_stats.keys():
             return 0
 
         if not app_stats["program_start"].isdigit():
             return 0
 
         uptime = int(time.time()) - int(app_stats["program_start"])
         if uptime < 0:
             return 0
 
         return uptime
 
     def _parse_config_buffer(self, fbuffer):
-        """ Parse buffered chunk of config into dict """
+        """Parse buffered chunk of config into dict"""
         if len(fbuffer) < 1 or not fbuffer[0].endswith("{"):
             # Invalid input
             return {}
 
         entity = {}
         entity_type = fbuffer.pop(0)
         entity_type = entity_type.rstrip("{")
@@ -391,55 +388,55 @@
 
         if sane["passive_checks"] not in [0, 1]:
             sane["passive_checks"] = 0
 
         return sane
 
     def _trans_binary(self, value):
-        """ Given value is expected to be a binary - 0/1 """
+        """Given value is expected to be a binary - 0/1"""
         try:
             conv = int(value)
         except ValueError:
             return 0
 
         if conv not in [0, 1]:
             return 0
 
         return conv
 
     def _trans_dtime(self, value):
-        """ Translate scheduled downtime """
+        """Translate scheduled downtime"""
         try:
             conv = int(value)
         except ValueError:
             return 0
 
         if conv < 1:
             return 0
 
         return conv
 
     def _trans_host_state(self, state):
-        """ Translate/validate Host state """
+        """Translate/validate Host state"""
         if state == 0:
             return "ok"
         elif state == 1:
             return "down"
         else:
             return "unreachable"
 
     def _trans_svc_state(self, state):
-        """ Translate/validate Service state """
+        """Translate/validate Service state"""
         if state == 0:
             return "ok"
         elif state == 1:
             return "warning"
         elif state == 2:
             return "critical"
         else:
             return "unknown"
 
     def _trim(self, somestr):
-        """ Trim left-right given string """
+        """Trim left-right given string"""
         tmp = RE_LSPACES.sub("", somestr)
         tmp = RE_TSPACES.sub("", tmp)
         return str(tmp)
```

### Comparing `diamond-next-4.0.515/src/collectors/dseopscenter/dseopscenter.py` & `diamond-next-5.0.0/src/collectors/dseopscenter/dseopscenter.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,207 +1,232 @@
 # coding=utf-8
 
 """
 Collect the DataStax OpsCenter metrics
 """
 
-import urllib2
 import datetime
+import urllib.request
+
+import diamond.collector
 
 try:
     import json
 except ImportError:
     import simplejson as json
 
-import diamond.collector
-
 
 class DseOpsCenterCollector(diamond.collector.Collector):
     last_run_time = 0
     column_families = None
     last_schema_sync_time = 0
 
     def get_default_config_help(self):
-        config_help = super(DseOpsCenterCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': "",
-            'port': "",
-            'cluster_id': "Set cluster ID/name.\n",
-            'metrics': "You can list explicit metrics if you like,\n"
-            " by default all know metrics are included.\n",
-            'node_group': "Set node group name, any by default\n",
-            'default_tail_opts': "Chaning these is not recommended.",
-        })
+        config_help = super(DseOpsCenterCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "",
+                "port": "",
+                "cluster_id": "Set cluster ID/name.\n",
+                "metrics": "You can list explicit metrics if you like,\n by default all know metrics are included.\n",
+                "node_group": "Set node group name, any by default\n",
+                "default_tail_opts": "Chaning these is not recommended.",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(DseOpsCenterCollector, self).get_default_config()
         metrics = [
-            'cf-bf-false-positives',
-            'cf-bf-false-ratio',
-            'cf-bf-space-used',
-            'cf-keycache-hit-rate',
-            'cf-keycache-hits',
-            'cf-keycache-requests',
-            'cf-live-disk-used',
-            'cf-live-sstables',
-            'cf-pending-tasks',
-            'cf-read-latency-op',
-            'cf-read-ops',
-            'cf-rowcache-hit-rate',
-            'cf-rowcache-hits',
-            'cf-rowcache-requests',
-            'cf-total-disk-used',
-            'cf-write-latency-op',
-            'cf-write-ops',
-            'cms-collection-count',
-            'cms-collection-time',
-            'data-load',
-            'heap-committed',
-            'heap-max',
-            'heap-used',
-            'key-cache-hit-rate',
-            'key-cache-hits',
-            'key-cache-requests',
-            'nonheap-committed',
-            'nonheap-max',
-            'nonheap-used',
-            'pending-compaction-tasks',
-            'pending-flush-sorter-tasks',
-            'pending-flushes',
-            'pending-gossip-tasks',
-            'pending-hinted-handoff',
-            'pending-internal-responses',
-            'pending-memtable-post-flushers',
-            'pending-migrations',
-            'pending-misc-tasks',
-            'pending-read-ops',
-            'pending-read-repair-tasks',
-            'pending-repair-tasks',
-            'pending-repl-on-write-tasks',
-            'pending-request-responses',
-            'pending-streams',
-            'pending-write-ops',
-            'read-latency-op',
-            'read-ops',
-            'row-cache-hit-rate',
-            'row-cache-hits',
-            'row-cache-requests',
-            'solr-avg-time-per-req',
-            'solr-errors',
-            'solr-requests',
-            'solr-timeouts',
-            'total-bytes-compacted',
-            'total-compactions-completed',
-            'write-latency-op',
-            'write-ops',
+            "cf-bf-false-positives",
+            "cf-bf-false-ratio",
+            "cf-bf-space-used",
+            "cf-keycache-hit-rate",
+            "cf-keycache-hits",
+            "cf-keycache-requests",
+            "cf-live-disk-used",
+            "cf-live-sstables",
+            "cf-pending-tasks",
+            "cf-read-latency-op",
+            "cf-read-ops",
+            "cf-rowcache-hit-rate",
+            "cf-rowcache-hits",
+            "cf-rowcache-requests",
+            "cf-total-disk-used",
+            "cf-write-latency-op",
+            "cf-write-ops",
+            "cms-collection-count",
+            "cms-collection-time",
+            "data-load",
+            "heap-committed",
+            "heap-max",
+            "heap-used",
+            "key-cache-hit-rate",
+            "key-cache-hits",
+            "key-cache-requests",
+            "nonheap-committed",
+            "nonheap-max",
+            "nonheap-used",
+            "pending-compaction-tasks",
+            "pending-flush-sorter-tasks",
+            "pending-flushes",
+            "pending-gossip-tasks",
+            "pending-hinted-handoff",
+            "pending-internal-responses",
+            "pending-memtable-post-flushers",
+            "pending-migrations",
+            "pending-misc-tasks",
+            "pending-read-ops",
+            "pending-read-repair-tasks",
+            "pending-repair-tasks",
+            "pending-repl-on-write-tasks",
+            "pending-request-responses",
+            "pending-streams",
+            "pending-write-ops",
+            "read-latency-op",
+            "read-ops",
+            "row-cache-hit-rate",
+            "row-cache-hits",
+            "row-cache-requests",
+            "solr-avg-time-per-req",
+            "solr-errors",
+            "solr-requests",
+            "solr-timeouts",
+            "total-bytes-compacted",
+            "total-compactions-completed",
+            "write-latency-op",
+            "write-ops",
         ]
-        config.update({
-            'host':              '127.0.0.1',
-            'port':              8888,
-            'path':              'cassandra',
-            'node_group':        '*',
-            'metrics':           ','.join(metrics),
-            'default_tail_opts': '&forecast=0&node_aggregation=1',
-
-        })
+        config.update(
+            {
+                "host": "127.0.0.1",
+                "port": 8888,
+                "path": "cassandra",
+                "node_group": "*",
+                "metrics": ",".join(metrics),
+                "default_tail_opts": "&forecast=0&node_aggregation=1",
+            }
+        )
         return config
 
     def _get_schema(self):
-        time_now = int(datetime.datetime.utcnow().strftime('%s'))
-        if ((self.column_families is None or
-             (time_now - self.last_schema_sync_time < 3600))):
+        time_now = int(datetime.datetime.utcnow().strftime("%s"))
+
+        if self.column_families is None or (
+            time_now - self.last_schema_sync_time < 3600
+        ):
             return False
-        url = 'http://%s:%i/%s/keyspaces' % (self.config['host'],
-                                             int(self.config['port']),
-                                             self.config['cluster_id'])
+
+        url = "http://%s:%i/%s/keyspaces" % (
+            self.config["host"],
+            int(self.config["port"]),
+            self.config["cluster_id"],
+        )
+
         try:
-            response = urllib2.urlopen(url)
-        except Exception, err:
-            self.log.error('%s: %s', url, err)
+            response = urllib.request.urlopen(url)
+        except Exception as err:
+            self.log.error("%s: %s", url, err)
+
             return False
 
         try:
             result = json.load(response)
             column_families = []
+
             for ks in result:
                 i = []
-                for cf in result[ks]['column_families']:
+
+                for cf in result[ks]["column_families"]:
                     i.append("%s.%s" % (ks, cf))
+
                 column_families.append(i)
-            self.column_families = ','.join(sum(column_families, []))
-            self.log.debug('DseOpsCenterCollector columnfamilies = %s',
-                           self.column_families)
+
+            self.column_families = ",".join(sum(column_families, []))
+            self.log.debug(
+                "DseOpsCenterCollector columnfamilies = %s", self.column_families
+            )
             self.last_schema_sync_time = time_now
-            return True
 
+            return True
         except (TypeError, ValueError):
-            self.log.error(
-                "Unable to parse response from opscenter as a json object")
+            self.log.error("Unable to parse response from opscenter as a json object")
+
             return False
 
     def _get(self, start, end, step=60):
         self._get_schema()
-        url = ('http://%s:%i/%s/new-metrics?node_group=%s&columnfamilies=%s'
-               '&metrics=%s&start=%i&end=%i&step=%i%s') % (
-            self.config['host'],
-            int(self.config['port']),
-            self.config['cluster_id'],
-            self.config['node_group'],
-            self.column_families,
-            self.config['metrics'],
-            start, end, step,
-            self.config['default_tail_opts'])
+        url = (
+            "http://%s:%i/%s/new-metrics?node_group=%s&columnfamilies=%s&metrics=%s&start=%i&end=%i&step=%i%s"
+            % (
+                self.config["host"],
+                int(self.config["port"]),
+                self.config["cluster_id"],
+                self.config["node_group"],
+                self.column_families,
+                self.config["metrics"],
+                start,
+                end,
+                step,
+                self.config["default_tail_opts"],
+            )
+        )
 
         try:
-            response = urllib2.urlopen(url)
-        except Exception, err:
-            self.log.error('%s: %s', url, err)
+            response = urllib.request.urlopen(url)
+        except Exception as err:
+            self.log.error("%s: %s", url, err)
+
             return False
 
-        self.log.debug('DseOpsCenterCollector metrics url = %s', url)
+        self.log.debug("DseOpsCenterCollector metrics url = %s", url)
 
         try:
             return json.load(response)
         except (TypeError, ValueError):
-            self.log.error(
-                "Unable to parse response from opscenter as a json object")
+            self.log.error("Unable to parse response from opscenter as a json object")
+
             return False
 
     def collect(self):
         metrics = {}
+
         if json is None:
-            self.log.error('Unable to import json')
+            self.log.error("Unable to import json")
+
             return None
 
-        time_now = int(datetime.datetime.utcnow().strftime('%s'))
+        time_now = int(datetime.datetime.utcnow().strftime("%s"))
 
-        self.log.debug('DseOpsCenterCollector last_run_time = %i',
-                       self.last_run_time)
+        self.log.debug("DseOpsCenterCollector last_run_time = %i", self.last_run_time)
 
         if self.last_run_time == 0:
             self.last_run_time = time_now - 60
         if time_now - self.last_run_time >= 60:
             result = self._get(self.last_run_time, time_now)
             self.last_run_time = time_now
+
             if not result:
                 return None
-            self.log.debug('DseOpsCenterCollector result = %s', result)
-            for data in result['data'][self.config['node_group']]:
-                if data['data-points'][0][0] is not None:
-                    if 'columnfamily' in data:
-                        k = '.'.join([data['metric'],
-                                      data['columnfamily']])
-                        metrics[k] = data['data-points'][0][0]
+
+            self.log.debug("DseOpsCenterCollector result = %s", result)
+
+            for data in result["data"][self.config["node_group"]]:
+                if data["data-points"][0][0] is not None:
+                    if "columnfamily" in data:
+                        k = ".".join([data["metric"], data["columnfamily"]])
+                        metrics[k] = data["data-points"][0][0]
                     else:
-                        metrics[data['metric']] = data['data-points'][0][0]
-            self.log.debug('DseOpsCenterCollector metrics = %s', metrics)
+                        metrics[data["metric"]] = data["data-points"][0][0]
+
+            self.log.debug("DseOpsCenterCollector metrics = %s", metrics)
+
             for key in metrics:
                 self.publish(key, metrics[key])
         else:
-            self.log.debug(
-                "DseOpsCenterCollector can only run once every minute")
+            self.log.debug("DseOpsCenterCollector can only run once every minute")
+
             return None
```

### Comparing `diamond-next-4.0.515/src/collectors/libvirtkvm/libvirtkvm.py` & `diamond-next-5.0.0/src/collectors/libvirtkvm/libvirtkvm.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,71 +6,67 @@
 #### Dependencies
 
  * python-libvirt, xml
 
 """
 
 import diamond.collector
-from diamond.collector import str_to_bool
+
 try:
     from xml.etree import ElementTree
 except ImportError:
     import cElementTree as ElementTree
 
 try:
     import libvirt
 except ImportError:
     libvirt = None
 
 
 class LibvirtKVMCollector(diamond.collector.Collector):
-    blockStats = {
-        'read_reqs':   0,
-        'read_bytes':  1,
-        'write_reqs':  2,
-        'write_bytes': 3
-    }
+    blockStats = {"read_reqs": 0, "read_bytes": 1, "write_reqs": 2, "write_bytes": 3}
 
     vifStats = {
-        'rx_bytes':   0,
-        'rx_packets': 1,
-        'rx_errors':  2,
-        'rx_drops':   3,
-        'tx_bytes':   4,
-        'tx_packets': 5,
-        'tx_errors':  6,
-        'tx_drops':   7
+        "rx_bytes": 0,
+        "rx_packets": 1,
+        "rx_errors": 2,
+        "rx_drops": 3,
+        "tx_bytes": 4,
+        "tx_packets": 5,
+        "tx_errors": 6,
+        "tx_drops": 7,
     }
 
     def get_default_config_help(self):
-        config_help = super(LibvirtKVMCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'uri': """The libvirt connection URI. By default it's
-'qemu:///system'. One decent option is
-'qemu+unix:///system?socket=/var/run/libvirt/libvit-sock-ro'.""",
-            'sort_by_uuid': """Use the <uuid> of the instance instead of the
- default <name>, useful in Openstack deploments where <name> is only
-specific to the compute node""",
-            'cpu_absolute': """CPU stats reported as percentage by default, or
-as cummulative nanoseconds since VM creation if this is True."""
-        })
+        config_help = super(LibvirtKVMCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "uri": "The libvirt connection URI. By default it's 'qemu:///system'. "
+                "One decent option is 'qemu+unix:///system?socket=/var/run/libvirt/libvit-sock-ro'.",
+                "sort_by_uuid": "Use the <uuid> of the instance instead of the default <name>, "
+                "useful in Openstack deploments where <name> is onlyspecific to the compute node",
+                "cpu_absolute": "CPU stats reported as percentage by default, or as cummulative nanoseconds "
+                "since VM creation if this is True.",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(LibvirtKVMCollector, self).get_default_config()
-        config.update({
-            'path':     'libvirt-kvm',
-            'sort_by_uuid': False,
-            'uri':      'qemu:///system',
-            'cpu_absolute': False
-        })
+        config.update(
+            {
+                "path": "libvirt-kvm",
+                "sort_by_uuid": False,
+                "uri": "qemu:///system",
+                "cpu_absolute": False,
+            }
+        )
         return config
 
     def get_devices(self, dom, type):
         devices = []
 
         # Create a XML tree from the domain XML description.
         tree = ElementTree.fromstring(dom.XMLDesc(0))
@@ -79,87 +75,88 @@
             dev = target.get("dev")
             if dev not in devices:
                 devices.append(dev)
 
         return devices
 
     def get_disk_devices(self, dom):
-        return self.get_devices(dom, 'disk')
+        return self.get_devices(dom, "disk")
 
     def get_network_devices(self, dom):
-        return self.get_devices(dom, 'interface')
+        return self.get_devices(dom, "interface")
 
     def report_cpu_metric(self, statname, value, instance):
         # Value in cummulative nanoseconds
-        if str_to_bool(self.config['cpu_absolute']):
+        if diamond.collector.str_to_bool(self.config["cpu_absolute"]):
             metric = value
         else:
             # Nanoseconds (10^9), however, we want to express in 100%
-            metric = self.derivative(statname, float(value) / 10000000.0,
-                                     max_value=diamond.collector.MAX_COUNTER,
-                                     instance=instance)
+            metric = self.derivative(
+                statname,
+                float(value) / 10000000.0,
+                max_value=diamond.collector.MAX_COUNTER,
+                instance=instance,
+            )
+
         self.publish(statname, metric, instance=instance)
 
     def collect(self):
         if libvirt is None:
-            self.log.error('Unable to import libvirt')
+            self.log.error("Unable to import libvirt")
             return {}
 
-        conn = libvirt.openReadOnly(self.config['uri'])
+        conn = libvirt.openReadOnly(self.config["uri"])
+
         for dom in [conn.lookupByID(n) for n in conn.listDomainsID()]:
-            if str_to_bool(self.config['sort_by_uuid']):
+            if diamond.collector.str_to_bool(self.config["sort_by_uuid"]):
                 name = dom.UUIDString()
             else:
                 name = dom.name()
 
             # CPU stats
             vcpus = dom.getCPUStats(True, 0)
             totalcpu = 0
             idx = 0
+
             for vcpu in vcpus:
-                cputime = vcpu['cpu_time']
-                self.report_cpu_metric('cpu.%s.time' % idx, cputime, name)
+                cputime = vcpu["cpu_time"]
+                self.report_cpu_metric("cpu.%s.time" % idx, cputime, name)
                 idx += 1
                 totalcpu += cputime
-            self.report_cpu_metric('cpu.total.time', totalcpu, name)
+            self.report_cpu_metric("cpu.total.time", totalcpu, name)
 
             # Disk stats
             disks = self.get_disk_devices(dom)
             accum = {}
             for stat in self.blockStats.keys():
                 accum[stat] = 0
 
             for disk in disks:
                 stats = dom.blockStats(disk)
                 for stat in self.blockStats.keys():
                     idx = self.blockStats[stat]
                     val = stats[idx]
                     accum[stat] += val
-                    self.publish('block.%s.%s' % (disk, stat), val,
-                                 instance=name)
+                    self.publish("block.%s.%s" % (disk, stat), val, instance=name)
             for stat in self.blockStats.keys():
-                self.publish('block.total.%s' % stat, accum[stat],
-                             instance=name)
+                self.publish("block.total.%s" % stat, accum[stat], instance=name)
 
             # Network stats
             vifs = self.get_network_devices(dom)
             accum = {}
             for stat in self.vifStats.keys():
                 accum[stat] = 0
 
             for vif in vifs:
                 stats = dom.interfaceStats(vif)
                 for stat in self.vifStats.keys():
                     idx = self.vifStats[stat]
                     val = stats[idx]
                     accum[stat] += val
-                    self.publish('net.%s.%s' % (vif, stat), val,
-                                 instance=name)
+                    self.publish("net.%s.%s" % (vif, stat), val, instance=name)
             for stat in self.vifStats.keys():
-                self.publish('net.total.%s' % stat, accum[stat],
-                             instance=name)
+                self.publish("net.total.%s" % stat, accum[stat], instance=name)
 
             # Memory stats
             mem = dom.memoryStats()
-            self.publish('memory.nominal', mem['actual'] * 1024,
-                         instance=name)
-            self.publish('memory.rss', mem['rss'] * 1024, instance=name)
+            self.publish("memory.nominal", mem["actual"] * 1024, instance=name)
+            self.publish("memory.rss", mem["rss"] * 1024, instance=name)
```

### Comparing `diamond-next-4.0.515/src/collectors/proc/proc.py` & `diamond-next-5.0.0/src/collectors/proc/proc.py`

 * *Files 12% similar despite different names*

```diff
@@ -6,73 +6,70 @@
 
 #### Dependencies
 
  * /proc/stat
 
 """
 
-import platform
 import os
+import platform
+
 import diamond.collector
 
 # Detect the architecture of the system
 # and set the counters for MAX_VALUES
 # appropriately. Otherwise, rolling over
 # counters will cause incorrect or
 # negative values.
-if platform.architecture()[0] == '64bit':
-    counter = (2 ** 64) - 1
+if platform.architecture()[0] == "64bit":
+    counter = (2**64) - 1
 else:
-    counter = (2 ** 32) - 1
+    counter = (2**32) - 1
 
 
 class ProcessStatCollector(diamond.collector.Collector):
-
-    PROC = '/proc/stat'
+    PROC = "/proc/stat"
 
     def get_default_config_help(self):
-        config_help = super(ProcessStatCollector,
-                            self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(ProcessStatCollector, self).get_default_config_help()
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ProcessStatCollector, self).get_default_config()
-        config.update({
-            'path':     'proc'
-        })
+        config.update({"path": "proc"})
         return config
 
     def collect(self):
         """
         Collect process stat data
         """
         if not os.access(self.PROC, os.R_OK):
             return False
 
         # Open PROC file
-        file = open(self.PROC, 'r')
+        file = open(self.PROC, "r")
 
         # Get data
         for line in file:
 
-            if line.startswith('ctxt') or line.startswith('processes'):
+            if line.startswith("ctxt") or line.startswith("processes"):
                 data = line.split()
                 metric_name = data[0]
                 metric_value = int(data[1])
-                metric_value = int(self.derivative(metric_name,
-                                                   long(metric_value),
-                                                   counter))
+                metric_value = int(
+                    self.derivative(metric_name, int(metric_value), counter)
+                )
                 self.publish(metric_name, metric_value)
 
-            if line.startswith('procs_') or line.startswith('btime'):
+            if line.startswith("procs_") or line.startswith("btime"):
                 data = line.split()
                 metric_name = data[0]
                 metric_value = int(data[1])
                 self.publish(metric_name, metric_value)
 
         # Close file
         file.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/eventstoreprojections/eventstoreprojections.py` & `diamond-next-5.0.0/src/collectors/eventstoreprojections/eventstoreprojections.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,115 +9,120 @@
 Note: "$" are replaced by underscores (_) in the projection
 name to avoid problems with hostedGraphite/Grafana.
 
 This collector is based upon the HTTPJSONCollector.
 
 #### Dependencies
 
- * urllib2
+ * urllib
 
 """
 
-import urllib2
 import json
+import urllib.error
+import urllib.request
+
 import diamond.collector
 
 
 class EventstoreProjectionsCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(
-            EventstoreProjectionsCollector, self).get_default_config_help(
+            EventstoreProjectionsCollector, self
+        ).get_default_config_help()
+        config_help.update(
+            {
+                "path": "name of the metric in the metricpath",
+                "protocol": "protocol used to connect to eventstore",
+                "hostname": "hostname of the eventstore instance",
+                "route": "route in eventstore for projections",
+                "port": "tcp port where eventstore is listening",
+                "headers": "Header variable if needed",
+                "replace_dollarsign": "A value to replace a dollar sign ($) in projection names by",
+                "debug": "Enable or disable debug mode",
+            }
         )
-        config_help.update({
-            'path': "name of the metric in the metricpath",
-            'protocol': 'protocol used to connect to eventstore',
-            'hostname': 'hostname of the eventstore instance',
-            'route': 'route in eventstore for projections',
-            'port': 'tcp port where eventstore is listening',
-            'headers': 'Header variable if needed',
-            'replace_dollarsign':
-            'A value to replace a dollar sign ($) in projection names by',
-            'debug': 'Enable or disable debug mode',
-        })
         return config_help
 
     def get_default_config(self):
         default_config = super(
-            EventstoreProjectionsCollector, self).get_default_config(
+            EventstoreProjectionsCollector, self
+        ).get_default_config()
+        default_config.update(
+            {
+                "path": "eventstore",
+                "protocol": "http://",
+                "hostname": "localhost",
+                "route": "/projections/all-non-transient",
+                "port": 2113,
+                "headers": {"User-Agent": "Diamond Eventstore metrics collector"},
+                "replace_dollarsign": "_",
+                "debug": False,
+            }
         )
-        default_config.update({
-            'path': "eventstore",
-            'protocol': 'http://',
-            'hostname': 'localhost',
-            'route': '/projections/all-non-transient',
-            'port': 2113,
-            'headers': {'User-Agent': 'Diamond Eventstore metrics collector'},
-            'replace_dollarsign': '_',
-            'debug': False,
-        })
         return default_config
 
     def _json_to_flat_metrics(self, prefix, data):
-
         for key, value in data.items():
             if isinstance(value, dict):
-                for k, v in self._json_to_flat_metrics(
-                        "%s.%s" % (prefix, key), value):
+                for k, v in self._json_to_flat_metrics("%s.%s" % (prefix, key), value):
                     yield k, v
-            elif isinstance(value, basestring):
+            elif isinstance(value, str):
                 if value == "Running":
                     value = 1
-                    yield ("%s.%s" % (prefix, key), value)
+                    yield "%s.%s" % (prefix, key), value
                 elif value == "Stopped":
                     value = 0
-                    yield ("%s.%s" % (prefix, key), value)
+                    yield "%s.%s" % (prefix, key), value
                 else:
-                    if self.config['debug']:
+                    if self.config["debug"]:
                         self.log.debug("ignoring string value = %s", value)
                     continue
             else:
                 try:
                     int(value)
                 except ValueError:
                     self.log.debug("cast to int failed, value = %s", value)
                 finally:
-                    yield ("%s.%s" % (prefix, key), value)
+                    yield "%s.%s" % (prefix, key), value
 
     def collect(self):
         eventstore_host = "%s%s:%s%s" % (
-            self.config['protocol'],
-            self.config['hostname'],
-            self.config['port'],
-            self.config['route']
+            self.config["protocol"],
+            self.config["hostname"],
+            self.config["port"],
+            self.config["route"],
         )
 
-        req = urllib2.Request(eventstore_host, headers=self.config['headers'])
-        req.add_header('Content-type', 'application/json')
+        req = urllib.request.Request(eventstore_host, headers=self.config["headers"])
+        req.add_header("Content-type", "application/json")
 
         try:
-            resp = urllib2.urlopen(req)
-        except urllib2.URLError as e:
+            resp = urllib.request.urlopen(req)
+        except urllib.error.URLError as e:
             self.log.error("Can't open url %s. %s", eventstore_host, e)
         else:
             content = resp.read()
+
             try:
                 json_dict = json.loads(content)
-                projections = json_dict['projections']
-
+                projections = json_dict["projections"]
                 data = {}
+
                 for projection in projections:
-                    if self.config['replace_dollarsign']:
+                    if self.config["replace_dollarsign"]:
                         name = projection["name"].replace(
-                            '$',
-                            self.config['replace_dollarsign']
+                            "$", self.config["replace_dollarsign"]
                         )
                     else:
                         name = projection["name"]
+
                     data[name] = projection
             except ValueError as e:
-                self.log.error("failed parsing JSON Object \
-                                from %s. %s", eventstore_host, e)
+                self.log.error(
+                    "failed parsing JSON Object from %s. %s", eventstore_host, e
+                )
             else:
                 for metric_name, metric_value in self._json_to_flat_metrics(
-                        "projections", data):
+                    "projections", data
+                ):
                     self.publish(metric_name, metric_value)
```

### Comparing `diamond-next-4.0.515/src/collectors/eventstoreprojections/tests/fixtures/projections` & `diamond-next-5.0.0/src/collectors/eventstoreprojections/tests/fixtures/projections`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/src/collectors/beanstalkd/beanstalkd.py` & `diamond-next-5.0.0/src/collectors/beanstalkd/beanstalkd.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,82 +8,95 @@
 #### Dependencies
 
  * beanstalkc
 
 """
 
 import re
+
 import diamond.collector
 
 try:
     import beanstalkc
 except ImportError:
     beanstalkc = None
 
 
 class BeanstalkdCollector(diamond.collector.Collector):
-    SKIP_LIST = ['version', 'id', 'hostname']
-    COUNTERS_REGEX = re.compile(
-        r'^(cmd-.*|job-timeouts|total-jobs|total-connections)$')
+    SKIP_LIST = ["version", "id", "hostname"]
+    COUNTERS_REGEX = re.compile(r"^(cmd-.*|job-timeouts|total-jobs|total-connections)$")
 
     def get_default_config_help(self):
-        config_help = super(BeanstalkdCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': 'Hostname',
-            'port': 'Port',
-        })
+        config_help = super(BeanstalkdCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "Hostname",
+                "port": "Port",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(BeanstalkdCollector, self).get_default_config()
-        config.update({
-            'path':     'beanstalkd',
-            'host':     'localhost',
-            'port':     11300,
-        })
+        config.update(
+            {
+                "path": "beanstalkd",
+                "host": "localhost",
+                "port": 11300,
+            }
+        )
+
         return config
 
     def _get_stats(self):
         stats = {}
+
         try:
-            connection = beanstalkc.Connection(self.config['host'],
-                                               int(self.config['port']))
-        except beanstalkc.BeanstalkcException, e:
+            connection = beanstalkc.Connection(
+                self.config["host"], int(self.config["port"])
+            )
+        except beanstalkc.BeanstalkcException as e:
             self.log.error("Couldn't connect to beanstalkd: %s", e)
+
             return {}
 
-        stats['instance'] = connection.stats()
-        stats['tubes'] = []
+        stats["instance"] = connection.stats()
+        stats["tubes"] = []
 
         for tube in connection.tubes():
             tube_stats = connection.stats_tube(tube)
-            stats['tubes'].append(tube_stats)
+            stats["tubes"].append(tube_stats)
 
         return stats
 
     def collect(self):
         if beanstalkc is None:
-            self.log.error('Unable to import beanstalkc')
+            self.log.error("Unable to import beanstalkc")
+
             return {}
 
         info = self._get_stats()
 
-        for stat, value in info['instance'].items():
+        for stat, value in info["instance"].items():
             if stat not in self.SKIP_LIST:
-                self.publish(stat, value,
-                             metric_type=self.get_metric_type(stat))
+                self.publish(stat, value, metric_type=self.get_metric_type(stat))
+
+        for tube_stats in info["tubes"]:
+            tube = tube_stats["name"]
 
-        for tube_stats in info['tubes']:
-            tube = tube_stats['name']
             for stat, value in tube_stats.items():
-                if stat != 'name':
-                    self.publish('tubes.%s.%s' % (tube, stat), value,
-                                 metric_type=self.get_metric_type(stat))
+                if stat != "name":
+                    self.publish(
+                        "tubes.%s.%s" % (tube, stat),
+                        value,
+                        metric_type=self.get_metric_type(stat),
+                    )
 
     def get_metric_type(self, stat):
         if self.COUNTERS_REGEX.match(stat):
-            return 'COUNTER'
-        return 'GAUGE'
+            return "COUNTER"
+
+        return "GAUGE"
```

### Comparing `diamond-next-4.0.515/src/collectors/vmstat/vmstat.py` & `diamond-next-5.0.0/src/collectors/vmstat/vmstat.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,58 +5,59 @@
 
 #### Dependencies
 
  * /proc/vmstat
 
 """
 
-import diamond.collector
 import os
 import re
 
+import diamond.collector
+
 
 class VMStatCollector(diamond.collector.Collector):
+    PROC = "/proc/vmstat"
 
-    PROC = '/proc/vmstat'
     MAX_VALUES = {
-        'pgfault': diamond.collector.MAX_COUNTER,
-        'pgmajfault': diamond.collector.MAX_COUNTER,
-        'pgpgin': diamond.collector.MAX_COUNTER,
-        'pgpgout': diamond.collector.MAX_COUNTER,
-        'pswpin': diamond.collector.MAX_COUNTER,
-        'pswpout': diamond.collector.MAX_COUNTER,
+        "pgfault": diamond.collector.MAX_COUNTER,
+        "pgmajfault": diamond.collector.MAX_COUNTER,
+        "pgpgin": diamond.collector.MAX_COUNTER,
+        "pgpgout": diamond.collector.MAX_COUNTER,
+        "pswpin": diamond.collector.MAX_COUNTER,
+        "pswpout": diamond.collector.MAX_COUNTER,
     }
 
     def get_default_config_help(self):
         config_help = super(VMStatCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(VMStatCollector, self).get_default_config()
-        config.update({
-            'path':     'vmstat'
-        })
+        config.update({"path": "vmstat"})
         return config
 
     def collect(self):
         if not os.access(self.PROC, os.R_OK):
             return None
 
         # open file
         file = open(self.PROC)
-        exp = '^(pgfault|pgmajfault|pgpgin|pgpgout|pswpin|pswpout)\s(\d+)'
+        exp = "^(pgfault|pgmajfault|pgpgin|pgpgout|pswpin|pswpout)\s(\d+)"
         reg = re.compile(exp)
+
         # Build regex
         for line in file:
             match = reg.match(line)
+
             if match:
                 name = match.group(1)
                 value = match.group(2)
                 max_value = self.MAX_VALUES[name]
                 derived = self.derivative(name, int(value), max_value)
                 self.publish(name, derived, raw_value=int(value), precision=2)
```

### Comparing `diamond-next-4.0.515/src/collectors/loadavg/loadavg.py` & `diamond-next-5.0.0/src/collectors/loadavg/loadavg.py`

 * *Files 21% similar despite different names*

```diff
@@ -5,64 +5,61 @@
 
 #### Dependencies
 
  * /proc/loadavg
 
 """
 
-import diamond.collector
-import re
-import os
 import multiprocessing
-from diamond.collector import str_to_bool
+import os
+import re
+
+import diamond.collector
 
 
 class LoadAverageCollector(diamond.collector.Collector):
+    PROC_LOADAVG = "/proc/loadavg"
 
-    PROC_LOADAVG = '/proc/loadavg'
-    PROC_LOADAVG_RE = re.compile(r'([\d.]+) ([\d.]+) ([\d.]+) (\d+)/(\d+)')
+    PROC_LOADAVG_RE = re.compile(r"([\d.]+) ([\d.]+) ([\d.]+) (\d+)/(\d+)")
 
     def get_default_config_help(self):
-        config_help = super(LoadAverageCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'simple':   'Only collect the 1 minute load average'
-        })
+        config_help = super(LoadAverageCollector, self).get_default_config_help()
+        config_help.update({"simple": "Only collect the 1 minute load average"})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(LoadAverageCollector, self).get_default_config()
-        config.update({
-            'path':     'loadavg',
-            'simple':   'False'
-        })
+        config.update({"path": "loadavg", "simple": "False"})
         return config
 
     def collect(self):
         load01, load05, load15 = os.getloadavg()
         cpu_count = multiprocessing.cpu_count()
 
-        if not str_to_bool(self.config['simple']):
-            self.publish_gauge('01', load01, 2)
-            self.publish_gauge('05', load05, 2)
-            self.publish_gauge('15', load15, 2)
-            self.publish_gauge('01_normalized', load01 / cpu_count, 2)
-            self.publish_gauge('05_normalized', load05 / cpu_count, 2)
-            self.publish_gauge('15_normalized', load15 / cpu_count, 2)
+        if not diamond.collector.str_to_bool(self.config["simple"]):
+            self.publish_gauge("01", load01, 2)
+            self.publish_gauge("05", load05, 2)
+            self.publish_gauge("15", load15, 2)
+            self.publish_gauge("01_normalized", load01 / cpu_count, 2)
+            self.publish_gauge("05_normalized", load05 / cpu_count, 2)
+            self.publish_gauge("15_normalized", load15 / cpu_count, 2)
         else:
-            self.publish_gauge('load', load01, 2)
-            self.publish_gauge('load_normalized', load01 / cpu_count, 2)
+            self.publish_gauge("load", load01, 2)
+            self.publish_gauge("load_normalized", load01 / cpu_count, 2)
 
         # Legacy: add process/thread counters provided by
         # /proc/loadavg (if available).
         if os.access(self.PROC_LOADAVG, os.R_OK):
             file = open(self.PROC_LOADAVG)
+
             for line in file:
                 match = self.PROC_LOADAVG_RE.match(line)
+
                 if match:
-                    self.publish_gauge('processes_running',
-                                       int(match.group(4)))
-                    self.publish_gauge('processes_total', int(match.group(5)))
+                    self.publish_gauge("processes_running", int(match.group(4)))
+                    self.publish_gauge("processes_total", int(match.group(5)))
+
             file.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/scribe/scribe.py` & `diamond-next-5.0.0/src/collectors/scribe/scribe.py`

 * *Files 13% similar despite different names*

```diff
@@ -5,54 +5,56 @@
 
 #### Dependencies
 
     * /usr/sbin/scribe_ctrl, distributed with scribe
 
 """
 
-import subprocess
 import string
+import subprocess
 
 import diamond.collector
 
 
 class ScribeCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(ScribeCollector, self).get_default_config_help()
-        config_help.update({
-            'scribe_ctrl_bin': 'Path to scribe_ctrl binary',
-            'scribe_port': 'Scribe port',
-        })
+        config_help.update(
+            {
+                "scribe_ctrl_bin": "Path to scribe_ctrl binary",
+                "scribe_port": "Scribe port",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         config = super(ScribeCollector, self).get_default_config()
-        config.update({
-            'path': 'scribe',
-            'scribe_ctrl_bin': self.find_binary('/usr/sbin/scribe_ctrl'),
-            'scribe_port': None,
-        })
+        config.update(
+            {
+                "path": "scribe",
+                "scribe_ctrl_bin": self.find_binary("/usr/sbin/scribe_ctrl"),
+                "scribe_port": None,
+            }
+        )
         return config
 
     def key_to_metric(self, key):
         """Replace all non-letter characters with underscores"""
-        return ''.join(l if l in string.letters else '_' for l in key)
+        return "".join(l if l in string.ascii_letters else "_" for l in key)
 
     def get_scribe_ctrl_output(self):
-        cmd = [self.config['scribe_ctrl_bin'], 'counters']
+        cmd = [self.config["scribe_ctrl_bin"], "counters"]
 
-        if self.config['scribe_port'] is not None:
-            cmd.append(self.config['scribe_port'])
+        if self.config["scribe_port"] is not None:
+            cmd.append(self.config["scribe_port"])
 
         self.log.debug("Running command %r", cmd)
 
         try:
-            p = subprocess.Popen(cmd, stdout=subprocess.PIPE,
-                                 stderr=subprocess.PIPE)
+            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
         except OSError:
             self.log.exception("Unable to run %r", cmd)
             return ""
 
         stdout, stderr = p.communicate()
 
         if p.wait() != 0:
@@ -63,15 +65,15 @@
 
     def get_scribe_stats(self):
         output = self.get_scribe_ctrl_output()
 
         data = {}
 
         for line in output.splitlines():
-            key, val = line.rsplit(':', 1)
+            key, val = line.rsplit(":", 1)
             metric = self.key_to_metric(key)
             data[metric] = int(val)
 
         return data.items()
 
     def collect(self):
         for stat, val in self.get_scribe_stats():
```

### Comparing `diamond-next-4.0.515/src/collectors/squid/squid.py` & `diamond-next-5.0.0/src/collectors/squid/squid.py`

 * *Files 20% similar despite different names*

```diff
@@ -5,100 +5,103 @@
 
 #### Dependencies
 
 """
 
 import re
 import socket
+
 import diamond.collector
 
 
 class SquidCollector(diamond.collector.Collector):
-
     def __init__(self, *args, **kwargs):
         self.host_pattern = re.compile("(([^@]+)@)?([^:]+)(:([0-9]+))?")
         self.stat_pattern = re.compile("^([^ ]+) = ([0-9\.]+)$")
 
         super(SquidCollector, self).__init__(*args, **kwargs)
 
     def process_config(self):
         super(SquidCollector, self).process_config()
         self.squid_hosts = {}
 
-        for host in self.config['hosts']:
+        for host in self.config["hosts"]:
             matches = self.host_pattern.match(host)
 
             if matches.group(5):
                 port = matches.group(5)
             else:
                 port = 3128
 
             if matches.group(2):
                 nick = matches.group(2)
             else:
                 nick = port
 
-            self.squid_hosts[nick] = {
-                'host': matches.group(3),
-                'port': int(port)
-            }
+            self.squid_hosts[nick] = {"host": matches.group(3), "port": int(port)}
 
     def get_default_config_help(self):
         config_help = super(SquidCollector, self).get_default_config_help()
-        config_help.update({
-            'hosts': 'List of hosts to collect from. Format is ' +
-            '[nickname@]host[:port], [nickname@]host[:port], etc',
-        })
+        config_help.update(
+            {
+                "hosts": "List of hosts to collect from. Format is "
+                + "[nickname@]host[:port], [nickname@]host[:port], etc",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(SquidCollector, self).get_default_config()
-        config.update({
-            'hosts': ['localhost:3128'],
-            'path': 'squid',
-        })
+        config.update(
+            {
+                "hosts": ["localhost:3128"],
+                "path": "squid",
+            }
+        )
         return config
 
     def _getData(self, host, port):
         try:
             squid_sock = socket.socket()
             squid_sock.connect((host, int(port)))
             squid_sock.settimeout(0.25)
             squid_sock.sendall(
-                "GET cache_object://localhost/counters HTTP/1.0\r\n" +
-                "Host: localhost\r\n" +
-                "Accept: */*\r\n" +
-                "Connection: close\r\n\r\n")
+                "GET cache_object://localhost/counters HTTP/1.0\r\n"
+                + "Host: localhost\r\n"
+                + "Accept: */*\r\n"
+                + "Connection: close\r\n\r\n"
+            )
 
-            fulldata = ''
+            fulldata = ""
 
             while True:
                 data = squid_sock.recv(1024)
                 if not data:
                     break
                 fulldata = fulldata + data
-        except Exception, e:
-            self.log.error('Couldnt connect to squid: %s', e)
+        except Exception as e:
+            self.log.error("Could not connect to squid: %s", e)
             return None
         squid_sock.close()
 
         return fulldata
 
     def collect(self):
         for nickname in self.squid_hosts.keys():
             squid_host = self.squid_hosts[nickname]
 
-            fulldata = self._getData(squid_host['host'],
-                                     squid_host['port'])
+            fulldata = self._getData(squid_host["host"], squid_host["port"])
 
             if fulldata is not None:
                 fulldata = fulldata.splitlines()
 
                 for data in fulldata:
                     matches = self.stat_pattern.match(data)
+
                     if matches:
-                        self.publish_counter("%s.%s" % (nickname,
-                                                        matches.group(1)),
-                                             float(matches.group(2)))
+                        self.publish_counter(
+                            "%s.%s" % (nickname, matches.group(1)),
+                            float(matches.group(2)),
+                        )
```

### Comparing `diamond-next-4.0.515/src/collectors/ipmisensor/ipmisensor.py` & `diamond-next-5.0.0/src/collectors/ipmisensor/ipmisensor.py`

 * *Files 21% similar despite different names*

```diff
@@ -8,58 +8,60 @@
 
 #### Dependencies
 
  * [ipmitool](http://openipmi.sourceforge.net/)
 
 """
 
-import diamond.collector
-from diamond.collector import str_to_bool
-from subprocess import Popen, PIPE
-import os
 import getpass
+import os
+import subprocess
+
+import diamond.collector
 
 
 class IPMISensorCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(IPMISensorCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'bin': 'Path to the ipmitool binary',
-            'use_sudo': 'Use sudo?',
-            'sudo_cmd': 'Path to sudo',
-            'thresholds': 'Collect thresholds as well as reading',
-            'delimiter': 'Parse blanks in sensor names into a delimiter'
-        })
+        config_help = super(IPMISensorCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "bin": "Path to the ipmitool binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+                "thresholds": "Collect thresholds as well as reading",
+                "delimiter": "Parse blanks in sensor names into a delimiter",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(IPMISensorCollector, self).get_default_config()
-        config.update({
-            'bin':              '/usr/bin/ipmitool',
-            'use_sudo':         False,
-            'sudo_cmd':         '/usr/bin/sudo',
-            'path':             'ipmi.sensors',
-            'thresholds':       False,
-            'delimiter':        '.'
-        })
+        config.update(
+            {
+                "bin": "/usr/bin/ipmitool",
+                "use_sudo": False,
+                "sudo_cmd": "/usr/bin/sudo",
+                "path": "ipmi.sensors",
+                "thresholds": False,
+                "delimiter": ".",
+            }
+        )
         return config
 
     def parse_value(self, value):
         """
         Convert value string to float for reporting
         """
         value = value.strip()
 
         # Skip missing sensors
-        if value == 'na':
+        if value == "na":
             return None
 
         # Try just getting the float value
         try:
             return float(value)
         except:
             pass
@@ -70,35 +72,36 @@
         except:
             pass
 
         # No luck, bail
         return None
 
     def collect(self):
-        use_sudo = str_to_bool(self.config['use_sudo'])
-        if ((not os.access(self.config['bin'], os.X_OK) or
-             (use_sudo and
-              not os.access(self.config['sudo_cmd'], os.X_OK)))):
+        use_sudo = diamond.collector.str_to_bool(self.config["use_sudo"])
+
+        if not os.access(self.config["bin"], os.X_OK) or (
+            use_sudo and not os.access(self.config["sudo_cmd"], os.X_OK)
+        ):
             return False
 
-        command = [self.config['bin'], 'sensor']
+        command = [self.config["bin"], "sensor"]
+
+        if use_sudo and getpass.getuser() != "root":
+            command.insert(0, self.config["sudo_cmd"])
 
-        if use_sudo and getpass.getuser() != 'root':
-            command.insert(0, self.config['sudo_cmd'])
+        p = subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0][:-1]
 
-        p = Popen(command, stdout=PIPE).communicate()[0][:-1]
+        for i, v in enumerate(p.split(b"\n")):
+            data = v.split(b"|")
 
-        for i, v in enumerate(p.split("\n")):
-            data = v.split("|")
             try:
                 # Complex keys are fun!
                 metric_name = data[0].strip()
                 metric_name = metric_name.replace(".", "_")
-                metric_name = metric_name.replace(" ",
-                                                  self.config['delimiter'])
+                metric_name = metric_name.replace(" ", self.config["delimiter"])
                 metrics = []
 
                 # Each sensor line is a column seperated by a | with the
                 # following descriptions:
                 # 1. Sensor ID
                 # 2. Sensor Reading
                 # 3. Units
@@ -106,35 +109,50 @@
                 # 5. Lower Non-Recoverable
                 # 6. Lower Critical
                 # 7. Lower Non-Critical
                 # 8. Upper Non-Critical
                 # 9. Upper Critical
                 # 10. Upper Non-Recoverable
 
-                if not self.config['thresholds']:
+                if not self.config["thresholds"]:
                     metrics.append((metric_name, self.parse_value(data[1])))
                 else:
-                    metrics.append((metric_name + ".Reading",
-                                    self.parse_value(data[1])))
-                    metrics.append((metric_name + ".Lower.NonRecoverable",
-                                    self.parse_value(data[4])))
-                    metrics.append((metric_name + ".Lower.Critical",
-                                    self.parse_value(data[5])))
-                    metrics.append((metric_name + ".Lower.NonCritical",
-                                    self.parse_value(data[6])))
-                    metrics.append((metric_name + ".Upper.NonCritical",
-                                    self.parse_value(data[7])))
-                    metrics.append((metric_name + ".Upper.Critical",
-                                    self.parse_value(data[8])))
-                    metrics.append((metric_name + ".Upper.NonRecoverable",
-                                    self.parse_value(data[9])))
-
-                [self.publish(name, value)
-                 for (name, value) in metrics
-                 if value is not None]
+                    metrics.append(
+                        (metric_name + ".Reading", self.parse_value(data[1]))
+                    )
+                    metrics.append(
+                        (
+                            metric_name + ".Lower.NonRecoverable",
+                            self.parse_value(data[4]),
+                        )
+                    )
+                    metrics.append(
+                        (metric_name + ".Lower.Critical", self.parse_value(data[5]))
+                    )
+                    metrics.append(
+                        (metric_name + ".Lower.NonCritical", self.parse_value(data[6]))
+                    )
+                    metrics.append(
+                        (metric_name + ".Upper.NonCritical", self.parse_value(data[7]))
+                    )
+                    metrics.append(
+                        (metric_name + ".Upper.Critical", self.parse_value(data[8]))
+                    )
+                    metrics.append(
+                        (
+                            metric_name + ".Upper.NonRecoverable",
+                            self.parse_value(data[9]),
+                        )
+                    )
+
+                [
+                    self.publish(name, value)
+                    for (name, value) in metrics
+                    if value is not None
+                ]
 
             except ValueError:
                 continue
             except IndexError:
                 continue
 
         return True
```

### Comparing `diamond-next-4.0.515/src/collectors/ipvs/ipvs.py` & `diamond-next-5.0.0/src/collectors/ipvs/ipvs.py`

 * *Files 17% similar despite different names*

```diff
@@ -5,167 +5,188 @@
 
 #### Dependencies
 
  * /usr/sbin/ipvsadmin
 
 """
 
-import diamond.collector
-import subprocess
 import os
-import string
-from diamond.collector import str_to_bool
+import subprocess
 
+import diamond.collector
 
-class IPVSCollector(diamond.collector.Collector):
 
+class IPVSCollector(diamond.collector.Collector):
     def process_config(self):
         super(IPVSCollector, self).process_config()
+
         # Verify the --exact flag works
-        self.statcommand = [self.config['bin'], '--list', '--stats',
-                            '--numeric', '--exact']
-        self.concommand = [self.config['bin'], '--list', '--numeric']
-
-        if str_to_bool(self.config['use_sudo']):
-            self.statcommand.insert(0, self.config['sudo_cmd'])
-            self.concommand.insert(0, self.config['sudo_cmd'])
-            # The -n (non-interactive) option prevents sudo from
-            # prompting the user for a password.
-            self.statcommand.insert(1, '-n')
-            self.concommand.insert(1, '-n')
+        self.statcommand = [
+            self.config["bin"],
+            "--list",
+            "--stats",
+            "--numeric",
+            "--exact",
+        ]
+        self.concommand = [self.config["bin"], "--list", "--numeric"]
+
+        if diamond.collector.str_to_bool(self.config["use_sudo"]):
+            self.statcommand.insert(0, self.config["sudo_cmd"])
+            self.concommand.insert(0, self.config["sudo_cmd"])
+
+            # The -n (non-interactive) option prevents sudo from prompting the user for a password.
+            self.statcommand.insert(1, "-n")
+            self.concommand.insert(1, "-n")
 
     def get_default_config_help(self):
         config_help = super(IPVSCollector, self).get_default_config_help()
-        config_help.update({
-            'bin': 'Path to ipvsadm binary',
-            'use_sudo': 'Use sudo?',
-            'sudo_cmd': 'Path to sudo',
-        })
+        config_help.update(
+            {
+                "bin": "Path to ipvsadm binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(IPVSCollector, self).get_default_config()
-        config.update({
-            'bin':              '/usr/sbin/ipvsadm',
-            'use_sudo':         True,
-            'sudo_cmd':         '/usr/bin/sudo',
-            'path':             'ipvs'
-        })
+        config.update(
+            {
+                "bin": "/usr/sbin/ipvsadm",
+                "use_sudo": True,
+                "sudo_cmd": "/usr/bin/sudo",
+                "path": "ipvs",
+            }
+        )
+
         return config
 
     def collect(self):
-        if not os.access(self.config['bin'], os.X_OK):
-            self.log.error("%s does not exist, or is not executable",
-                           self.config['bin'])
+        if not os.access(self.config["bin"], os.X_OK):
+            self.log.error(
+                "%s does not exist, or is not executable", self.config["bin"]
+            )
+
             return False
 
-        if ((str_to_bool(self.config['use_sudo']) and
-             not os.access(self.config['sudo_cmd'], os.X_OK))):
-            self.log.error("%s does not exist, or is not executable",
-                           self.config['sudo_cmd'])
+        if diamond.collector.str_to_bool(self.config["use_sudo"]) and not os.access(
+            self.config["sudo_cmd"], os.X_OK
+        ):
+            self.log.error(
+                "%s does not exist, or is not executable", self.config["sudo_cmd"]
+            )
+
             return False
 
-        p = subprocess.Popen(self.statcommand, stdout=subprocess.PIPE,
-                             stderr=subprocess.PIPE)
+        p = subprocess.Popen(
+            self.statcommand, stdout=subprocess.PIPE, stderr=subprocess.PIPE
+        )
         p.wait()
 
         if p.returncode == 255:
-            self.statcommand = filter(
-                lambda a: a != '--exact', self.statcommand)
+            self.statcommand = filter(lambda a: a != "--exact", self.statcommand)
 
-        p = subprocess.Popen(self.statcommand,
-                             stdout=subprocess.PIPE).communicate()[0][:-1]
+        p = subprocess.Popen(self.statcommand, stdout=subprocess.PIPE).communicate()[0][
+            :-1
+        ]
 
         columns = {
-            'conns': 2,
-            'inpkts': 3,
-            'outpkts': 4,
-            'inbytes': 5,
-            'outbytes': 6,
+            "conns": 2,
+            "inpkts": 3,
+            "outpkts": 4,
+            "inbytes": 5,
+            "outbytes": 6,
         }
 
         external = ""
         backend = ""
+
         for i, line in enumerate(p.split("\n")):
             if i < 3:
                 continue
+
             row = line.split()
 
             if row[0] == "TCP" or row[0] == "UDP":
-                external = row[0] + "_" + string.replace(row[1], ".", "_")
+                external = row[0] + "_" + row[1].replace(".", "_")
                 backend = "total"
             elif row[0] == "->":
-                backend = string.replace(row[1], ".", "_")
+                backend = row[1].replace(".", "_")
             else:
                 continue
 
-            for metric, column in columns.iteritems():
+            for metric, column in iter(columns.items()):
                 metric_name = ".".join([external, backend, metric])
                 # metric_value = int(row[column])
                 value = row[column]
-                if value.endswith('K'):
-                    metric_value = int(value[0:len(value) - 1]) * 1024
-                elif value.endswith('M'):
-                    metric_value = (int(value[0:len(value) - 1]) * 1024 * 1024)
-                elif value.endswith('G'):
-                    metric_value = (
-                        int(value[0:len(value) - 1]) * 1024 * 1024 * 1024)
+
+                if value.endswith("K"):
+                    metric_value = int(value[0 : len(value) - 1]) * 1024
+                elif value.endswith("M"):
+                    metric_value = int(value[0 : len(value) - 1]) * 1024 * 1024
+                elif value.endswith("G"):
+                    metric_value = int(value[0 : len(value) - 1]) * 1024 * 1024 * 1024
                 else:
                     metric_value = float(value)
 
                 self.publish(metric_name, metric_value)
 
-        p = subprocess.Popen(self.concommand,
-                             stdout=subprocess.PIPE).communicate()[0][:-1]
+        p = subprocess.Popen(self.concommand, stdout=subprocess.PIPE).communicate()[0][
+            :-1
+        ]
 
         columns = {
-            'active': 4,
-            'inactive': 5,
+            "active": 4,
+            "inactive": 5,
         }
 
         external = ""
         backend = ""
         total = {}
+
         for i, line in enumerate(p.split("\n")):
             if i < 3:
                 continue
+
             row = line.split()
 
             if row[0] == "TCP" or row[0] == "UDP":
                 if total:
-                    for metric, value in total.iteritems():
-                        self.publish(
-                            ".".join([external, "total", metric]), value)
+                    for metric, value in iter(total.items()):
+                        self.publish(".".join([external, "total", metric]), value)
 
                 for k in columns.keys():
                     total[k] = 0.0
 
-                external = row[0] + "_" + string.replace(row[1], ".", "_")
+                external = row[0] + "_" + row[1].replace(".", "_")
+
                 continue
             elif row[0] == "->":
-                backend = string.replace(row[1], ".", "_")
+                backend = row[1].replace(".", "_")
             else:
                 continue
 
-            for metric, column in columns.iteritems():
+            for metric, column in iter(columns.items()):
                 metric_name = ".".join([external, backend, metric])
                 # metric_value = int(row[column])
                 value = row[column]
-                if value.endswith('K'):
-                    metric_value = int(value[0:len(value) - 1]) * 1024
-                elif value.endswith('M'):
-                    metric_value = int(value[0:len(value) - 1]) * 1024 * 1024
-                elif value.endswith('G'):
-                    metric_value = (
-                        int(value[0:len(value) - 1]) * 1024 * 1024 * 1024)
+
+                if value.endswith("K"):
+                    metric_value = int(value[0 : len(value) - 1]) * 1024
+                elif value.endswith("M"):
+                    metric_value = int(value[0 : len(value) - 1]) * 1024 * 1024
+                elif value.endswith("G"):
+                    metric_value = int(value[0 : len(value) - 1]) * 1024 * 1024 * 1024
                 else:
                     metric_value = float(value)
 
                 total[metric] += metric_value
                 self.publish(metric_name, metric_value)
 
         if total:
-            for metric, value in total.iteritems():
+            for metric, value in iter(total.items()):
                 self.publish(".".join([external, "total", metric]), value)
```

### Comparing `diamond-next-4.0.515/src/collectors/files/files.py` & `diamond-next-5.0.0/src/collectors/files/files.py`

 * *Files 15% similar despite different names*

```diff
@@ -3,58 +3,65 @@
 """
 This class collects data from plain text files
 
 #### Dependencies
 
 """
 
-import diamond.collector
 import os
 import re
 
-_RE = re.compile(r'([A-Za-z0-9._-]+)[\s=:]+(-?[0-9]+)(\.?\d*)')
+import diamond.collector
 
+_RE = re.compile(r"([A-Za-z0-9._-]+)[\s=:]+(-?[0-9]+)(\.?\d*)")
 
-class FilesCollector(diamond.collector.Collector):
 
+class FilesCollector(diamond.collector.Collector):
     def get_default_config_help(self):
         config_help = super(FilesCollector, self).get_default_config_help()
-        config_help.update({
-            'path': 'Prefix added to all stats collected by this module, a '
-                    'single dot means don''t add prefix',
-            'dir': 'The directory that the performance files are in',
-            'delete': 'Delete files after they are picked up',
-        })
+        config_help.update(
+            {
+                "path": "Prefix added to all stats collected by this module, a "
+                "single dot means don"
+                "t add prefix",
+                "dir": "The directory that the performance files are in",
+                "delete": "Delete files after they are picked up",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns default collector settings.
         """
         config = super(FilesCollector, self).get_default_config()
-        config.update({
-            'path': '.',
-            'dir': '/tmp/diamond',
-            'delete': False,
-        })
+        config.update(
+            {
+                "path": ".",
+                "dir": "/tmp/diamond",
+                "delete": False,
+            }
+        )
         return config
 
     def collect(self):
-        if os.path.exists(self.config['dir']):
-            for fn in os.listdir(self.config['dir']):
-                if os.path.isfile(os.path.join(self.config['dir'], fn)):
+        if os.path.exists(self.config["dir"]):
+            for fn in os.listdir(self.config["dir"]):
+                if os.path.isfile(os.path.join(self.config["dir"], fn)):
                     try:
-                        fh = open(os.path.join(self.config['dir'], fn))
+                        fh = open(os.path.join(self.config["dir"], fn))
                         found = False
                         for line in fh:
                             m = _RE.match(line)
-                            if (m):
+                            if m:
                                 self.publish(
                                     m.groups()[0],
                                     m.groups()[1] + m.groups()[2],
-                                    precision=max(0, len(m.groups()[2]) - 1))
+                                    precision=max(0, len(m.groups()[2]) - 1),
+                                )
                                 found = True
                         fh.close()
-                        if (found and self.config['delete']):
-                            os.unlink(os.path.join(self.config['dir'], fn))
+
+                        if found and self.config["delete"]:
+                            os.unlink(os.path.join(self.config["dir"], fn))
                     except:
                         pass
```

### Comparing `diamond-next-4.0.515/src/collectors/http/http.py` & `diamond-next-5.0.0/src/collectors/httpc/httpc.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # coding=utf-8
 
 """
 Collect statistics from a HTTP or HTTPS connexion
 
 #### Dependencies
 
- * urllib2
+ * urllib
 
 #### Usage
 Add the collector config as :
 
 enabled = True
 ttl_multiplier = 2
 path_suffix = ""
@@ -23,71 +23,71 @@
     - servers.<hostname>.http.<url>.time (time to download the page in microsec)
 
     '.' and '/' chars are replaced by __, url looking like
        http://www.site.com/admin/page.html are replaced by
        http:__www_site_com_admin_page_html
 """
 
-import urllib2
-import diamond.collector
 import datetime
+import urllib.request
+
+import diamond.collector
 
 
 class HttpCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(HttpCollector, self).get_default_config_help()
-        config_help.update({
-            'req_port': 'Port',
-            'req_url':
-            'array of full URL to get (ex : https://www.ici.net/mypage.html)',
-            'req_vhost':
-            'Host header variable if needed. Will be added to every request',
-        })
+        config_help.update(
+            {
+                "req_port": "Port",
+                "req_url": "array of full URL to get (ex : https://www.ici.net/mypage.html)",
+                "req_vhost": "Host header variable if needed. Will be added to every request",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         default_config = super(HttpCollector, self).get_default_config()
-        default_config['path'] = 'http'
-        default_config['req_vhost'] = ''
-        default_config['req_url'] = ['http://localhost/']
+        default_config["path"] = "http"
+        default_config["req_vhost"] = ""
+        default_config["req_url"] = ["http://localhost/"]
 
-        default_config['headers'] = {'User-Agent': 'Diamond HTTP collector', }
+        default_config["headers"] = {"User-Agent": "Diamond HTTP collector"}
         return default_config
 
     def collect(self):
-        # create urllib2 vars
-        if self.config['req_vhost'] != "":
-            self.config['headers']['Host'] = self.config['req_vhost']
+        # create urllib vars
+        if self.config["req_vhost"] != "":
+            self.config["headers"]["Host"] = self.config["req_vhost"]
 
         # time the request
-        for url in self.config['req_url']:
+        for url in self.config["req_url"]:
             self.log.debug("collecting %s", str(url))
             req_start = datetime.datetime.now()
-            req = urllib2.Request(url, headers=self.config['headers'])
+            req = urllib.request.Request(url, headers=self.config["headers"])
+
             try:
-                handle = urllib2.urlopen(req)
+                handle = urllib.request.urlopen(req)
                 the_page = handle.read()
                 req_end = datetime.datetime.now()
                 req_time = req_end - req_start
 
                 # build a compatible name : no '.' and no'/' in the name
-                metric_name = url.replace(
-                    '/', '_').replace(
-                    '.', '_').replace(
-                    '\\', '').replace(
-                    ':', '')
+                metric_name = (
+                    url.replace("/", "_")
+                    .replace(".", "_")
+                    .replace("\\", "")
+                    .replace(":", "")
+                )
                 # metric_name = url.split("/")[-1].replace(".", "_")
-                if metric_name == '':
+                if metric_name == "":
                     metric_name = "root"
                 self.publish_gauge(
-                    metric_name + '.time',
-                    req_time.seconds * 1000000 + req_time.microseconds)
-                self.publish_gauge(
-                    metric_name + '.size',
-                    len(the_page))
-
-            except IOError, e:
-                self.log.error("Unable to open %s",
-                               self.config['req_url'])
-            except Exception, e:
+                    metric_name + ".time",
+                    req_time.seconds * 1000000 + req_time.microseconds,
+                )
+                self.publish_gauge(metric_name + ".size", len(the_page))
+
+            except IOError:
+                self.log.error("Unable to open %s", self.config["req_url"])
+            except Exception as e:
                 self.log.error("Unknown error opening url: %s", e)
```

### Comparing `diamond-next-4.0.515/src/collectors/pgbouncer/pgbouncer.py` & `diamond-next-5.0.0/src/collectors/pgbouncer/pgbouncer.py`

 * *Files 24% similar despite different names*

```diff
@@ -22,107 +22,114 @@
 host = localhost
 port = 6433
 password = foobar
 ```
 
 """
 
-from collections import defaultdict
+import collections
 
 import diamond.collector
 
 try:
     import psycopg2
     import psycopg2.extras
+
     psycopg2  # workaround for pyflakes issue #13
 except ImportError:
     psycopg2 = None
 
-STATS_QUERIES = ['SHOW POOLS', 'SHOW STATS']
-IGNORE_COLUMNS = ['user']
+STATS_QUERIES = ["SHOW POOLS", "SHOW STATS"]
+IGNORE_COLUMNS = ["user"]
 
 
 class PgbouncerCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(PgbouncerCollector, self).get_default_config_help()
-        config_help.update({
-            'user': 'Username',
-            'password': 'Password',
-            'instances': 'A subcategory of pgbouncer instances with a host '
-                         'and port, and optionally user and password can be '
-                         'overridden per instance (see example).',
-        })
+        config_help.update(
+            {
+                "user": "Username",
+                "password": "Password",
+                "instances": "A subcategory of pgbouncer instances with a host "
+                "and port, and optionally user and password can be "
+                "overridden per instance (see example).",
+            }
+        )
 
         return config_help
 
     def get_default_config(self):
         config = super(PgbouncerCollector, self).get_default_config()
-        config.update({
-            'path': 'pgbouncer',
-            'method': 'Threaded',
-            'user': 'postgres',
-            'password': '',
-            'instances': {},
-        })
+        config.update(
+            {
+                "path": "pgbouncer",
+                "method": "Threaded",
+                "user": "postgres",
+                "password": "",
+                "instances": {},
+            }
+        )
 
         return config
 
     def collect(self):
         if psycopg2 is None:
-            self.log.error('Unable to import module psycopg2.')
+            self.log.error("Unable to import module psycopg2.")
+
             return {}
 
-        instances = self.config['instances']
+        instances = self.config["instances"]
+
         # HACK: setting default with subcategory messes up merging of configs,
         # so we only set the default if one wasn't provided.
         if not instances:
             instances = {
-                'default': {
-                    'host': 'localhost',
-                    'port': '6432',
+                "default": {
+                    "host": "localhost",
+                    "port": "6432",
                 }
             }
 
-        for name, instance in instances.iteritems():
-            host = instance['host']
-            port = instance['port']
-            user = instance.get('user') or self.config['user']
-            password = instance.get('password') or self.config['password']
-
-            for database, stats in self._get_stats_by_database(
-                    host, port, user, password).iteritems():
-                for stat_name, stat_value in stats.iteritems():
+        for name, instance in iter(instances.items()):
+            host = instance["host"]
+            port = instance["port"]
+            user = instance.get("user") or self.config["user"]
+            password = instance.get("password") or self.config["password"]
+
+            for database, stats in iter(
+                self._get_stats_by_database(host, port, user, password).items()
+            ):
+                for stat_name, stat_value in iter(stats.items()):
                     self.publish(
-                        self._get_metric_name(name, database, stat_name),
-                        stat_value)
+                        self._get_metric_name(name, database, stat_name), stat_value
+                    )
 
     def _get_metric_name(self, name, database, stat_name):
-        name = name.replace('.', '_').replace(':', '_').strip()
-        return '.'.join([name, database, stat_name])
+        name = name.replace(".", "_").replace(":", "_").strip()
+
+        return ".".join([name, database, stat_name])
 
     def _get_stats_by_database(self, host, port, user, password):
         # Mapping of database name -> stats.
-        databases = defaultdict(dict)
-        conn = psycopg2.connect(database='pgbouncer',
-                                user=user,
-                                password=password,
-                                host=host,
-                                port=port)
+        databases = collections.defaultdict(dict)
+        conn = psycopg2.connect(
+            database="pgbouncer", user=user, password=password, host=host, port=port
+        )
 
         # Avoid using transactions, set isolation level to autocommit
         conn.set_isolation_level(0)
 
         cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
 
         for query in STATS_QUERIES:
             cursor.execute(query)
+
             for row in cursor.fetchall():
                 stats = row.copy()
-                database = stats.pop('database')
+                database = stats.pop("database")
 
                 for ignore in IGNORE_COLUMNS:
                     if ignore in stats:
                         stats.pop(ignore)
 
                 databases[database].update(stats)
```

### Comparing `diamond-next-4.0.515/src/collectors/ossec/ossec.py` & `diamond-next-5.0.0/src/collectors/ossec/ossec.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,72 +11,78 @@
 
 #### Dependencies
 
  * /var/ossec/bin/agent_control
 
 """
 
-import diamond.collector
-from diamond.collector import str_to_bool
-import subprocess
 import re
+import subprocess
 
+import diamond.collector
 
-class OssecCollector(diamond.collector.Collector):
 
+class OssecCollector(diamond.collector.Collector):
     def get_default_config_help(self):
         config_help = super(OssecCollector, self).get_default_config_help()
-        config_help.update({
-            'bin': 'Path to agent_control binary',
-            'use_sudo': 'Use sudo?',
-            'sudo_cmd': 'Path to sudo',
-        })
+        config_help.update(
+            {
+                "bin": "Path to agent_control binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(OssecCollector, self).get_default_config()
-        config.update({
-            'bin':              '/var/ossec/bin/agent_control',
-            'use_sudo':         True,
-            'sudo_cmd':         '/usr/bin/sudo',
-            'path':             'ossec',
-        })
+        config.update(
+            {
+                "bin": "/var/ossec/bin/agent_control",
+                "use_sudo": True,
+                "sudo_cmd": "/usr/bin/sudo",
+                "path": "ossec",
+            }
+        )
         return config
 
     def collect(self):
-        command = [self.config['bin'], '-l']
+        command = [self.config["bin"], "-l"]
 
-        if str_to_bool(self.config['use_sudo']):
-            command.insert(0, self.config['sudo_cmd'])
+        if diamond.collector.str_to_bool(self.config["use_sudo"]):
+            command.insert(0, self.config["sudo_cmd"])
 
         try:
             p = subprocess.Popen(command, stdout=subprocess.PIPE)
             res = p.communicate()[0]
-        except Exception, e:
-            self.log.error('Unable to exec cmd: %s, because %s'
-                           % (' '.join(command), str(e)))
+        except Exception as e:
+            self.log.error(
+                "Unable to exec cmd: %s, because %s" % (" ".join(command), str(e))
+            )
             return
 
-        if res == '':
-            self.log.error('Empty result from exec cmd: %s'
-                           % (' '.join(command)))
+        if res == "":
+            self.log.error("Empty result from exec cmd: %s" % (" ".join(command)))
             return
 
         states = {}
+
         for line in res.split("\n"):
             #    ID: 000, Name: local-ossec-001.localdomain (server), IP:\
             # 127.0.0.1, Active/Local
-            if not line.startswith('   ID: '):
+            if not line.startswith("   ID: "):
                 continue
-            fragments = line.split(',')
+
+            fragments = line.split(",")
             state = fragments[-1].lstrip()
+
             if state not in states:
                 states[state] = 1
             else:
                 states[state] += 1
 
         for state, count in states.items():
-            name = 'agents.' + re.sub('[^a-z]', '_', state.lower())
+            name = "agents." + re.sub("[^a-z]", "_", state.lower())
             self.publish(name, count)
```

### Comparing `diamond-next-4.0.515/src/collectors/disktemp/disktemp.py` & `diamond-next-5.0.0/src/collectors/disktemp/disktemp.py`

 * *Files 18% similar despite different names*

```diff
@@ -14,87 +14,91 @@
 """
 
 import os
 import re
 import subprocess
 
 import diamond.collector
-from diamond.collector import str_to_bool
 
 
 class DiskTemperatureCollector(diamond.collector.Collector):
-
     def process_config(self):
         super(DiskTemperatureCollector, self).process_config()
-        self.devices = re.compile(self.config['devices'])
+        self.devices = re.compile(self.config["devices"])
 
     def get_default_config_help(self):
-        config_help = super(DiskTemperatureCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'devices': "device regex to collect stats on",
-            'bin':         'The path to the hddtemp binary',
-            'use_sudo':    'Use sudo?',
-            'sudo_cmd':    'Path to sudo',
-        })
+        config_help = super(DiskTemperatureCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "devices": "device regex to collect stats on",
+                "bin": "The path to the hddtemp binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns default configuration options.
         """
         config = super(DiskTemperatureCollector, self).get_default_config()
-        config.update({
-            'path': 'disktemp',
-            'bin': 'hddtemp',
-            'use_sudo': False,
-            'sudo_cmd': '/usr/bin/sudo',
-            'devices': '^disk[0-9]$|^sd[a-z]$|^hd[a-z]$'
-        })
+        config.update(
+            {
+                "path": "disktemp",
+                "bin": "hddtemp",
+                "use_sudo": False,
+                "sudo_cmd": "/usr/bin/sudo",
+                "devices": "^disk[0-9]$|^sd[a-z]$|^hd[a-z]$",
+            }
+        )
         return config
 
     def get_temp(self, device):
-        command = [self.config['bin'], '-n', device]
+        command = [self.config["bin"], "-n", device]
 
-        if str_to_bool(self.config['use_sudo']):
-            command.insert(0, self.config['sudo_cmd'])
+        if diamond.collector.str_to_bool(self.config["use_sudo"]):
+            command.insert(0, self.config["sudo_cmd"])
 
         return subprocess.Popen(command, stdout=subprocess.PIPE)
 
     def match_device(self, device, path):
         m = self.devices.match(device)
+
         if m:
             key = device
-            # If the regex has a capture group for pretty printing, pick
-            # the last matched capture group
+
+            # If the regex has a capture group for pretty printing, pick the last matched capture group
             if self.devices.groups > 0:
-                key = '.'.join(filter(None, [g for g in m.groups()]))
+                key = ".".join(filter(None, [g for g in m.groups()]))
 
-            return {key: self.get_temp(os.path.join('/dev', device))}
+            return {key: self.get_temp(os.path.join("/dev", device))}
 
         return {}
 
     def collect(self):
         """
         Collect and publish disk temperatures
         """
         instances = {}
 
         # Support disks such as /dev/(sd.*)
-        for device in os.listdir('/dev/'):
-            instances.update(self.match_device(device, '/dev/'))
+        for device in os.listdir("/dev/"):
+            instances.update(self.match_device(device, "/dev/"))
 
         # Support disk by id such as /dev/disk/by-id/wwn-(.*)
-        for device_id in os.listdir('/dev/disk/by-id/'):
-            instances.update(self.match_device(device, '/dev/disk/by-id/'))
+        for device in os.listdir("/dev/disk/by-id/"):
+            instances.update(self.match_device(device, "/dev/disk/by-id/"))
 
         metrics = {}
+
         for device, p in instances.items():
             output = p.communicate()[0].strip()
 
             try:
                 metrics[device + ".Temperature"] = float(output)
             except:
-                self.log.warn('Disk temperature retrieval failed on ' + device)
+                self.log.warn("Disk temperature retrieval failed on " + device)
 
         for metric in metrics.keys():
             self.publish(metric, metrics[metric])
```

### Comparing `diamond-next-4.0.515/src/collectors/example/example.py` & `diamond-next-5.0.0/src/collectors/example/example.py`

 * *Files 5% similar despite different names*

```diff
@@ -17,14 +17,21 @@
 form, they need to implement a single method called "collect".
 
     import diamond.collector
 
     class ExampleCollector(diamond.collector.Collector):
 
         def collect(self):
+
+            # Set Metric Path. By default it will be the collector's name
+            # (servers.hostname.ExampleCollector.my.example.metric)
+            self.config.update({
+                'path':     'example',
+            })
+
             # Set Metric Name
             metric_name = "my.example.metric"
 
             # Set Metric Value
             metric_value = 42
 
             # Publish Metric
@@ -44,32 +51,32 @@
     collectors_path = /tmp/diamond/collectors/
     collectors_config_path = /tmp/diamond/collectors/
 
     collectors_reload_interval = 3600
 
     [handlers]
 
-    [[default]]
+    [[handlersDefault]]
 
     [[ArchiveHandler]]
     log_file = /dev/stdout
 
     [collectors]
-    [[default]]
+    [[collectorsDefault]]
 
 and then run diamond in foreground mode:
 
     # diamond -f -l --skip-pidfile -c /tmp/diamond/diamond.conf
 
 Diamond supports dynamic addition of collectors. Its configured to scan for new
 collectors on a regular interval (configured in diamond.cfg).
 If diamond detects a new collector, or that a collectors module has changed
 (based on the file's mtime), it will be reloaded.
 
-Diamond looks for collectors in /usr/lib/diamond/collectors/ (on Ubuntu). By
+Diamond looks for collectors in /usr/share/diamond/collectors/ (on Ubuntu). By
 default diamond will invoke the *collect* method every 60 seconds.
 
 Diamond collectors that require a separate configuration file should place a
 .cfg file in /etc/diamond/collectors/.
 The configuration file name should match the name of the diamond collector
 class.  For example, a collector called
 *examplecollector.ExampleCollector* could have its configuration file placed in
@@ -77,29 +84,25 @@
 
 """
 
 import diamond.collector
 
 
 class ExampleCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(ExampleCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ExampleCollector, self).get_default_config()
-        config.update({
-            'path':     'example'
-        })
+        config.update({"path": "example"})
         return config
 
     def collect(self):
         """
         Overrides the Collector.collect method
         """
```

### Comparing `diamond-next-4.0.515/src/collectors/sockstat/sockstat.py` & `diamond-next-5.0.0/src/collectors/numa/numa.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,70 +1,62 @@
+#!/usr/bin/env python
 # coding=utf-8
 
 """
-Uses /proc/net/sockstat to collect data on number of open sockets
+This class collects data on NUMA utilization
 
 #### Dependencies
 
- * /proc/net/sockstat
+* numactl
 
 """
 
-import diamond.collector
+import logging
 import re
-import os
-from collections import defaultdict
+import subprocess
+
+import diamond.collector
 
+node_re = re.compile("(?P<node>^node d+ (free|size)): (?P<size>d+) MB")
 
-_RE = re.compile('|'.join([
-    r'sockets: used (?P<used>\d+)?',
-    r'(TCP|TCP6): inuse (?P<tcp_inuse>\d+)' +
-    r'( orphan (?P<tcp_orphan>\d+) ' +
-    r'tw (?P<tcp_tw>\d+) ' +
-    r'alloc (?P<tcp_alloc>\d+) ' +
-    r'mem (?P<tcp_mem>\d+))?',
-    r'(UDP|UDP6): inuse (?P<udp_inuse>\d+)( mem (?P<udp_mem>\d+))?'
-]))
-
-
-class SockstatCollector(diamond.collector.Collector):
-
-    PROCS = ['/proc/net/sockstat', '/proc/net/sockstat6']
-
-    def get_default_config_help(self):
-        config_help = super(SockstatCollector, self).get_default_config_help()
-        config_help.update({
-        })
-        return config_help
 
+class NumaCollector(diamond.collector.Collector):
     def get_default_config(self):
         """
         Returns the default collector settings
         """
-        config = super(SockstatCollector, self).get_default_config()
-        config.update({
-            'path':     'sockets',
-        })
+        config = super(NumaCollector, self).get_default_config()
+        config.update(
+            {
+                "path": "numa",
+                "bin": self.find_binary("numactl"),
+            }
+        )
+
         return config
 
     def collect(self):
-
-        result = defaultdict(int)
-        for path in self.PROCS:
-            if not os.access(path, os.R_OK):
+        p = subprocess.Popen(
+            [self.config["bin"], "--hardware"],
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+        )
+
+        output, errors = p.communicate()
+
+        lines = str(output).split("\n")
+
+        for line in lines:
+            try:
+                match = node_re.search(str(line))
+
+                if match:
+                    logging.debug(
+                        "Matched: %s %s" % (match.group("node"), match.group("size"))
+                    )
+                    metric_name = "%s_MB" % match.group("node").replace(" ", "_")
+                    metric_value = int(match.group("size"))
+                    logging.debug("Publishing %s %s" % (metric_name, metric_value))
+                    self.publish(metric_name, metric_value)
+            except Exception as e:
+                logging.error("Failed because: %s" % str(e))
                 continue
-
-            f = open(path)
-            self.collect_stat(result, f)
-            f.close()
-
-        for key, value in result.items():
-            self.publish(key, value, metric_type='GAUGE')
-
-    def collect_stat(self, data, f):
-
-        for line in f:
-            match = _RE.match(line)
-            if match:
-                for key, value in match.groupdict().items():
-                    if value:
-                        data[key] += int(value)
```

### Comparing `diamond-next-4.0.515/src/collectors/puppetdashboard/puppetdashboard.py` & `diamond-next-5.0.0/src/collectors/puppetdashboard/puppetdashboard.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,63 +1,68 @@
 # coding=utf-8
 
 """
 Collect metrics from Puppet Dashboard
 
 #### Dependencies
 
- * urllib2
+ * urllib
 
 """
 
-import urllib2
 import re
+import urllib.request
+
 import diamond.collector
 
 
 class PuppetDashboardCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(PuppetDashboardCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': 'Hostname to collect from',
-            'port': 'Port number to collect from',
-            'path': 'Path to the dashboard',
-        })
+        config_help = super(PuppetDashboardCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "Hostname to collect from",
+                "port": "Port number to collect from",
+                "path": "Path to the dashboard",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(PuppetDashboardCollector, self).get_default_config()
-        config.update({
-            'host': 'localhost',
-            'port': 5678,
-            'path': 'puppetdashboard',
-        })
+        config.update(
+            {
+                "host": "localhost",
+                "port": 5678,
+                "path": "puppetdashboard",
+            }
+        )
         return config
 
     def collect(self):
         try:
-            response = urllib2.urlopen("http://%s:%s/" % (
-                self.config['host'], int(self.config['port'])))
-        except Exception, e:
-            self.log.error('Couldnt connect to puppet-dashboard: %s', e)
+            response = urllib.request.urlopen(
+                "http://%s:%s/" % (self.config["host"], int(self.config["port"]))
+            )
+        except Exception as e:
+            self.log.error("Could not connect to puppet-dashboard: %s", e)
             return {}
 
         for line in response.readlines():
             line = line.strip()
 
             if line == "":
                 continue
 
             try:
                 regex = re.compile(
-                    "<a href=\"/nodes/(?P<key>[\w.]+)\">(?P<count>[\d.]+)</a>")
+                    '<a href="/nodes/(?P<key>[\w.]+)">(?P<count>[\d.]+)</a>'
+                )
                 r = regex.search(line)
                 results = r.groupdict()
 
-                self.publish(results['key'], results['count'])
-            except Exception, e:
-                self.log.error('Couldnt parse the output: %s', e)
+                self.publish(results["key"], results["count"])
+            except Exception as e:
+                self.log.error("Could not parse the output: %s", e)
```

### Comparing `diamond-next-4.0.515/src/collectors/vmsfs/vmsfs.py` & `diamond-next-5.0.0/src/collectors/vmsfs/vmsfs.py`

 * *Files 14% similar despite different names*

```diff
@@ -5,74 +5,76 @@
 
 #### Dependencies
 
  * /sys/fs/vmsfs <- vmsfs package, vmsfs mounted
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
 
-class VMSFSCollector(diamond.collector.Collector):
 
-    SYSFS = '/sys/fs/vmsfs'
+class VMSFSCollector(diamond.collector.Collector):
+    SYSFS = "/sys/fs/vmsfs"
 
     VMSFS_STATS = {
-        'resident': ('cur_resident', 4096),
-        'allocated': ('cur_allocated', 4096)
+        "resident": ("cur_resident", 4096),
+        "allocated": ("cur_allocated", 4096),
     }
 
     def vmsfs_stats_read(self, filename):
         stats = {}
 
         # Open vmsfs sys info.
         stats_fd = None
+
         try:
             stats_fd = open(filename)
 
             for line in stats_fd:
                 tokens = line.split()
-                stats[tokens[0][0:-1]] = long(tokens[1])
+                stats[tokens[0][0:-1]] = int(tokens[1])
         except:
             if stats_fd:
                 stats_fd.close()
 
         return stats
 
-    def vmsfs_stats_dispatch(self, filename, prefix=''):
+    def vmsfs_stats_dispatch(self, filename, prefix=""):
         stats = self.vmsfs_stats_read(filename)
+
         for stat in self.VMSFS_STATS:
             name = self.VMSFS_STATS[stat][0]
             scale = self.VMSFS_STATS[stat][1]
+
             if name in stats:
                 self.publish(prefix + name, stats[name] * scale)
 
     def get_default_config_help(self):
         config_help = super(VMSFSCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(VMSFSCollector, self).get_default_config()
-        config.update({
-            'path':     'vmsfs'
-        })
+        config.update({"path": "vmsfs"})
+
         return config
 
     def collect(self):
         if not os.access(self.SYSFS, os.R_OK | os.X_OK):
             return None
 
         # Dispatch total stats.
-        self.vmsfs_stats_dispatch(os.path.join(self.SYSFS, 'stats'))
+        self.vmsfs_stats_dispatch(os.path.join(self.SYSFS, "stats"))
 
         # Dispatch per-generation stats.
         # NOTE: We do not currently report the per-generation statistics to
         # diamond. This is because we do not have a good strategy for
         # aggregating generation data and exposing it in a sensible way. There
         # are three strategies:
         #  1) Collect everything at the host level.
@@ -81,14 +83,13 @@
         #  2) Collect at the top-level (one virtual host per generation).
         #     Then the problem is finding the generation through UI tools, etc.
         #  3) Figure out some way to put the stats in each instance associated
         #     with that generation.
         # We favor (2) currently, but there's not much value in implementing it
         # until it can be exposed to the user.
         if False:
-            TO_IGNORE = ('stats', 'version',
-                         '00000000-0000-0000-0000-000000000000')
+            to_ignore = ("stats", "version", "00000000-0000-0000-0000-000000000000")
             files = os.listdir(self.SYSFS)
+
             for f in files:
-                if f not in TO_IGNORE:
-                    self.vmsfs_stats_dispatch('/sys/fs/vmsfs/' + f,
-                                              prefix=('%s.' % f))
+                if f not in to_ignore:
+                    self.vmsfs_stats_dispatch("/sys/fs/vmsfs/" + f, prefix=("%s." % f))
```

### Comparing `diamond-next-4.0.515/src/collectors/network/network.py` & `diamond-next-5.0.0/src/collectors/network/network.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,128 +6,140 @@
 
 #### Dependencies
 
  * /proc/net/dev
 
 """
 
-import diamond.collector
-from diamond.collector import str_to_bool
-import diamond.convertor
 import os
 import re
 
+import diamond.collector
+import diamond.convertor
+
 try:
     import psutil
 except ImportError:
     psutil = None
 
 
 class NetworkCollector(diamond.collector.Collector):
-
-    PROC = '/proc/net/dev'
+    PROC = "/proc/net/dev"
 
     def get_default_config_help(self):
         config_help = super(NetworkCollector, self).get_default_config_help()
-        config_help.update({
-            'interfaces': 'List of interface types to collect',
-            'greedy': 'Greedy match interfaces',
-        })
+        config_help.update(
+            {
+                "interfaces": "List of interface types to collect",
+                "greedy": "Greedy match interfaces",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(NetworkCollector, self).get_default_config()
-        config.update({
-            'path':         'network',
-            'interfaces':   ['eth', 'bond', 'em', 'p1p', 'eno', 'enp', 'ens',
-                             'enx'],
-            'byte_unit':    ['bit', 'byte'],
-            'greedy':       'true',
-        })
+        config.update(
+            {
+                "path": "network",
+                "interfaces": ["eth", "bond", "em", "p1p", "eno", "enp", "ens", "enx"],
+                "byte_unit": ["bit", "byte"],
+                "greedy": "true",
+            }
+        )
         return config
 
     def collect(self):
         """
         Collect network interface stats.
         """
 
         # Initialize results
         results = {}
 
         if os.access(self.PROC, os.R_OK):
-
             # Open File
             file = open(self.PROC)
             # Build Regular Expression
-            greed = ''
-            if str_to_bool(self.config['greedy']):
-                greed = '\S*'
-
-            exp = (('^(?:\s*)((?:%s)%s):(?:\s*)' +
-                    '(?P<rx_bytes>\d+)(?:\s*)' +
-                    '(?P<rx_packets>\w+)(?:\s*)' +
-                    '(?P<rx_errors>\d+)(?:\s*)' +
-                    '(?P<rx_drop>\d+)(?:\s*)' +
-                    '(?P<rx_fifo>\d+)(?:\s*)' +
-                    '(?P<rx_frame>\d+)(?:\s*)' +
-                    '(?P<rx_compressed>\d+)(?:\s*)' +
-                    '(?P<rx_multicast>\d+)(?:\s*)' +
-                    '(?P<tx_bytes>\d+)(?:\s*)' +
-                    '(?P<tx_packets>\w+)(?:\s*)' +
-                    '(?P<tx_errors>\d+)(?:\s*)' +
-                    '(?P<tx_drop>\d+)(?:\s*)' +
-                    '(?P<tx_fifo>\d+)(?:\s*)' +
-                    '(?P<tx_colls>\d+)(?:\s*)' +
-                    '(?P<tx_carrier>\d+)(?:\s*)' +
-                    '(?P<tx_compressed>\d+)(?:.*)$') %
-                   (('|'.join(self.config['interfaces'])), greed))
+            greed = ""
+
+            if diamond.collector.str_to_bool(self.config["greedy"]):
+                greed = "\S*"
+
+            exp = (
+                "^(?:\s*)((?:%s)%s):(?:\s*)"
+                + "(?P<rx_bytes>\d+)(?:\s*)"
+                + "(?P<rx_packets>\w+)(?:\s*)"
+                + "(?P<rx_errors>\d+)(?:\s*)"
+                + "(?P<rx_drop>\d+)(?:\s*)"
+                + "(?P<rx_fifo>\d+)(?:\s*)"
+                + "(?P<rx_frame>\d+)(?:\s*)"
+                + "(?P<rx_compressed>\d+)(?:\s*)"
+                + "(?P<rx_multicast>\d+)(?:\s*)"
+                + "(?P<tx_bytes>\d+)(?:\s*)"
+                + "(?P<tx_packets>\w+)(?:\s*)"
+                + "(?P<tx_errors>\d+)(?:\s*)"
+                + "(?P<tx_drop>\d+)(?:\s*)"
+                + "(?P<tx_fifo>\d+)(?:\s*)"
+                + "(?P<tx_colls>\d+)(?:\s*)"
+                + "(?P<tx_carrier>\d+)(?:\s*)"
+                + "(?P<tx_compressed>\d+)(?:.*)$"
+            ) % (("|".join(self.config["interfaces"])), greed)
             reg = re.compile(exp)
             # Match Interfaces
+
             for line in file:
                 match = reg.match(line)
+
                 if match:
                     device = match.group(1)
                     results[device] = match.groupdict()
+
             # Close File
             file.close()
         else:
             if not psutil:
-                self.log.error('Unable to import psutil')
-                self.log.error('No network metrics retrieved')
+                self.log.error("Unable to import psutil")
+                self.log.error("No network metrics retrieved")
                 return None
 
             network_stats = psutil.network_io_counters(True)
+
             for device in network_stats.keys():
                 network_stat = network_stats[device]
                 results[device] = {}
-                results[device]['rx_bytes'] = network_stat.bytes_recv
-                results[device]['tx_bytes'] = network_stat.bytes_sent
-                results[device]['rx_packets'] = network_stat.packets_recv
-                results[device]['tx_packets'] = network_stat.packets_sent
+                results[device]["rx_bytes"] = network_stat.bytes_recv
+                results[device]["tx_bytes"] = network_stat.bytes_sent
+                results[device]["rx_packets"] = network_stat.packets_recv
+                results[device]["tx_packets"] = network_stat.packets_sent
 
         for device in results:
             stats = results[device]
+
             for s, v in stats.items():
                 # Get Metric Name
-                metric_name = '.'.join([device, s])
+                metric_name = ".".join([device, s])
+
                 # Get Metric Value
-                metric_value = self.derivative(metric_name,
-                                               long(v),
-                                               diamond.collector.MAX_COUNTER)
+                metric_value = self.derivative(
+                    metric_name, int(v), diamond.collector.MAX_COUNTER
+                )
 
                 # Convert rx_bytes and tx_bytes
-                if s == 'rx_bytes' or s == 'tx_bytes':
-                    convertor = diamond.convertor.binary(value=metric_value,
-                                                         unit='byte')
+                if s == "rx_bytes" or s == "tx_bytes":
+                    convertor = diamond.convertor.binary(
+                        value=metric_value, unit="byte"
+                    )
 
-                    for u in self.config['byte_unit']:
+                    for u in self.config["byte_unit"]:
                         # Public Converted Metric
-                        self.publish(metric_name.replace('bytes', u),
-                                     convertor.get(unit=u), 2)
+                        self.publish(
+                            metric_name.replace("bytes", u), convertor.get(unit=u), 2
+                        )
                 else:
                     # Publish Metric Derivative
                     self.publish(metric_name, metric_value)
 
         return None
```

### Comparing `diamond-next-4.0.515/src/collectors/memory_cgroup/memory_cgroup.py` & `diamond-next-5.0.0/src/collectors/memory_cgroup/memory_cgroup.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,97 +23,108 @@
 Metrics with total_ prefixes - summarized data from children CGroups.
 
 #### Dependencies
 
 /sys/fs/cgroup/memory/memory.stat
 """
 
-import diamond.collector
-import diamond.convertor
 import os
 import re
 
+import diamond.collector
+import diamond.convertor
+
 _KEY_MAPPING = [
-    'cache',
-    'rss',
-    'swap',
-    'total_rss',
-    'total_cache',
-    'total_swap',
+    "cache",
+    "rss",
+    "swap",
+    "total_rss",
+    "total_cache",
+    "total_swap",
 ]
 
 
 class MemoryCgroupCollector(diamond.collector.Collector):
-
     def process_config(self):
         super(MemoryCgroupCollector, self).process_config()
-        self.memory_path = self.config['memory_path']
-        self.skip = self.config['skip']
+        self.memory_path = self.config["memory_path"]
+        self.skip = self.config["skip"]
+
         if not isinstance(self.skip, list):
             self.skip = [self.skip]
+
         self.skip = [re.compile(e) for e in self.skip]
 
     def should_skip(self, path):
         for skip_re in self.skip:
             if skip_re.search(path):
                 return True
+
         return False
 
     def get_default_config_help(self):
-        config_help = super(
-            MemoryCgroupCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(MemoryCgroupCollector, self).get_default_config_help()
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(MemoryCgroupCollector, self).get_default_config()
-        config.update({
-            'path':     'memory_cgroup',
-            'memory_path': '/sys/fs/cgroup/memory/',
-            'skip': [],
-        })
+        config.update(
+            {
+                "path": "memory_cgroup",
+                "memory_path": "/sys/fs/cgroup/memory/",
+                "skip": [],
+            }
+        )
+
         return config
 
     def collect(self):
         # find all memory.stat files
         matches = []
+
         for root, dirnames, filenames in os.walk(self.memory_path):
             if not self.should_skip(root):
                 for filename in filenames:
-                    if filename == 'memory.stat':
-                        # matches will contain a tuple contain path to
-                        # cpuacct.stat and the parent of the stat
-                        parent = root.replace(self.memory_path,
-                                              "").replace("/", ".")
-                        if parent == '':
-                            parent = 'system'
+                    if filename == "memory.stat":
+                        # matches will contain a tuple contain path to cpuacct.stat and the parent of the stat
+                        parent = root.replace(self.memory_path, "").replace("/", ".")
+
+                        if parent == "":
+                            parent = "system"
+
                         matches.append((parent, os.path.join(root, filename)))
 
         # Read metrics from cpuacct files
         results = {}
+
         for match in matches:
             results[match[0]] = {}
             stat_file = open(match[1])
             elements = [line.split() for line in stat_file]
             stat_file.close()
 
             for el in elements:
                 name, value = el
+
                 if name not in _KEY_MAPPING:
                     continue
-                for unit in self.config['byte_unit']:
+
+                for unit in self.config["byte_unit"]:
                     value = diamond.convertor.binary.convert(
-                        value=value, oldUnit='B', newUnit=unit)
+                        value=value, old_unit="B", new_unit=unit
+                    )
                     results[match[0]][name] = value
                     # TODO: We only support one unit node here. Fix it!
                     break
 
         # create metrics from collected utimes and stimes for cgroups
-        for parent, cpuacct in results.iteritems():
-            for key, value in cpuacct.iteritems():
-                metric_name = '.'.join([parent, key])
-                self.publish(metric_name, value, metric_type='GAUGE')
+        for parent, cpuacct in iter(results.items()):
+            for key, value in iter(cpuacct.items()):
+                metric_name = ".".join([parent, key])
+                self.publish(metric_name, value, metric_type="GAUGE")
+
         return True
```

### Comparing `diamond-next-4.0.515/src/collectors/portstat/portstat.py` & `diamond-next-5.0.0/src/collectors/portstat/portstat.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,75 +1,86 @@
 """
 The PortStatCollector collects metrics about ports listed in config file.
 
-##### Dependencies
+#### Dependencies
 
 * psutil
 
 """
 
-from collections import defaultdict
+import collections
+
 import diamond.collector
 
 try:
     import psutil
 except ImportError:
     psutil = None
 
 
 def get_port_stats(port):
     """
     Iterate over connections and count states for specified port
     :param port: port for which stats are collected
     :return: Counter with port states
     """
-    cnts = defaultdict(int)
+    cnts = collections.defaultdict(int)
+
     for c in psutil.net_connections():
         c_port = c.laddr[1]
+
         if c_port != port:
             continue
+
         status = c.status.lower()
         cnts[status] += 1
+
     return cnts
 
 
 class PortStatCollector(diamond.collector.Collector):
-
     def __init__(self, *args, **kwargs):
         super(PortStatCollector, self).__init__(*args, **kwargs)
         self.ports = {}
-        for port_name, cfg in self.config['port'].items():
+
+        for port_name, cfg in self.config["port"].items():
             port_cfg = {}
-            for key in ('number',):
+
+            for key in ("number",):
                 port_cfg[key] = cfg.get(key, [])
+
             self.ports[port_name] = port_cfg
 
     def get_default_config_help(self):
         config_help = super(PortStatCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         config = super(PortStatCollector, self).get_default_config()
-        config.update({
-            'path': 'port',
-            'port': {},
-        })
+        config.update(
+            {
+                "path": "port",
+                "port": {},
+            }
+        )
+
         return config
 
     def collect(self):
         """
         Overrides the Collector.collect method
         """
 
         if psutil is None:
-            self.log.error('Unable to import module psutil')
+            self.log.error("Unable to import module psutil")
+
             return {}
 
-        for port_name, port_cfg in self.ports.iteritems():
-            port = int(port_cfg['number'])
+        for port_name, port_cfg in iter(self.ports.items()):
+            port = int(port_cfg["number"])
             stats = get_port_stats(port)
 
-            for stat_name, stat_value in stats.iteritems():
-                metric_name = '%s.%s' % (port_name, stat_name)
+            for stat_name, stat_value in iter(stats.items()):
+                metric_name = "%s.%s" % (port_name, stat_name)
                 self.publish(metric_name, stat_value)
```

### Comparing `diamond-next-4.0.515/src/collectors/puppetagent/puppetagent.py` & `diamond-next-5.0.0/src/collectors/users/users.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,64 +1,74 @@
 # coding=utf-8
 
 """
-Collect stats from puppet agent's last_run_summary.yaml
+Collects the number of users logged in and shells per user
 
 #### Dependencies
 
- * yaml
+ * [pyutmp](http://software.clapper.org/pyutmp/)
+or
+ * [utmp] (python-utmp on Debian and derivatives)
 
 """
 
-try:
-    import yaml
-except ImportError:
-    yaml = None
-
 import diamond.collector
 
+try:
+    from pyutmp import UtmpFile
+except ImportError:
+    UtmpFile = None
+try:
+    from utmp import UtmpRecord
+    import UTMPCONST
+except ImportError:
+    UtmpRecord = None
 
-class PuppetAgentCollector(diamond.collector.Collector):
 
+class UsersCollector(diamond.collector.Collector):
     def get_default_config_help(self):
-        config_help = super(PuppetAgentCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'yaml_path': "Path to last_run_summary.yaml",
-        })
+        """
+        Returns the default collector help text
+        """
+        config_help = super(UsersCollector, self).get_default_config_help()
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
-        config = super(PuppetAgentCollector, self).get_default_config()
-        config.update({
-            'yaml_path': '/var/lib/puppet/state/last_run_summary.yaml',
-            'path':     'puppetagent',
-        })
+        config = super(UsersCollector, self).get_default_config()
+        config.update(
+            {
+                "path": "users",
+                "utmp": None,
+            }
+        )
+
         return config
 
-    def _get_summary(self):
-        summary_fp = open(self.config['yaml_path'], 'r')
+    def collect(self):
+        if UtmpFile is None and UtmpRecord is None:
+            self.log.error("Unable to import either pyutmp or python-utmp")
 
-        try:
-            summary = yaml.load(summary_fp)
-        finally:
-            summary_fp.close()
+            return False
 
-        return summary
+        metrics = {"total": 0}
 
-    def collect(self):
-        if yaml is None:
-            self.log.error('Unable to import yaml')
-            return
-
-        summary = self._get_summary()
-
-        for sect, data in summary.iteritems():
-            for stat, value in data.iteritems():
-                if value is None or isinstance(value, basestring):
-                    continue
+        if UtmpFile:
+            for utmp in UtmpFile(path=self.config["utmp"]):
+                if utmp.ut_user_process:
+                    metrics[utmp.ut_user] = metrics.get(utmp.ut_user, 0) + 1
+                    metrics["total"] = metrics["total"] + 1
+
+        if UtmpRecord:
+            for utmp in UtmpRecord(fname=self.config["utmp"]):
+                if utmp.ut_type == UTMPCONST.USER_PROCESS:
+                    metrics[utmp.ut_user] = metrics.get(utmp.ut_user, 0) + 1
+                    metrics["total"] = metrics["total"] + 1
+
+        for metric_name in metrics.keys():
+            self.publish(metric_name, metrics[metric_name])
 
-                metric = '.'.join([sect, stat])
-                self.publish(metric, value)
+        return True
```

### Comparing `diamond-next-4.0.515/src/collectors/entropy/entropy.py` & `diamond-next-5.0.0/src/collectors/entropy/entropy.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,30 +5,28 @@
 
 #### Dependencies
 
  * /proc/sys/kernel/random/entropy_avail
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
+
 
 class EntropyStatCollector(diamond.collector.Collector):
-
-    PROC = '/proc/sys/kernel/random/entropy_avail'
+    PROC = "/proc/sys/kernel/random/entropy_avail"
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(EntropyStatCollector, self).get_default_config()
-        config.update({
-            'path':     'entropy'
-        })
+        config.update({"path": "entropy"})
         return config
 
     def collect(self):
         if not os.access(self.PROC, os.R_OK):
             return None
 
         # open file
```

### Comparing `diamond-next-4.0.515/src/collectors/tokumx/tokumx.py` & `diamond-next-5.0.0/src/collectors/tokumx/tokumx.py`

 * *Files 18% similar despite different names*

```diff
@@ -6,267 +6,296 @@
 
 #### Dependencies
 
  * pymongo
 
 """
 
+import re
+
 import diamond.collector
 from diamond.collector import str_to_bool
-import re
 
 try:
     import pymongo
 except ImportError:
     pymongo = None
 
 try:
     from pymongo import ReadPreference
 except ImportError:
     ReadPreference = None
 
 
 class TokuMXCollector(diamond.collector.Collector):
-
     def __init__(self, *args, **kwargs):
         self.__totals = {}
         super(TokuMXCollector, self).__init__(*args, **kwargs)
 
     def get_default_config_help(self):
         config_help = super(TokuMXCollector, self).get_default_config_help()
-        config_help.update({
-            'hosts': 'Array of hostname(:port) elements to get metrics from'
-                     'Set an alias by prefixing host:port with alias@',
-            'host': 'A single hostname(:port) to get metrics from'
-                    ' (can be used instead of hosts and overrides it)',
-            'user': 'Username for authenticated login (optional)',
-            'passwd': 'Password for authenticated login (optional)',
-            'databases': 'A regex of which databases to gather metrics for.'
-                         ' Defaults to all databases.',
-            'ignore_collections': 'A regex of which collections to ignore.'
-                                  ' MapReduce temporary collections (tmp.mr.*)'
-                                  ' are ignored by default.',
-            'network_timeout': 'Timeout for mongodb connection (in seconds).'
-                               ' There is no timeout by default.',
-            'simple': 'Only collect the same metrics as mongostat.',
-            'translate_collections': 'Translate dot (.) to underscores (_)'
-                                     ' in collection names.'
-        })
+        config_help.update(
+            {
+                "hosts": "Array of hostname(:port) elements to get metrics from"
+                "Set an alias by prefixing host:port with alias@",
+                "host": "A single hostname(:port) to get metrics from"
+                " (can be used instead of hosts and overrides it)",
+                "user": "Username for authenticated login (optional)",
+                "passwd": "Password for authenticated login (optional)",
+                "databases": "A regex of which databases to gather metrics for."
+                " Defaults to all databases.",
+                "ignore_collections": "A regex of which collections to ignore."
+                " MapReduce temporary collections (tmp.mr.*)"
+                " are ignored by default.",
+                "network_timeout": "Timeout for mongodb connection (in seconds)."
+                " There is no timeout by default.",
+                "simple": "Only collect the same metrics as mongostat.",
+                "translate_collections": "Translate dot (.) to underscores (_)"
+                " in collection names.",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(TokuMXCollector, self).get_default_config()
-        config.update({
-            'path':      'mongo',
-            'hosts':     ['localhost'],
-            'user':      None,
-            'passwd':      None,
-            'databases': '.*',
-            'ignore_collections': '^tmp\.mr\.',
-            'network_timeout': None,
-            'simple': 'False',
-            'translate_collections': 'False'
-        })
+        config.update(
+            {
+                "path": "mongo",
+                "hosts": ["localhost"],
+                "user": None,
+                "passwd": None,
+                "databases": ".*",
+                "ignore_collections": "^tmp\.mr\.",
+                "network_timeout": None,
+                "simple": "False",
+                "translate_collections": "False",
+            }
+        )
         return config
 
     def collect(self):
-        """Collect number values from db.serverStatus() and db.engineStatus()"""
+        """Collect number values from db.server_status() and db.engine_status()"""
 
         if pymongo is None:
-            self.log.error('Unable to import pymongo')
+            self.log.error("Unable to import pymongo")
             return
 
         # we need this for backwards compatibility
-        if 'host' in self.config:
-            self.config['hosts'] = [self.config['host']]
+        if "host" in self.config:
+            self.config["hosts"] = [self.config["host"]]
 
         # convert network_timeout to integer
-        if self.config['network_timeout']:
-            self.config['network_timeout'] = int(
-                self.config['network_timeout'])
+        if self.config["network_timeout"]:
+            self.config["network_timeout"] = int(self.config["network_timeout"])
 
         # use auth if given
-        if 'user' in self.config:
-            user = self.config['user']
+        if "user" in self.config:
+            user = self.config["user"]
         else:
             user = None
 
-        if 'passwd' in self.config:
-            passwd = self.config['passwd']
+        if "passwd" in self.config:
+            passwd = self.config["passwd"]
         else:
             passwd = None
 
-        for host in self.config['hosts']:
-            if len(self.config['hosts']) == 1:
+        for host in self.config["hosts"]:
+            if len(self.config["hosts"]) == 1:
                 # one host only, no need to have a prefix
                 base_prefix = []
             else:
-                matches = re.search('((.+)\@)?(.+)?', host)
+                matches = re.search("((.+)\@)?(.+)?", host)
                 alias = matches.group(2)
                 host = matches.group(3)
 
                 if alias is None:
-                    base_prefix = [re.sub('[:\.]', '_', host)]
+                    base_prefix = [re.sub("[:\.]", "_", host)]
                 else:
                     base_prefix = [alias]
 
             try:
                 if ReadPreference is None:
                     conn = pymongo.Connection(
                         host,
-                        network_timeout=self.config['network_timeout'],
-                        slave_okay=True
+                        network_timeout=self.config["network_timeout"],
+                        slave_okay=True,
                     )
                 else:
                     conn = pymongo.Connection(
                         host,
-                        network_timeout=self.config['network_timeout'],
+                        network_timeout=self.config["network_timeout"],
                         read_preference=ReadPreference.SECONDARY,
                     )
-            except Exception, e:
-                self.log.error('Couldnt connect to mongodb: %s', e)
+            except Exception as e:
+                self.log.error("Could not connect to mongodb: %s", e)
                 continue
 
             # try auth
             if user:
                 try:
                     conn.admin.authenticate(user, passwd)
-                except Exception, e:
+                except Exception as e:
                     self.log.error(
-                        'User auth given, but could not autheticate' +
-                        ' with host: %s, err: %s' % (host, e))
-                    return{}
-
-            serverStatus = conn.db.command('serverStatus')
-            engineStatus = conn.db.command('engineStatus')
-            data = dict(serverStatus.items() + engineStatus.items())
+                        "User auth given, but could not autheticate"
+                        + " with host: %s, err: %s" % (host, e)
+                    )
+
+                    return {}
+
+            server_status = conn.db.command("server_status")
+            engine_status = conn.db.command("engine_status")
+            data = dict(server_status.items() + engine_status.items())
 
             self._publish_transformed(data, base_prefix)
-            if str_to_bool(self.config['simple']):
+
+            if str_to_bool(self.config["simple"]):
                 data = self._extract_simple_data(data)
 
             self._publish_dict_with_prefix(data, base_prefix)
-            db_name_filter = re.compile(self.config['databases'])
-            ignored_collections = re.compile(self.config['ignore_collections'])
+            db_name_filter = re.compile(self.config["databases"])
+            ignored_collections = re.compile(self.config["ignore_collections"])
+
             for db_name in conn.database_names():
                 if not db_name_filter.search(db_name):
                     continue
-                db_stats = conn[db_name].command('dbStats')
-                db_prefix = base_prefix + ['databases', db_name]
+
+                db_stats = conn[db_name].command("dbStats")
+                db_prefix = base_prefix + ["databases", db_name]
                 self._publish_dict_with_prefix(db_stats, db_prefix)
+
                 for collection_name in conn[db_name].collection_names():
                     if ignored_collections.search(collection_name):
                         continue
-                    collection_stats = conn[db_name].command('collstats',
-                                                             collection_name)
-                    if str_to_bool(self.config['translate_collections']):
-                        collection_name = collection_name.replace('.', '_')
+
+                    collection_stats = conn[db_name].command(
+                        "collstats", collection_name
+                    )
+
+                    if str_to_bool(self.config["translate_collections"]):
+                        collection_name = collection_name.replace(".", "_")
+
                     collection_prefix = db_prefix + [collection_name]
-                    self._publish_dict_with_prefix(collection_stats,
-                                                   collection_prefix)
+                    self._publish_dict_with_prefix(collection_stats, collection_prefix)
 
     def _publish_transformed(self, data, base_prefix):
-        """ Publish values of type: counter or percent """
-        self._publish_dict_with_prefix(data.get('opcounters', {}),
-                                       base_prefix + ['opcounters_per_sec'],
-                                       self.publish_counter)
-        self._publish_dict_with_prefix(data.get('opcountersRepl', {}),
-                                       base_prefix +
-                                       ['opcountersRepl_per_sec'],
-                                       self.publish_counter)
-        self._publish_dict_with_prefix(data.get('network', {}),
-                                       base_prefix + ['network_per_sec'],
-                                       self.publish_counter)
-        self._publish_metrics(base_prefix + ['extra_info_per_sec'],
-                              'page_faults',
-                              data.get('extra_info', {}),
-                              self.publish_counter)
+        """Publish values of type: counter or percent"""
+        self._publish_dict_with_prefix(
+            data.get("opcounters", {}),
+            base_prefix + ["opcounters_per_sec"],
+            self.publish_counter,
+        )
+        self._publish_dict_with_prefix(
+            data.get("opcountersRepl", {}),
+            base_prefix + ["opcountersRepl_per_sec"],
+            self.publish_counter,
+        )
+        self._publish_dict_with_prefix(
+            data.get("network", {}),
+            base_prefix + ["network_per_sec"],
+            self.publish_counter,
+        )
+        self._publish_metrics(
+            base_prefix + ["extra_info_per_sec"],
+            "page_faults",
+            data.get("extra_info", {}),
+            self.publish_counter,
+        )
 
         def get_dotted_value(data, key_name):
-            key_name = key_name.split('.')
+            key_name = key_name.split(".")
+
             for i in key_name:
                 data = data.get(i, {})
+
                 if not data:
                     return 0
+
             return data
 
         def compute_interval(data, total_name):
             current_total = get_dotted_value(data, total_name)
-            total_key = '.'.join(base_prefix + [total_name])
+            total_key = ".".join(base_prefix + [total_name])
             last_total = self.__totals.get(total_key, current_total)
             interval = current_total - last_total
             self.__totals[total_key] = current_total
+
             return interval
 
         def publish_percent(value_name, total_name, data):
             value = float(get_dotted_value(data, value_name) * 100)
             interval = compute_interval(data, total_name)
-            key = '.'.join(base_prefix + ['percent', value_name])
-            self.publish_counter(key, value, time_delta=bool(interval),
-                                 interval=interval)
+            key = ".".join(base_prefix + ["percent", value_name])
+            self.publish_counter(
+                key, value, time_delta=bool(interval), interval=interval
+            )
 
-        publish_percent('globalLock.lockTime', 'globalLock.totalTime', data)
+        publish_percent("globalLock.lockTime", "globalLock.totalTime", data)
+
+        locks = data.get("locks")
 
-        locks = data.get('locks')
         if locks:
-            if '.' in locks:
-                locks['_global_'] = locks['.']
-                del (locks['.'])
-            key_prefix = '.'.join(base_prefix + ['percent'])
-            db_name_filter = re.compile(self.config['databases'])
-            interval = compute_interval(data, 'uptimeMillis')
+            if "." in locks:
+                locks["_global_"] = locks["."]
+                del locks["."]
+
+            key_prefix = ".".join(base_prefix + ["percent"])
+            db_name_filter = re.compile(self.config["databases"])
+            interval = compute_interval(data, "uptimeMillis")
+
             for db_name in locks:
                 if not db_name_filter.search(db_name):
                     continue
-                r = get_dotted_value(
-                    locks,
-                    '%s.timeLockedMicros.r' % db_name)
-                R = get_dotted_value(
-                    locks,
-                    '.%s.timeLockedMicros.R' % db_name)
+
+                r = get_dotted_value(locks, "%s.timeLockedMicros.r" % db_name)
+                R = get_dotted_value(locks, ".%s.timeLockedMicros.R" % db_name)
                 value = float(r + R) / 10
+
                 if value:
                     self.publish_counter(
-                        key_prefix + '.locks.%s.read' % db_name,
-                        value, time_delta=bool(interval),
-                        interval=interval)
-                w = get_dotted_value(
-                    locks,
-                    '%s.timeLockedMicros.w' % db_name)
-                W = get_dotted_value(
-                    locks,
-                    '%s.timeLockedMicros.W' % db_name)
+                        key_prefix + ".locks.%s.read" % db_name,
+                        value,
+                        time_delta=bool(interval),
+                        interval=interval,
+                    )
+
+                w = get_dotted_value(locks, "%s.timeLockedMicros.w" % db_name)
+                W = get_dotted_value(locks, "%s.timeLockedMicros.W" % db_name)
                 value = float(w + W) / 10
+
                 if value:
                     self.publish_counter(
-                        key_prefix + '.locks.%s.write' % db_name,
-                        value, time_delta=bool(interval), interval=interval)
+                        key_prefix + ".locks.%s.write" % db_name,
+                        value,
+                        time_delta=bool(interval),
+                        interval=interval,
+                    )
 
     def _publish_dict_with_prefix(self, dict, prefix, publishfn=None):
         for key in dict:
             self._publish_metrics(prefix, key, dict, publishfn)
 
     def _publish_metrics(self, prev_keys, key, data, publishfn=None):
         """Recursively publish keys"""
         if key not in data:
             return
+
         value = data[key]
         keys = prev_keys + [key]
+
         if not publishfn:
             publishfn = self.publish
+
         if isinstance(value, dict):
             for new_key in value:
                 self._publish_metrics(keys, new_key, value)
         elif isinstance(value, int) or isinstance(value, float):
-            publishfn('.'.join(keys), value)
-        elif isinstance(value, long):
-            publishfn('.'.join(keys), float(value))
+            publishfn(".".join(keys), value)
 
     def _extract_simple_data(self, data):
         return {
-            'connections': data.get('connections'),
-            'globalLock': data.get('globalLock'),
-            'indexCounters': data.get('indexCounters')
+            "connections": data.get("connections"),
+            "globalLock": data.get("globalLock"),
+            "indexCounters": data.get("indexCounters"),
         }
```

### Comparing `diamond-next-4.0.515/src/collectors/aurora/aurora.py` & `diamond-next-5.0.0/src/collectors/aurora/aurora.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,51 +1,50 @@
-import diamond.collector
+import urllib.request
 
-import urllib2
+import diamond.collector
 
 
 class AuroraCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(AuroraCollector,
-                            self).get_default_config_help()
+        config_help = super(AuroraCollector, self).get_default_config_help()
 
-        config_help.update({
-            'host': 'Scheduler Hostname',
-            'port': 'Scheduler HTTP Metrics Port',
-            'path': 'Collector path. Defaults to "aurora"',
-            'scheme': 'http'
-        })
+        config_help.update(
+            {
+                "host": "Scheduler Hostname",
+                "port": "Scheduler HTTP Metrics Port",
+                "path": 'Collector path. Defaults to "aurora"',
+                "scheme": "http",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         config = super(AuroraCollector, self).get_default_config()
-        config.update({
-            'path': 'aurora',
-            'host': 'localhost',
-            'port': 8081,
-            'scheme': 'http'
-        })
+        config.update(
+            {"path": "aurora", "host": "localhost", "port": 8081, "scheme": "http"}
+        )
         return config
 
     def collect(self):
-        url = "%s://%s:%s/vars" % (self.config['scheme'],
-                                   self.config['host'],
-                                   self.config['port'])
+        url = "%s://%s:%s/vars" % (
+            self.config["scheme"],
+            self.config["host"],
+            self.config["port"],
+        )
 
-        response = urllib2.urlopen(url)
+        response = urllib.request.urlopen(url)
 
         for line in response.readlines():
             properties = line.split()
 
             # Not all lines returned will have a numeric metric.
             # To account for this, we attempt to cast the 'value'
             # portion as a float. If that's not possible, NBD, we
             # just move on.
             try:
                 if len(properties) > 1:
-                    subpath = properties[0].replace('/', '.').replace('_', '.')
+                    subpath = properties[0].replace("/", ".").replace("_", ".")
                     value = float(properties[1])
 
                     self.publish(subpath, value)
             except ValueError:
                 continue
```

### Comparing `diamond-next-4.0.515/src/collectors/diskspace/diskspace.py` & `diamond-next-5.0.0/src/collectors/diskspace/diskspace.py`

 * *Files 9% similar despite different names*

```diff
@@ -16,203 +16,213 @@
     exclude_filters = ^/boot, ^/mnt
 
     # exclude everything that includes the letter 'm'
     exclude_filters = m,
 
 """
 
-import diamond.collector
-import diamond.convertor
 import os
 import re
 
+import diamond.collector
+import diamond.convertor
+
 try:
     import psutil
 except ImportError:
     psutil = None
 
 
 class DiskSpaceCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(DiskSpaceCollector, self).get_default_config_help()
-        config_help.update({
-            'filesystems': "filesystems to examine",
-            'exclude_filters':
-                "A list of regex patterns. Any filesystem" +
-                " matching any of these patterns will be excluded from disk" +
-                " space metrics collection",
-        })
+        config_help.update(
+            {
+                "filesystems": "filesystems to examine",
+                "exclude_filters": "A list of regex patterns. Any filesystem"
+                + " matching any of these patterns will be excluded from disk"
+                + " space metrics collection",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(DiskSpaceCollector, self).get_default_config()
-        config.update({
-            'path': 'diskspace',
-            # filesystems to examine
-            'filesystems': 'ext2, ext3, ext4, xfs, glusterfs, nfs, nfs4, ' +
-                           ' ntfs, hfs, fat32, fat16, btrfs',
-
-            # exclude_filters
-            #   A list of regex patterns
-            #   A filesystem matching any of these patterns will be excluded
-            #   from disk space metrics collection.
-            #
-            # Examples:
-            #       exclude_filters =,
-            # no exclude filters at all
-            #       exclude_filters = ^/boot, ^/mnt
-            # exclude everything that begins /boot or /mnt
-            #       exclude_filters = m,
-            # exclude everything that includes the letter "m"
-            'exclude_filters': ['^/export/home'],
-
-            # Default numeric output
-            'byte_unit': ['byte']
-        })
+        config.update(
+            {
+                "path": "diskspace",
+                # filesystems to examine
+                "filesystems": "ext2, ext3, ext4, xfs, glusterfs, nfs, nfs4, ntfs, hfs, fat32, fat16, btrfs",
+                # exclude_filters
+                #   A list of regex patterns
+                #   A filesystem matching any of these patterns will be excluded
+                #   from disk space metrics collection.
+                #
+                # Examples:
+                #       exclude_filters =,
+                # no exclude filters at all
+                #       exclude_filters = ^/boot, ^/mnt
+                # exclude everything that begins /boot or /mnt
+                #       exclude_filters = m,
+                # exclude everything that includes the letter "m"
+                "exclude_filters": ["^/export/home"],
+                # Default numeric output
+                "byte_unit": ["byte"],
+            }
+        )
+
         return config
 
     def process_config(self):
         super(DiskSpaceCollector, self).process_config()
+
         # Precompile things
-        self.exclude_filters = self.config['exclude_filters']
-        if isinstance(self.exclude_filters, basestring):
+        self.exclude_filters = self.config["exclude_filters"]
+
+        if isinstance(self.exclude_filters, str):
             self.exclude_filters = [self.exclude_filters]
 
         if not self.exclude_filters:
-            self.exclude_reg = re.compile('!.*')
+            self.exclude_reg = re.compile("!.*")
         else:
-            self.exclude_reg = re.compile('|'.join(self.exclude_filters))
+            self.exclude_reg = re.compile("|".join(self.exclude_filters))
 
         self.filesystems = []
-        if isinstance(self.config['filesystems'], basestring):
-            for filesystem in self.config['filesystems'].split(','):
+
+        if isinstance(self.config["filesystems"], str):
+            for filesystem in self.config["filesystems"].split(","):
                 self.filesystems.append(filesystem.strip())
-        elif isinstance(self.config['filesystems'], list):
-            self.filesystems = self.config['filesystems']
+        elif isinstance(self.config["filesystems"], list):
+            self.filesystems = self.config["filesystems"]
 
     def get_disk_labels(self):
         """
         Creates a mapping of device nodes to filesystem labels
         """
-        path = '/dev/disk/by-label/'
+        path = "/dev/disk/by-label/"
         labels = {}
+
         if not os.path.isdir(path):
             return labels
 
         for label in os.listdir(path):
-            label = label.replace('\\x2f', '/')
-            device = os.path.realpath(path + '/' + label)
+            label = label.replace("\\x2f", "/")
+            device = os.path.realpath(path + "/" + label)
             labels[device] = label
 
         return labels
 
     def get_file_systems(self):
         """
         Creates a map of mounted filesystems on the machine.
 
         iostat(1): Each sector has size of 512 bytes.
 
         Returns:
-          (major, minor) -> FileSystem(device, mount_point)
+          st_dev -> FileSystem(device, mount_point)
         """
         result = {}
-        if os.access('/proc/mounts', os.R_OK):
-            file = open('/proc/mounts')
+
+        if os.access("/proc/mounts", os.R_OK):
+            file = open("/proc/mounts")
+
             for line in file:
                 try:
                     mount = line.split()
                     device = mount[0]
                     mount_point = mount[1]
                     fs_type = mount[2]
                 except (IndexError, ValueError):
                     continue
 
-                # Skip the filesystem if it is not in the list of valid
-                # filesystems
+                # Skip the filesystem if it is not in the list of valid filesystems
                 if fs_type not in self.filesystems:
-                    self.log.debug("Ignoring %s since it is of type %s " +
-                                   " which is not in the list of filesystems.",
-                                   mount_point, fs_type)
+                    self.log.debug(
+                        "Ignoring %s since it is of type %s which is not in the list of filesystems.",
+                        mount_point,
+                        fs_type,
+                    )
                     continue
 
                 # Process the filters
                 if self.exclude_reg.search(mount_point):
-                    self.log.debug("Ignoring %s since it is in the " +
-                                   "exclude_filter list.", mount_point)
+                    self.log.debug(
+                        "Ignoring %s since it is in the exclude_filter list.",
+                        mount_point,
+                    )
                     continue
 
-                if ((mount_point.startswith('/dev') or
-                     mount_point.startswith('/proc') or
-                     mount_point.startswith('/sys'))):
-                    continue
-
-                if ((('/' in device or device == 'tmpfs') and
-                     mount_point.startswith('/'))):
+                if ("/" in device or device == "tmpfs") and mount_point.startswith("/"):
                     try:
                         stat = os.stat(mount_point)
-                        major = os.major(stat.st_dev)
-                        minor = os.minor(stat.st_dev)
                     except OSError:
-                        self.log.debug("Path %s is not mounted - skipping.",
-                                       mount_point)
+                        self.log.debug(
+                            "Path %s is not mounted - skipping.", mount_point
+                        )
                         continue
 
-                    if (major, minor) in result:
+                    if stat.st_dev in result:
                         continue
 
-                    result[(major, minor)] = {
-                        'device': os.path.realpath(device),
-                        'mount_point': mount_point,
-                        'fs_type': fs_type
+                    result[stat.st_dev] = {
+                        "device": os.path.realpath(device),
+                        "mount_point": mount_point,
+                        "fs_type": fs_type,
                     }
 
             file.close()
 
         else:
             if not psutil:
-                self.log.error('Unable to import psutil')
+                self.log.error("Unable to import psutil")
                 return None
 
             partitions = psutil.disk_partitions(False)
+
             for partition in partitions:
-                result[(0, len(result))] = {
-                    'device': os.path.realpath(partition.device),
-                    'mount_point': partition.mountpoint,
-                    'fs_type': partition.fstype
+                result[len(result)] = {
+                    "device": os.path.realpath(partition.device),
+                    "mount_point": partition.mountpoint,
+                    "fs_type": partition.fstype,
                 }
+
             pass
 
         return result
 
     def collect(self):
         labels = self.get_disk_labels()
         results = self.get_file_systems()
+
         if not results:
-            self.log.error('No diskspace metrics retrieved')
+            self.log.error("No diskspace metrics retrieved")
+
             return None
 
-        for key, info in results.iteritems():
-            if info['device'] in labels:
-                name = labels[info['device']]
+        for info in iter(results.values()):
+            if info["device"] in labels:
+                name = labels[info["device"]]
             else:
-                name = info['mount_point'].replace('/', '_')
-                name = name.replace('.', '_').replace('\\', '')
-                if name == '_':
-                    name = 'root'
-                if name == '_tmp':
-                    name = 'tmp'
+                name = info["mount_point"].replace("/", "_")
+                name = name.replace(".", "_").replace("\\", "")
+
+                if name == "_":
+                    name = "root"
+
+                if name == "_tmp":
+                    name = "tmp"
 
-            if hasattr(os, 'statvfs'):  # POSIX
+            if hasattr(os, "statvfs"):  # POSIX
                 try:
-                    data = os.statvfs(info['mount_point'])
-                except OSError, e:
+                    data = os.statvfs(info["mount_point"])
+                except OSError as e:
                     self.log.exception(e)
                     continue
 
                 # Changed from data.f_bsize as f_frsize seems to be a more
                 # accurate representation of block size on multiple POSIX
                 # operating systems.
                 block_size = data.f_frsize
@@ -220,55 +230,61 @@
                 blocks_total = data.f_blocks
                 blocks_free = data.f_bfree
                 blocks_avail = data.f_bavail
                 inodes_total = data.f_files
                 inodes_free = data.f_ffree
                 inodes_avail = data.f_favail
 
-            elif os.name == 'nt':       # Windows
+            elif os.name == "nt":  # Windows
                 # fixme: used still not exact compared to disk_usage.py
                 # from psutil
-                raw_data = psutil.disk_usage(info['mount_point'])
+                raw_data = psutil.disk_usage(info["mount_point"])
 
                 block_size = 1  # fixme: ?
 
                 blocks_total = raw_data.total
                 blocks_free = raw_data.free
 
             else:
                 raise NotImplementedError("platform not supported")
 
-            for unit in self.config['byte_unit']:
-                metric_name = '%s.%s_percentfree' % (name, unit)
-                metric_value = float(blocks_free) / float(
-                    blocks_free + (blocks_total - blocks_free)) * 100
+            for unit in self.config["byte_unit"]:
+                metric_name = "%s.%s_percentfree" % (name, unit)
+                metric_value = (
+                    float(blocks_free)
+                    / float(blocks_free + (blocks_total - blocks_free))
+                    * 100
+                )
                 self.publish_gauge(metric_name, metric_value, 2)
 
-                metric_name = '%s.%s_used' % (name, unit)
-                metric_value = float(block_size) * float(
-                    blocks_total - blocks_free)
+                metric_name = "%s.%s_used" % (name, unit)
+                metric_value = float(block_size) * float(blocks_total - blocks_free)
                 metric_value = diamond.convertor.binary.convert(
-                    value=metric_value, oldUnit='byte', newUnit=unit)
+                    value=metric_value, old_unit="byte", new_unit=unit
+                )
                 self.publish_gauge(metric_name, metric_value, 2)
 
-                metric_name = '%s.%s_free' % (name, unit)
+                metric_name = "%s.%s_free" % (name, unit)
                 metric_value = float(block_size) * float(blocks_free)
                 metric_value = diamond.convertor.binary.convert(
-                    value=metric_value, oldUnit='byte', newUnit=unit)
+                    value=metric_value, old_unit="byte", new_unit=unit
+                )
                 self.publish_gauge(metric_name, metric_value, 2)
 
-                if os.name != 'nt':
-                    metric_name = '%s.%s_avail' % (name, unit)
+                if os.name != "nt":
+                    metric_name = "%s.%s_avail" % (name, unit)
                     metric_value = float(block_size) * float(blocks_avail)
                     metric_value = diamond.convertor.binary.convert(
-                        value=metric_value, oldUnit='byte', newUnit=unit)
+                        value=metric_value, old_unit="byte", new_unit=unit
+                    )
                     self.publish_gauge(metric_name, metric_value, 2)
 
-            if os.name != 'nt':
+            if os.name != "nt":
                 if float(inodes_total) > 0:
                     self.publish_gauge(
-                        '%s.inodes_percentfree' % name,
-                        float(inodes_free) / float(inodes_total) * 100)
-                self.publish_gauge('%s.inodes_used' % name,
-                                   inodes_total - inodes_free)
-                self.publish_gauge('%s.inodes_free' % name, inodes_free)
-                self.publish_gauge('%s.inodes_avail' % name, inodes_avail)
+                        "%s.inodes_percentfree" % name,
+                        float(inodes_free) / float(inodes_total) * 100,
+                    )
+
+                self.publish_gauge("%s.inodes_used" % name, inodes_total - inodes_free)
+                self.publish_gauge("%s.inodes_free" % name, inodes_free)
+                self.publish_gauge("%s.inodes_avail" % name, inodes_avail)
```

### Comparing `diamond-next-4.0.515/src/collectors/apcupsd/apcupsd.py` & `diamond-next-5.0.0/src/collectors/apcupsd/apcupsd.py`

 * *Files 13% similar despite different names*

```diff
@@ -9,95 +9,115 @@
 
 #### Dependencies
 
  * apcuspd in NIS mode
 
 """
 
-import diamond.collector
-import socket
-from struct import pack
 import re
+import socket
+import struct
+
 import time
 
+import diamond.collector
 
-class ApcupsdCollector(diamond.collector.Collector):
 
+class ApcupsdCollector(diamond.collector.Collector):
     def get_default_config_help(self):
         config_help = super(ApcupsdCollector, self).get_default_config_help()
-        config_help.update({
-            'hostname': 'Hostname to collect from',
-            'port': 'port to collect from. defaults to 3551',
-            'metrics':
-                'List of metrics. Valid metric keys can be found [here]' +
-                '(http://www.apcupsd.com/manual/' +
-                'manual.html#status-report-fields)'
-        })
+        config_help.update(
+            {
+                "hostname": "Hostname to collect from",
+                "port": "port to collect from. defaults to 3551",
+                "metrics": "List of metrics. Valid metric keys can be found [here]"
+                + "(http://www.apcupsd.com/manual/"
+                + "manual.html#status-report-fields)",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ApcupsdCollector, self).get_default_config()
-        config.update({
-            'path':     'apcupsd',
-            'hostname': 'localhost',
-            'port': 3551,
-            'metrics': ['LINEV', 'LOADPCT', 'BCHARGE', 'TIMELEFT', 'BATTV',
-                        'NUMXFERS', 'TONBATT', 'MAXLINEV', 'MINLINEV',
-                        'OUTPUTV', 'ITEMP', 'LINEFREQ', 'CUMONBATT', ],
-        })
+        config.update(
+            {
+                "path": "apcupsd",
+                "hostname": "localhost",
+                "port": 3551,
+                "metrics": [
+                    "LINEV",
+                    "LOADPCT",
+                    "BCHARGE",
+                    "TIMELEFT",
+                    "BATTV",
+                    "NUMXFERS",
+                    "TONBATT",
+                    "MAXLINEV",
+                    "MINLINEV",
+                    "OUTPUTV",
+                    "ITEMP",
+                    "LINEFREQ",
+                    "CUMONBATT",
+                ],
+            }
+        )
         return config
 
-    def getData(self):
+    def get_data(self):
         # Get the data via TCP stream
         s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-        s.connect((self.config['hostname'], int(self.config['port'])))
+        s.connect((self.config["hostname"], int(self.config["port"])))
 
         # Packet is pad byte, size byte, and command
-        s.send(pack('xb6s', 6, 'status'))
+        s.send(struct.pack("xb6s", 6, "status"))
 
         # Ditch the header
         s.recv(1024)
-        time.sleep(.25)
+        time.sleep(0.25)
         data = s.recv(4096)
 
         # We're done. Close the socket
         s.close()
+
         return data
 
     def collect(self):
         metrics = {}
         raw = {}
 
-        data = self.getData()
+        data = self.get_data()
+        data = data.split("\n\x00")
 
-        data = data.split('\n\x00')
         for d in data:
             matches = re.search("([A-Z]+)\s+:\s+(.*)$", d)
+
             if matches:
                 value = matches.group(2).strip()
                 raw[matches.group(1)] = matches.group(2).strip()
                 vmatch = re.search("([0-9.]+)", value)
+
                 if not vmatch:
                     continue
+
                 try:
                     value = float(vmatch.group(1))
                 except ValueError:
                     continue
+
                 metrics[matches.group(1)] = value
 
-        for metric in self.config['metrics']:
+        for metric in self.config["metrics"]:
             if metric not in metrics:
                 continue
 
-            metric_name = "%s.%s" % (raw['UPSNAME'], metric)
-
+            metric_name = "%s.%s" % (raw["UPSNAME"], metric)
             value = metrics[metric]
 
-            if metric in ['TONBATT', 'CUMONBATT', 'NUMXFERS']:
+            if metric in ["TONBATT", "CUMONBATT", "NUMXFERS"]:
                 value = self.derivative(metric_name, metrics[metric])
 
             self.publish(metric_name, value)
 
         return True
```

### Comparing `diamond-next-4.0.515/src/collectors/ip/ip.py` & `diamond-next-5.0.0/src/collectors/ip/ip.py`

 * *Files 20% similar despite different names*

```diff
@@ -19,102 +19,106 @@
 <tr><th>OutDiscards</th></tr>
 <tr><th>OutNoRoutes</th></tr>
 <tr><th>OutRequests</th></tr>
 </table>
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
 
-class IPCollector(diamond.collector.Collector):
 
+class IPCollector(diamond.collector.Collector):
     PROC = [
-        '/proc/net/snmp',
+        "/proc/net/snmp",
     ]
 
     GAUGES = [
-        'Forwarding',
-        'DefaultTTL',
+        "Forwarding",
+        "DefaultTTL",
     ]
 
     def process_config(self):
         super(IPCollector, self).process_config()
-        if self.config['allowed_names'] is None:
-            self.config['allowed_names'] = []
+
+        if self.config["allowed_names"] is None:
+            self.config["allowed_names"] = []
 
     def get_default_config_help(self):
         config_help = super(IPCollector, self).get_default_config_help()
-        config_help.update({
-            'allowed_names': 'list of entries to collect, empty to collect all'
-        })
+        config_help.update(
+            {"allowed_names": "list of entries to collect, empty to collect all"}
+        )
         return config_help
 
     def get_default_config(self):
-        ''' Returns the default collector settings
-        '''
+        """Returns the default collector settings"""
         config = super(IPCollector, self).get_default_config()
-        config.update({
-            'path': 'ip',
-            'allowed_names': 'InAddrErrors, InDelivers, InDiscards, ' +
-            'InHdrErrors, InReceives, InUnknownProtos, OutDiscards, ' +
-            'OutNoRoutes, OutRequests'
-        })
+        config.update(
+            {
+                "path": "ip",
+                "allowed_names": "InAddrErrors, InDelivers, InDiscards, "
+                + "InHdrErrors, InReceives, InUnknownProtos, OutDiscards, "
+                + "OutNoRoutes, OutRequests",
+            }
+        )
         return config
 
     def collect(self):
         metrics = {}
 
         for filepath in self.PROC:
             if not os.access(filepath, os.R_OK):
-                self.log.error('Permission to access %s denied', filepath)
+                self.log.error("Permission to access %s denied", filepath)
                 continue
 
-            header = ''
-            data = ''
+            header = ""
+            data = ""
 
             # Seek the file for the lines which start with Ip
             file = open(filepath)
 
             if not file:
-                self.log.error('Failed to open %s', filepath)
+                self.log.error("Failed to open %s", filepath)
                 continue
 
             while True:
                 line = file.readline()
 
                 # Reached EOF?
                 if len(line) == 0:
                     break
 
                 # Line has metrics?
-                if line.startswith('Ip'):
+                if line.startswith("Ip"):
                     header = line
                     data = file.readline()
                     break
             file.close()
 
             # No data from the file?
-            if header == '' or data == '':
-                self.log.error('%s has no lines starting with Ip' % filepath)
+            if header == "" or data == "":
+                self.log.error("%s has no lines starting with Ip" % filepath)
                 continue
 
             header = header.split()
             data = data.split()
 
             # Zip up the keys and values
-            for i in xrange(1, len(header)):
+            for i in range(1, len(header)):
                 metrics[header[i]] = data[i]
 
         for metric_name in metrics.keys():
-            if ((len(self.config['allowed_names']) > 0 and
-                 metric_name not in self.config['allowed_names'])):
+            if (
+                len(self.config["allowed_names"]) > 0
+                and metric_name not in self.config["allowed_names"]
+            ):
                 continue
 
-            value = long(metrics[metric_name])
+            value = int(metrics[metric_name])
 
             # Publish the metric
             if metric_name in self.GAUGES:
                 self.publish_gauge(metric_name, value, 0)
             else:
                 self.publish_counter(metric_name, value, 0)
```

### Comparing `diamond-next-4.0.515/src/collectors/uptime/uptime.py` & `diamond-next-5.0.0/src/collectors/uptime/uptime.py`

 * *Files 18% similar despite different names*

```diff
@@ -5,44 +5,42 @@
 
 #### Dependencies
 
     * /proc/uptime
 
 """
 
-from diamond.collector import Collector
-from diamond import convertor
-
 import os
 
+import diamond.collector
+import diamond.convertor
+
 
-class UptimeCollector(Collector):
-    PROC = '/proc/uptime'
+class UptimeCollector(diamond.collector.Collector):
+    PROC = "/proc/uptime"
 
     def get_default_config(self):
         config = super(UptimeCollector, self).get_default_config()
-        config.update({
-            'path': 'uptime',
-            'metric_name': 'minutes'
-        })
+        config.update({"path": "uptime", "metric_name": "minutes"})
         return config
 
     def collect(self):
         if not os.path.exists(self.PROC):
-            self.log.error('Input path %s does not exist' % self.PROC)
+            self.log.error("Input path %s does not exist" % self.PROC)
             return {}
 
         v = self.read()
+
         if v is not None:
-            self.publish(self.config['metric_name'], v)
+            self.publish(self.config["metric_name"], v)
 
     def read(self):
         try:
             fd = open(self.PROC)
             uptime = fd.readline()
             fd.close()
             v = float(uptime.split()[0].strip())
-            return convertor.time.convert(v, 's', self.config['metric_name'])
-        except Exception, e:
-            self.log.error('Unable to read uptime from %s: %s' % (self.PROC,
-                                                                  e))
+
+            return diamond.convertor.time.convert(v, "s", self.config["metric_name"])
+        except Exception as e:
+            self.log.error("Unable to read uptime from %s: %s" % (self.PROC, e))
             return None
```

### Comparing `diamond-next-4.0.515/src/collectors/mysqlstat/mysqlstat.py` & `diamond-next-5.0.0/src/collectors/mysqlstat/mysqlstat.py`

 * *Files 18% similar despite different names*

```diff
@@ -20,402 +20,424 @@
 ```
 GRANT PROCESS ON *.* TO 'user'@'hostname' IDENTIFIED BY
 'password';
 ```
 
 #### Dependencies
 
- * MySQLdb
+ * mysqlclient
 
 """
 
-import diamond.collector
-from diamond.collector import str_to_bool
 import re
+
 import time
 
+import diamond.collector
+
 try:
-    import MySQLdb
-    from MySQLdb import MySQLError
+    import mysqlclient
+    from mysqlclient import MySQLError
 except ImportError:
-    MySQLdb = None
+    mysqlclient = None
     MySQLError = ValueError
 
 
 class MySQLCollector(diamond.collector.Collector):
-
     _GAUGE_KEYS = [
-        'Innodb_buffer_pool_pages_data', 'Innodb_buffer_pool_pages_dirty',
-        'Innodb_buffer_pool_pages_free',
-        'Innodb_buffer_pool_pages_misc', 'Innodb_buffer_pool_pages_total',
-        'Innodb_data_pending_fsyncs', 'Innodb_data_pending_reads',
-        'Innodb_data_pending_writes',
-        'Innodb_os_log_pending_fsyncs', 'Innodb_os_log_pending_writes',
-        'Innodb_page_size',
-        'Innodb_row_lock_current_waits', 'Innodb_row_lock_time',
-        'Innodb_row_lock_time_avg',
-        'Innodb_row_lock_time_max',
-        'Key_blocks_unused', 'Last_query_cost', 'Max_used_connections',
-        'Open_files', 'Open_streams', 'Open_table_definitions', 'Open_tables',
-        'Qcache_free_blocks', 'Qcache_free_memory',
-        'Qcache_queries_in_cache', 'Qcache_total_blocks',
-        'Seconds_Behind_Master',
-        'Slave_open_temp_tables',
-        'Threads_cached', 'Threads_connected', 'Threads_created',
-        'Threads_running',
+        "Innodb_buffer_pool_pages_data",
+        "Innodb_buffer_pool_pages_dirty",
+        "Innodb_buffer_pool_pages_free",
+        "Innodb_buffer_pool_pages_misc",
+        "Innodb_buffer_pool_pages_total",
+        "Innodb_data_pending_fsyncs",
+        "Innodb_data_pending_reads",
+        "Innodb_data_pending_writes",
+        "Innodb_os_log_pending_fsyncs",
+        "Innodb_os_log_pending_writes",
+        "Innodb_page_size",
+        "Innodb_row_lock_current_waits",
+        "Innodb_row_lock_time",
+        "Innodb_row_lock_time_avg",
+        "Innodb_row_lock_time_max",
+        "Key_blocks_unused",
+        "Last_query_cost",
+        "Max_used_connections",
+        "Open_files",
+        "Open_streams",
+        "Open_table_definitions",
+        "Open_tables",
+        "Qcache_free_blocks",
+        "Qcache_free_memory",
+        "Qcache_queries_in_cache",
+        "Qcache_total_blocks",
+        "Seconds_Behind_Master",
+        "Slave_open_temp_tables",
+        "Threads_cached",
+        "Threads_connected",
+        "Threads_created",
+        "Threads_running",
         # innodb status non counter keys
-        'Innodb_bp_created_per_sec',
-        'Innodb_bp_pages_evicted_no_access_per_sec',
-        'Innodb_bp_pages_not_young_per_sec',
-        'Innodb_bp_pages_read_ahead_per_sec', 'Innodb_bp_pages_young_per_sec',
-        'Innodb_bp_reads_per_sec', 'Innodb_bp_written_per_sec',
-        'Innodb_bp_add_alloc', 'Innodb_bp_db_pages',
-        'Innodb_bp_dictionary_alloc', 'Innodb_bp_free_buffers',
-        'Innodb_bp_hit_rate', 'Innodb_bp_io_cur_pages',
-        'Innodb_bp_io_sum_pages', 'Innodb_bp_io_unzip_cur_pages',
-        'Innodb_bp_io_unzip_sum_pages', 'Innodb_bp_lru_len',
-        'Innodb_bp_modified_pages', 'Innodb_bp_not_young_hit_rate',
-        'Innodb_bp_old_db_pages', 'Innodb_bp_pending_pages',
-        'Innodb_bp_pending_writes_flush_list', 'Innodb_bp_pending_writes_lru',
-        'Innodb_bp_pending_writes_single_page', 'Innodb_bp_size',
-        'Innodb_bp_total_alloc', 'Innodb_bp_unzip_lru_len',
-        'Innodb_bp_young_hit_rate',
-        'Innodb_hash_searches_per_sec',
-        'Innodb_io_syncs_per_sec',
-        'Innodb_log_io_per_sec',
-        'Innodb_non_hash_searches_per_sec',
-        'Innodb_per_sec_avg',
-        'Innodb_reads_per_sec',
-        'Innodb_rows_deleted_per_sec', 'Innodb_rows_inserted_per_sec',
-        'Innodb_rows_read_per_sec', 'Innodb_rows_updated_per_sec',
-        'Innodb_sem_spins_per_wait_mutex', 'Innodb_sem_spins_per_wait_rw_excl',
-        'Innodb_sem_spins_per_wait_rw_shared',
-        'Innodb_writes_per_sec',
-        'Innodb_bytes_per_read',
-        'Innodb_hash_node_heap', 'Innodb_hash_table_size',
-        'Innodb_hash_used_cells',
-        'Innodb_ibuf_free_list_len', 'Innodb_ibuf_seg_size', 'Innodb_ibuf_size',
-        'Innodb_io_ibuf_logs', 'Innodb_io_ibuf_reads', 'Innodb_io_ibuf_syncs',
-        'Innodb_io_pending_flush_bp', 'Innodb_io_pending_flush_log',
-        'Innodb_io_pending_reads', 'Innodb_io_pending_writes', '',
-        'Innodb_log_pending_checkpoint_writes', 'Innodb_log_pending_log_writes',
-        'Innodb_row_queries_inside', 'Innodb_row_queries_queue',
-        'Innodb_trx_history_list_length', 'Innodb_trx_total_lock_structs',
-        'Innodb_status_process_time', ]
+        "Innodb_bp_created_per_sec",
+        "Innodb_bp_pages_evicted_no_access_per_sec",
+        "Innodb_bp_pages_not_young_per_sec",
+        "Innodb_bp_pages_read_ahead_per_sec",
+        "Innodb_bp_pages_young_per_sec",
+        "Innodb_bp_reads_per_sec",
+        "Innodb_bp_written_per_sec",
+        "Innodb_bp_add_alloc",
+        "Innodb_bp_db_pages",
+        "Innodb_bp_dictionary_alloc",
+        "Innodb_bp_free_buffers",
+        "Innodb_bp_hit_rate",
+        "Innodb_bp_io_cur_pages",
+        "Innodb_bp_io_sum_pages",
+        "Innodb_bp_io_unzip_cur_pages",
+        "Innodb_bp_io_unzip_sum_pages",
+        "Innodb_bp_lru_len",
+        "Innodb_bp_modified_pages",
+        "Innodb_bp_not_young_hit_rate",
+        "Innodb_bp_old_db_pages",
+        "Innodb_bp_pending_pages",
+        "Innodb_bp_pending_writes_flush_list",
+        "Innodb_bp_pending_writes_lru",
+        "Innodb_bp_pending_writes_single_page",
+        "Innodb_bp_size",
+        "Innodb_bp_total_alloc",
+        "Innodb_bp_unzip_lru_len",
+        "Innodb_bp_young_hit_rate",
+        "Innodb_hash_searches_per_sec",
+        "Innodb_io_syncs_per_sec",
+        "Innodb_log_io_per_sec",
+        "Innodb_non_hash_searches_per_sec",
+        "Innodb_per_sec_avg",
+        "Innodb_reads_per_sec",
+        "Innodb_rows_deleted_per_sec",
+        "Innodb_rows_inserted_per_sec",
+        "Innodb_rows_read_per_sec",
+        "Innodb_rows_updated_per_sec",
+        "Innodb_sem_spins_per_wait_mutex",
+        "Innodb_sem_spins_per_wait_rw_excl",
+        "Innodb_sem_spins_per_wait_rw_shared",
+        "Innodb_writes_per_sec",
+        "Innodb_bytes_per_read",
+        "Innodb_hash_node_heap",
+        "Innodb_hash_table_size",
+        "Innodb_hash_used_cells",
+        "Innodb_ibuf_free_list_len",
+        "Innodb_ibuf_seg_size",
+        "Innodb_ibuf_size",
+        "Innodb_io_ibuf_logs",
+        "Innodb_io_ibuf_reads",
+        "Innodb_io_ibuf_syncs",
+        "Innodb_io_pending_flush_bp",
+        "Innodb_io_pending_flush_log",
+        "Innodb_io_pending_reads",
+        "Innodb_io_pending_writes",
+        "",
+        "Innodb_log_pending_checkpoint_writes",
+        "Innodb_log_pending_log_writes",
+        "Innodb_row_queries_inside",
+        "Innodb_row_queries_queue",
+        "Innodb_trx_history_list_length",
+        "Innodb_trx_total_lock_structs",
+        "Innodb_status_process_time",
+    ]
     _IGNORE_KEYS = [
-        'Master_Port', 'Master_Server_Id',
-        'Last_Errno', 'Last_IO_Errno', 'Last_SQL_Errno', ]
+        "Master_Port",
+        "Master_Server_Id",
+        "Last_Errno",
+        "Last_IO_Errno",
+        "Last_SQL_Errno",
+    ]
 
     innodb_status_keys = {
-        'Innodb_bp_total_alloc,' +
-        'Innodb_bp_add_alloc':
-            'Total memory allocated (\d+)\; in additional pool allocated (\d+)',
-        'Innodb_bp_reads_per_sec,' +
-        'Innodb_bp_created_per_sec,' +
-        'Innodb_bp_written_per_sec':
-            '(^\d+.\d+) reads/s, (\d+.\d+) creates/s, (\d+.\d+) writes/s',
-        'Innodb_io_ibuf_reads,Innodb_io_ibuf_logs,Innodb_io_ibuf_syncs':
-            ' ibuf aio reads: (\d+), log i/o\'s: (\d+), sync i/o\'s: (\d+)',
-        'Innodb_log_pending_log_writes,Innodb_log_pending_checkpoint_writes':
-            '(\d+) pending log writes, (\d+) pending chkp writes',
-        'Innodb_hash_searches_per_sec,Innodb_non_hash_searches_per_sec':
-            '(\d+.\d+) hash searches/s, (\d+.\d+) non-hash searches/s',
-        'Innodb_row_queries_inside,Innodb_row_queries_queue':
-            '(\d+) queries inside InnoDB, (\d+) queries in queue',
-        'Innodb_trx_total_lock_structs':
-            '(\d+) lock struct\(s\), ' +
-            'heap size (\d+), ' +
-            '(\d+) row lock\(s\), ' +
-            'undo log entries (\d+)',
-        'Innodb_log_io_total,Innodb_log_io_per_sec':
-            '(\d+) log i\/o\'s done, (\d+.\d+) log i\/o\'s\/second',
-        'Innodb_io_os_file_reads,Innodb_io_os_file_writes,' +
-        'Innodb_io_os_file_fsyncs':
-            '(\d+) OS file reads, (\d+) OS file writes, (\d+) OS fsyncs',
-        'Innodb_rows_inserted_per_sec,Innodb_rows_updated_per_sec,' +
-        'Innodb_rows_deleted_per_sec,Innodb_rows_read_per_sec':
-            '(\d+.\d+) inserts\/s, ' +
-            '(\d+.\d+) updates\/s, ' +
-            '(\d+.\d+) deletes\/s, ' +
-            '(\d+.\d+) reads\/s',
-        'Innodb_reads_per_sec,Innodb_bytes_per_read,Innodb_io_syncs_per_sec,' +
-        'Innodb_writes_per_sec':
-            '(\d+.\d+) reads\/s, (\d+) avg bytes\/read, (\d+.\d+) writes\/s, ' +
-            '(\d+.\d+) fsyncs\/s',
-        'Innodb_bp_pages_young_per_sec,Innodb_bp_pages_not_young_per_sec':
-            '(\d+.\d+) youngs\/s, (\d+.\d+) non-youngs\/s',
-        'Innodb_bp_hit_rate,Innodb_bp_young_hit_rate,' +
-        'Innodb_bp_not_young_hit_rate':
-            'Buffer pool hit rate (\d+) \/ \d+, ' +
-            'young-making rate (\d+) \/ \d+ not (\d+) \/ \d+',
-        'Innodb_bp_size':
-            'Buffer pool size   (\d+)',
-        'Innodb_bp_db_pages':
-            'Database pages     (\d+)',
-        'Innodb_bp_dictionary_alloc':
-            'Dictionary memory allocated (\d+)',
-        'Innodb_bp_free_buffers':
-            'Free buffers       (\d+)',
-        'Innodb_hash_table_size,Innodb_hash_node_heap':
-            'Hash table size (\d+), node heap has (\d+) buffer\(s\)',
-        'Innodb_trx_history_list_length':
-            'History list length (\d+)',
-        'Innodb_bp_io_sum_pages,Innodb_bp_io_cur_pages,' +
-        'Innodb_bp_io_unzip_sum_pages,Innodb_bp_io_unzip_cur_pages':
-            'I\/O sum\[(\d+)\]:cur\[(\d+)\], unzip sum\[(\d+)\]:cur\[(\d+)\]',
-        'Innodb_ibuf_size,Innodb_ibuf_free_list_len,Innodb_ibuf_seg_size,' +
-        'Innodb_ibuf_merges':
-            'Ibuf: size (\d+), free list len (\d+), seg size (\d+), (\d+) ' +
-            'merges',
-        'Innodb_bp_lru_len,Innodb_bp_unzip_lru_len':
-            'LRU len: (\d+), unzip_LRU len: (\d+)',
-        'Innodb_bp_modified_pages':
-            'Modified db pages  (\d+)',
-        'Innodb_sem_mutex_spin_waits,Innodb_sem_mutex_rounds,' +
-        'Innodb_sem_mutex_os_waits':
-            'Mutex spin waits (\d+), rounds (\d+), OS waits (\d+)',
-        'Innodb_rows_inserted,Innodb_rows_updated,Innodb_rows_deleted,' +
-        'Innodb_rows_read':
-            'Number of rows inserted (\d+), updated (\d+), deleted (\d+), ' +
-            'read (\d+)',
-        'Innodb_bp_old_db_pages':
-            'Old database pages (\d+)',
-        'Innodb_sem_os_reservation_count,' +
-        'Innodb_sem_os_signal_count':
-            'OS WAIT ARRAY INFO: reservation count (\d+), signal count (\d+)',
-        'Innodb_bp_pages_young,Innodb_bp_pages_not_young':
-            'Pages made young (\d+), not young (\d+)',
-        'Innodb_bp_pages_read,Innodb_bp_pages_created,Innodb_bp_pages_written':
-            'Pages read (\d+), created (\d+), written (\d+)',
-        'Innodb_bp_pages_read_ahead_per_sec,' +
-        'Innodb_bp_pages_evicted_no_access_per_sec,' +
-        'Innodb_status_bp_pages_random_read_ahead':
-            'Pages read ahead (\d+.\d+)/s, ' +
-            'evicted without access (\d+.\d+)\/s, ' +
-            'Random read ahead (\d+.\d+)/s',
-        'Innodb_io_pending_flush_log,Innodb_io_pending_flush_bp':
-            'Pending flushes \(fsync\) log: (\d+); buffer pool: (\d+)',
-        'Innodb_io_pending_reads,Innodb_io_pending_writes':
-            'Pending normal aio reads: (\d+) \[\d+, \d+, \d+, \d+\], aio ' +
-            'writes: (\d+) \[\d+, \d+, \d+, \d+\]',
-        'Innodb_bp_pending_writes_lru,Innodb_bp_pending_writes_flush_list,' +
-        'Innodb_bp_pending_writes_single_page':
-            'Pending writes: LRU (\d+), flush list (\d+), single page (\d+)',
-        'Innodb_per_sec_avg':
-            'Per second averages calculated from the last (\d+) seconds',
-        'Innodb_sem_rw_excl_spins,Innodb_sem_rw_excl_rounds,' +
-        'Innodb_sem_rw_excl_os_waits':
-            'RW-excl spins (\d+), rounds (\d+), OS waits (\d+)',
-        'Innodb_sem_shared_spins,Innodb_sem_shared_rounds,' +
-        'Innodb_sem_shared_os_waits':
-            'RW-shared spins (\d+), rounds (\d+), OS waits (\d+)',
-        'Innodb_sem_spins_per_wait_mutex,Innodb_sem_spins_per_wait_rw_shared,' +
-        'Innodb_sem_spins_per_wait_rw_excl':
-            'Spin rounds per wait: (\d+.\d+) mutex, (\d+.\d+) RW-shared, ' +
-            '(\d+.\d+) RW-excl',
-        'Innodb_main_thd_log_flush_writes':
-            'srv_master_thread log flush and writes: (\d+)',
-        'Innodb_main_thd_loops_one_sec,Innodb_main_thd_loops_sleeps,' +
-        'Innodb_main_thd_loops_ten_sec,Innodb_main_thd_loops_background,' +
-        'Innodb_main_thd_loops_flush':
-            'srv_master_thread loops: (\d+) 1_second, (\d+) sleeps, (\d+) ' +
-            '10_second, (\d+) background, (\d+) flush',
-        'Innodb_ibuf_inserts,Innodb_ibuf_merged_recs,Innodb_ibuf_merges':
-            '(\d+) inserts, (\d+) merged recs, (\d+) merges',
+        "Innodb_bp_total_alloc,"
+        + "Innodb_bp_add_alloc": "Total memory allocated (\d+)\; in additional pool allocated (\d+)",
+        "Innodb_bp_reads_per_sec,"
+        + "Innodb_bp_created_per_sec,"
+        + "Innodb_bp_written_per_sec": "(^\d+.\d+) reads/s, (\d+.\d+) creates/s, (\d+.\d+) writes/s",
+        "Innodb_io_ibuf_reads,Innodb_io_ibuf_logs,Innodb_io_ibuf_syncs": " ibuf aio reads: (\d+), log i/o's: (\d+), sync i/o's: (\d+)",
+        "Innodb_log_pending_log_writes,Innodb_log_pending_checkpoint_writes": "(\d+) pending log writes, (\d+) pending chkp writes",
+        "Innodb_hash_searches_per_sec,Innodb_non_hash_searches_per_sec": "(\d+.\d+) hash searches/s, (\d+.\d+) non-hash searches/s",
+        "Innodb_row_queries_inside,Innodb_row_queries_queue": "(\d+) queries inside InnoDB, (\d+) queries in queue",
+        "Innodb_trx_total_lock_structs": "(\d+) lock struct\(s\), "
+        + "heap size (\d+), "
+        + "(\d+) row lock\(s\), "
+        + "undo log entries (\d+)",
+        "Innodb_log_io_total,Innodb_log_io_per_sec": "(\d+) log i\/o's done, (\d+.\d+) log i\/o's\/second",
+        "Innodb_io_os_file_reads,Innodb_io_os_file_writes,"
+        + "Innodb_io_os_file_fsyncs": "(\d+) OS file reads, (\d+) OS file writes, (\d+) OS fsyncs",
+        "Innodb_rows_inserted_per_sec,Innodb_rows_updated_per_sec,"
+        + "Innodb_rows_deleted_per_sec,Innodb_rows_read_per_sec": "(\d+.\d+) inserts\/s, "
+        + "(\d+.\d+) updates\/s, "
+        + "(\d+.\d+) deletes\/s, "
+        + "(\d+.\d+) reads\/s",
+        "Innodb_reads_per_sec,Innodb_bytes_per_read,Innodb_io_syncs_per_sec,"
+        + "Innodb_writes_per_sec": "(\d+.\d+) reads\/s, (\d+) avg bytes\/read, (\d+.\d+) writes\/s, "
+        + "(\d+.\d+) fsyncs\/s",
+        "Innodb_bp_pages_young_per_sec,Innodb_bp_pages_not_young_per_sec": "(\d+.\d+) youngs\/s, (\d+.\d+) non-youngs\/s",
+        "Innodb_bp_hit_rate,Innodb_bp_young_hit_rate,"
+        + "Innodb_bp_not_young_hit_rate": "Buffer pool hit rate (\d+) \/ \d+, "
+        + "young-making rate (\d+) \/ \d+ not (\d+) \/ \d+",
+        "Innodb_bp_size": "Buffer pool size   (\d+)",
+        "Innodb_bp_db_pages": "Database pages     (\d+)",
+        "Innodb_bp_dictionary_alloc": "Dictionary memory allocated (\d+)",
+        "Innodb_bp_free_buffers": "Free buffers       (\d+)",
+        "Innodb_hash_table_size,Innodb_hash_node_heap": "Hash table size (\d+), node heap has (\d+) buffer\(s\)",
+        "Innodb_trx_history_list_length": "History list length (\d+)",
+        "Innodb_bp_io_sum_pages,Innodb_bp_io_cur_pages,"
+        + "Innodb_bp_io_unzip_sum_pages,Innodb_bp_io_unzip_cur_pages": "I\/O sum\[(\d+)\]:cur\[(\d+)\], unzip sum\[(\d+)\]:cur\[(\d+)\]",
+        "Innodb_ibuf_size,Innodb_ibuf_free_list_len,Innodb_ibuf_seg_size,"
+        + "Innodb_ibuf_merges": "Ibuf: size (\d+), free list len (\d+), seg size (\d+), (\d+) "
+        + "merges",
+        "Innodb_bp_lru_len,Innodb_bp_unzip_lru_len": "LRU len: (\d+), unzip_LRU len: (\d+)",
+        "Innodb_bp_modified_pages": "Modified db pages  (\d+)",
+        "Innodb_sem_mutex_spin_waits,Innodb_sem_mutex_rounds,"
+        + "Innodb_sem_mutex_os_waits": "Mutex spin waits (\d+), rounds (\d+), OS waits (\d+)",
+        "Innodb_rows_inserted,Innodb_rows_updated,Innodb_rows_deleted,"
+        + "Innodb_rows_read": "Number of rows inserted (\d+), updated (\d+), deleted (\d+), "
+        + "read (\d+)",
+        "Innodb_bp_old_db_pages": "Old database pages (\d+)",
+        "Innodb_sem_os_reservation_count,"
+        + "Innodb_sem_os_signal_count": "OS WAIT ARRAY INFO: reservation count (\d+), signal count (\d+)",
+        "Innodb_bp_pages_young,Innodb_bp_pages_not_young": "Pages made young (\d+), not young (\d+)",
+        "Innodb_bp_pages_read,Innodb_bp_pages_created,Innodb_bp_pages_written": "Pages read (\d+), created (\d+), written (\d+)",
+        "Innodb_bp_pages_read_ahead_per_sec,"
+        + "Innodb_bp_pages_evicted_no_access_per_sec,"
+        + "Innodb_status_bp_pages_random_read_ahead": "Pages read ahead (\d+.\d+)/s, "
+        + "evicted without access (\d+.\d+)\/s, "
+        + "Random read ahead (\d+.\d+)/s",
+        "Innodb_io_pending_flush_log,Innodb_io_pending_flush_bp": "Pending flushes \(fsync\) log: (\d+); buffer pool: (\d+)",
+        "Innodb_io_pending_reads,Innodb_io_pending_writes": "Pending normal aio reads: (\d+) \[\d+, \d+, \d+, \d+\], aio "
+        + "writes: (\d+) \[\d+, \d+, \d+, \d+\]",
+        "Innodb_bp_pending_writes_lru,Innodb_bp_pending_writes_flush_list,"
+        + "Innodb_bp_pending_writes_single_page": "Pending writes: LRU (\d+), flush list (\d+), single page (\d+)",
+        "Innodb_per_sec_avg": "Per second averages calculated from the last (\d+) seconds",
+        "Innodb_sem_rw_excl_spins,Innodb_sem_rw_excl_rounds,"
+        + "Innodb_sem_rw_excl_os_waits": "RW-excl spins (\d+), rounds (\d+), OS waits (\d+)",
+        "Innodb_sem_shared_spins,Innodb_sem_shared_rounds,"
+        + "Innodb_sem_shared_os_waits": "RW-shared spins (\d+), rounds (\d+), OS waits (\d+)",
+        "Innodb_sem_spins_per_wait_mutex,Innodb_sem_spins_per_wait_rw_shared,"
+        + "Innodb_sem_spins_per_wait_rw_excl": "Spin rounds per wait: (\d+.\d+) mutex, (\d+.\d+) RW-shared, "
+        + "(\d+.\d+) RW-excl",
+        "Innodb_main_thd_log_flush_writes": "srv_master_thread log flush and writes: (\d+)",
+        "Innodb_main_thd_loops_one_sec,Innodb_main_thd_loops_sleeps,"
+        + "Innodb_main_thd_loops_ten_sec,Innodb_main_thd_loops_background,"
+        + "Innodb_main_thd_loops_flush": "srv_master_thread loops: (\d+) 1_second, (\d+) sleeps, (\d+) "
+        + "10_second, (\d+) background, (\d+) flush",
+        "Innodb_ibuf_inserts,Innodb_ibuf_merged_recs,Innodb_ibuf_merges": "(\d+) inserts, (\d+) merged recs, (\d+) merges",
     }
     innodb_status_match = {}
 
     def __init__(self, *args, **kwargs):
         super(MySQLCollector, self).__init__(*args, **kwargs)
         for key in self.innodb_status_keys:
-            self.innodb_status_keys[key] = re.compile(
-                self.innodb_status_keys[key])
+            self.innodb_status_keys[key] = re.compile(self.innodb_status_keys[key])
 
     def process_config(self):
         super(MySQLCollector, self).process_config()
-        if self.config['hosts'].__class__.__name__ != 'list':
-            self.config['hosts'] = [self.config['hosts']]
+        if self.config["hosts"].__class__.__name__ != "list":
+            self.config["hosts"] = [self.config["hosts"]]
 
         # Move legacy config format to new format
-        if 'host' in self.config:
+        if "host" in self.config:
             hoststr = "%s:%s@%s:%s/%s" % (
-                self.config['user'],
-                self.config['passwd'],
-                self.config['host'],
-                self.config['port'],
-                self.config['db'],
+                self.config["user"],
+                self.config["passwd"],
+                self.config["host"],
+                self.config["port"],
+                self.config["db"],
             )
-            self.config['hosts'].append(hoststr)
+            self.config["hosts"].append(hoststr)
 
         # Normalize some config vars
-        self.config['master'] = str_to_bool(self.config['master'])
-        self.config['slave'] = str_to_bool(self.config['slave'])
-        self.config['innodb'] = str_to_bool(self.config['innodb'])
+        self.config["master"] = diamond.collector.str_to_bool(self.config["master"])
+        self.config["slave"] = diamond.collector.str_to_bool(self.config["slave"])
+        self.config["innodb"] = diamond.collector.str_to_bool(self.config["innodb"])
 
         self.db = None
 
     def get_default_config_help(self):
         config_help = super(MySQLCollector, self).get_default_config_help()
-        config_help.update({
-            'publish':
-                "Which rows of '[SHOW GLOBAL STATUS](http://dev.mysql." +
-                "com/doc/refman/5.1/en/show-status.html)' you would " +
-                "like to publish. Leave unset to publish all",
-            'slave': 'Collect SHOW SLAVE STATUS',
-            'master': 'Collect SHOW MASTER STATUS',
-            'innodb': 'Collect SHOW ENGINE INNODB STATUS',
-            'hosts': 'List of hosts to collect from. Format is ' +
-            'yourusername:yourpassword@host:port/db[/nickname]' +
-            'use db "None" to avoid connecting to a particular db'
-        })
+        config_help.update(
+            {
+                "publish": "Which rows of '[SHOW GLOBAL STATUS](http://dev.mysql."
+                + "com/doc/refman/5.1/en/show-status.html)' you would "
+                + "like to publish. Leave unset to publish all",
+                "slave": "Collect SHOW SLAVE STATUS",
+                "master": "Collect SHOW MASTER STATUS",
+                "innodb": "Collect SHOW ENGINE INNODB STATUS",
+                "hosts": "List of hosts to collect from. Format is "
+                + "yourusername:yourpassword@host:port/db[/nickname]"
+                + 'use db "None" to avoid connecting to a particular db',
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(MySQLCollector, self).get_default_config()
-        config.update({
-            'path':     'mysql',
-            # Connection settings
-            'hosts':    [],
-
-            # Which rows of 'SHOW GLOBAL STATUS' you would like to publish.
-            # http://dev.mysql.com/doc/refman/5.1/en/show-status.html
-            # Leave unset to publish all
-            # 'publish': '',
-
-            'slave':    False,
-            'master':   False,
-            'innodb':   False,
-        })
+        config.update(
+            {
+                "path": "mysql",
+                # Connection settings
+                "hosts": [],
+                # Which rows of 'SHOW GLOBAL STATUS' you would like to publish.
+                # http://dev.mysql.com/doc/refman/5.1/en/show-status.html
+                # Leave unset to publish all
+                # 'publish': '',
+                "slave": False,
+                "master": False,
+                "innodb": False,
+            }
+        )
+
         return config
 
     def get_db_stats(self, query):
-        cursor = self.db.cursor(cursorclass=MySQLdb.cursors.DictCursor)
+        cursor = self.db.cursor(cursorclass=mysqlclient.cursors.DictCursor)
 
         try:
             cursor.execute(query)
             return cursor.fetchall()
-        except MySQLError, e:
-            self.log.error('MySQLCollector could not get db stats', e)
+        except MySQLError as e:
+            self.log.error("MySQLCollector could not get db stats", e)
             return ()
 
     def connect(self, params):
         try:
-            self.db = MySQLdb.connect(**params)
-            self.log.debug('MySQLCollector: Connected to database.')
-        except MySQLError, e:
-            self.log.error('MySQLCollector couldnt connect to database %s', e)
+            self.db = mysqlclient.connect(**params)
+            self.log.debug("MySQLCollector: Connected to database.")
+        except MySQLError as e:
+            self.log.error("MySQLCollector couldn't connect to database %s", e)
             return False
+
         return True
 
     def disconnect(self):
         self.db.close()
 
     def get_db_global_status(self):
-        return self.get_db_stats('SHOW GLOBAL STATUS')
+        return self.get_db_stats("SHOW GLOBAL STATUS")
 
     def get_db_master_status(self):
-        return self.get_db_stats('SHOW MASTER STATUS')
+        return self.get_db_stats("SHOW MASTER STATUS")
 
     def get_db_slave_status(self):
-        return self.get_db_stats('SHOW SLAVE STATUS')
+        return self.get_db_stats("SHOW SLAVE STATUS")
 
     def get_db_innodb_status(self):
-        return self.get_db_stats('SHOW ENGINE INNODB STATUS')
+        return self.get_db_stats("SHOW ENGINE INNODB STATUS")
 
     def get_stats(self, params):
-        metrics = {'status': {}}
+        metrics = {"status": {}}
 
         if not self.connect(params):
             return metrics
 
         rows = self.get_db_global_status()
         for row in rows:
             try:
-                metrics['status'][row['Variable_name']] = float(row['Value'])
+                metrics["status"][row["Variable_name"]] = float(row["Value"])
             except:
                 pass
 
-        if self.config['master']:
-            metrics['master'] = {}
+        if self.config["master"]:
+            metrics["master"] = {}
             try:
                 rows = self.get_db_master_status()
                 for row_master in rows:
                     for key, value in row_master.items():
                         if key in self._IGNORE_KEYS:
                             continue
                         try:
-                            metrics['master'][key] = float(row_master[key])
+                            metrics["master"][key] = float(row_master[key])
                         except:
                             pass
             except:
-                self.log.error('MySQLCollector: Couldnt get master status')
+                self.log.error("MySQLCollector: Could not get master status")
                 pass
 
-        if self.config['slave']:
-            metrics['slave'] = {}
+        if self.config["slave"]:
+            metrics["slave"] = {}
+
             try:
                 rows = self.get_db_slave_status()
+
                 for row_slave in rows:
                     for key, value in row_slave.items():
                         if key in self._IGNORE_KEYS:
                             continue
                         try:
-                            metrics['slave'][key] = float(row_slave[key])
+                            metrics["slave"][key] = float(row_slave[key])
                         except:
                             pass
             except:
-                self.log.error('MySQLCollector: Couldnt get slave status')
+                self.log.error("MySQLCollector: Could not get slave status")
                 pass
 
-        if self.config['innodb']:
-            metrics['innodb'] = {}
+        if self.config["innodb"]:
+            metrics["innodb"] = {}
             innodb_status_timer = time.time()
+
             try:
                 rows = self.get_db_innodb_status()
-
                 innodb_status_output = rows[0]
-
                 todo = self.innodb_status_keys.keys()
-                for line in innodb_status_output['Status'].split('\n'):
+
+                for line in innodb_status_output["Status"].split("\n"):
                     for key in todo:
                         match = self.innodb_status_keys[key].match(line)
+
                         if match is not None:
                             todo.remove(key)
                             match_index = 1
-                            for key_index in key.split(','):
+
+                            for key_index in key.split(","):
                                 try:
                                     value = float(match.group(match_index))
                                     # store value
+
                                     if key_index in metrics:
-                                        self.log.debug("MySQLCollector: %s " +
-                                                       "already defined, " +
-                                                       "ignoring new value",
-                                                       key_index)
+                                        self.log.debug(
+                                            "MySQLCollector: %s "
+                                            + "already defined, "
+                                            + "ignoring new value",
+                                            key_index,
+                                        )
                                     else:
-                                        metrics['innodb'][key_index] = value
+                                        metrics["innodb"][key_index] = value
                                     match_index += 1
                                 except IndexError:
                                     self.log.debug(
-                                        "MySQLCollector: Cannot find value " +
-                                        "in innodb status for %s", key_index)
+                                        "MySQLCollector: Cannot find value "
+                                        + "in innodb status for %s",
+                                        key_index,
+                                    )
                 for key in todo:
-                    self.log.debug("MySQLCollector: %s regexp not matched " +
-                                   "in innodb status", key)
-            except Exception, innodb_status_error:
-                self.log.error('MySQLCollector: Couldnt get engine innodb ' +
-                               'status, check user permissions: %s',
-                               innodb_status_error)
+                    self.log.debug(
+                        "MySQLCollector: %s regexp not matched " + "in innodb status",
+                        key,
+                    )
+            except Exception as innodb_status_error:
+                self.log.error(
+                    "MySQLCollector: Could not get engine innodb "
+                    + "status, check user permissions: %s",
+                    innodb_status_error,
+                )
             Innodb_status_process_time = time.time() - innodb_status_timer
-            self.log.debug("MySQLCollector: innodb status process time: %f",
-                           Innodb_status_process_time)
+            self.log.debug(
+                "MySQLCollector: innodb status process time: %f",
+                Innodb_status_process_time,
+            )
             subkey = "Innodb_status_process_time"
-            metrics['innodb'][subkey] = Innodb_status_process_time
+            metrics["innodb"][subkey] = Innodb_status_process_time
 
         self.disconnect()
 
         return metrics
 
     def _publish_stats(self, nickname, metrics):
 
@@ -423,68 +445,73 @@
             for metric_name in metrics[key]:
                 metric_value = metrics[key][metric_name]
 
                 if type(metric_value) is not float:
                     continue
 
                 if metric_name not in self._GAUGE_KEYS:
-                    metric_value = self.derivative(nickname + metric_name,
-                                                   metric_value)
-                if key == 'status':
-                    if (('publish' not in self.config or
-                         metric_name in self.config['publish'])):
+                    metric_value = self.derivative(nickname + metric_name, metric_value)
+
+                if key == "status":
+                    if (
+                        "publish" not in self.config
+                        or metric_name in self.config["publish"]
+                    ):
                         self.publish(nickname + metric_name, metric_value)
                 else:
                     self.publish(nickname + metric_name, metric_value)
 
     def collect(self):
-
-        if MySQLdb is None:
-            self.log.error('Unable to import MySQLdb')
+        if mysqlclient is None:
+            self.log.error("Unable to import mysqlclient")
             return False
 
-        for host in self.config['hosts']:
-            matches = re.search(
-                '^([^:]*):([^@]*)@([^:]*):?([^/]*)/([^/]*)/?(.*)', host)
+        for host in self.config["hosts"]:
+            matches = re.search("^([^:]*):([^@]*)@([^:]*):?([^/]*)/([^/]*)/?(.*)", host)
 
             if not matches:
                 self.log.error(
-                    'Connection string not in required format, skipping: %s',
-                    host)
+                    "Connection string not in required format, skipping: %s", host
+                )
                 continue
 
-            params = {}
+            params = {"host": matches.group(3)}
 
-            params['host'] = matches.group(3)
             try:
-                params['port'] = int(matches.group(4))
+                params["port"] = int(matches.group(4))
             except ValueError:
-                params['port'] = 3306
-            params['db'] = matches.group(5)
-            params['user'] = matches.group(1)
-            params['passwd'] = matches.group(2)
+                params["port"] = 3306
+
+            params["db"] = matches.group(5)
+            params["user"] = matches.group(1)
+            params["passwd"] = matches.group(2)
 
             nickname = matches.group(6)
+
             if len(nickname):
-                nickname += '.'
+                nickname += "."
 
-            if params['db'] == 'None':
-                del params['db']
+            if params["db"] == "None":
+                del params["db"]
 
             try:
                 metrics = self.get_stats(params=params)
-            except Exception, e:
+            except Exception as e:
                 try:
                     self.disconnect()
-                except MySQLdb.ProgrammingError:
+                except mysqlclient.ProgrammingError:
                     pass
-                self.log.error('Collection failed for %s %s', nickname, e)
+
+                self.log.error("Collection failed for %s %s", nickname, e)
+
                 continue
 
             # Warn if publish contains an unknown variable
-            if 'publish' in self.config and metrics['status']:
-                for k in self.config['publish'].split():
-                    if k not in metrics['status']:
-                        self.log.error("No such key '%s' available, issue " +
-                                       "'show global status' for a full " +
-                                       "list", k)
+            if "publish" in self.config and metrics["status"]:
+                for k in self.config["publish"].split():
+                    if k not in metrics["status"]:
+                        self.log.error(
+                            "No such key '%s' available, issue 'show global status' for a full list",
+                            k,
+                        )
+
             self._publish_stats(nickname, metrics)
```

### Comparing `diamond-next-4.0.515/src/collectors/mysqlstat/mysql55.py` & `diamond-next-5.0.0/src/collectors/mysqlstat/mysql55.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,164 +7,151 @@
 
 [Blog](http://bit.ly/PbSkbN) announcement.
 
 [Snippet](http://bit.ly/SHwYhT) to build example graph.
 
 #### Dependencies
 
- * MySQLdb
+ * mysqlclient
  * MySQL 5.5.3+
 
 """
 
 from __future__ import division
 
+import re
+
+import time
+
+import diamond.collector
+
 try:
-    import MySQLdb
-    from MySQLdb import MySQLError
+    import mysqlclient
+    from mysqlclient import MySQLError
 except ImportError:
-    MySQLdb = None
-import diamond
-import time
-import re
+    mysqlclient = None
 
 
 class MySQLPerfCollector(diamond.collector.Collector):
-
     def process_config(self):
         super(MySQLPerfCollector, self).process_config()
         self.db = None
         self.last_wait_count = {}
         self.last_wait_sum = {}
         self.last_timestamp = {}
         self.last_data = {}
         self.monitors = {
-            'slave_sql': {
-                'wait/synch/cond/sql/MYSQL_RELAY_LOG::update_cond':
-                'wait_for_update',
-                'wait/io/file/innodb/innodb_data_file':
-                'innodb_data_file',
-                'wait/io/file/innodb/innodb_log_file':
-                'innodb_log_file',
-                'wait/io/file/myisam/dfile':
-                'myisam_dfile',
-                'wait/io/file/myisam/kfile':
-                'myisam_kfile',
-                'wait/io/file/sql/binlog':
-                'binlog',
-                'wait/io/file/sql/relay_log_info':
-                'relaylog_info',
-                'wait/io/file/sql/relaylog':
-                'relaylog',
-                'wait/synch/mutex/innodb':
-                'innodb_mutex',
-                'wait/synch/mutex':
-                'other_mutex',
-                'wait/synch/rwlock':
-                'rwlocks',
-                'wait/io':
-                'other_io',
+            "slave_sql": {
+                "wait/synch/cond/sql/MYSQL_RELAY_LOG::update_cond": "wait_for_update",
+                "wait/io/file/innodb/innodb_data_file": "innodb_data_file",
+                "wait/io/file/innodb/innodb_log_file": "innodb_log_file",
+                "wait/io/file/myisam/dfile": "myisam_dfile",
+                "wait/io/file/myisam/kfile": "myisam_kfile",
+                "wait/io/file/sql/binlog": "binlog",
+                "wait/io/file/sql/relay_log_info": "relaylog_info",
+                "wait/io/file/sql/relaylog": "relaylog",
+                "wait/synch/mutex/innodb": "innodb_mutex",
+                "wait/synch/mutex": "other_mutex",
+                "wait/synch/rwlock": "rwlocks",
+                "wait/io": "other_io",
+            },
+            "slave_io": {
+                "wait/io/file/sql/relaylog_index": "relaylog_index",
+                "wait/synch/mutex/sql/MYSQL_RELAY_LOG::LOCK_index": "relaylog_index_lock",
+                "wait/synch/mutex/sql/Master_info::data_lock": "master_info_lock",
+                "wait/synch/mutex/mysys/IO_CACHE::append_buffer_lock": "append_buffer_lock",
+                "wait/synch/mutex/sql/LOG::LOCK_log": "log_lock",
+                "wait/io/file/sql/master_info": "master_info",
+                "wait/io/file/sql/relaylog": "relaylog",
+                "wait/synch/mutex": "other_mutex",
+                "wait/synch/rwlock": "rwlocks",
+                "wait/io": "other_io",
             },
-            'slave_io': {
-                'wait/io/file/sql/relaylog_index':
-                'relaylog_index',
-                'wait/synch/mutex/sql/MYSQL_RELAY_LOG::LOCK_index':
-                'relaylog_index_lock',
-                'wait/synch/mutex/sql/Master_info::data_lock':
-                'master_info_lock',
-                'wait/synch/mutex/mysys/IO_CACHE::append_buffer_lock':
-                'append_buffer_lock',
-                'wait/synch/mutex/sql/LOG::LOCK_log':
-                'log_lock',
-                'wait/io/file/sql/master_info':
-                'master_info',
-                'wait/io/file/sql/relaylog':
-                'relaylog',
-                'wait/synch/mutex':
-                'other_mutex',
-                'wait/synch/rwlock':
-                'rwlocks',
-                'wait/io':
-                'other_io',
-            }
         }
 
-        if self.config['hosts'].__class__.__name__ != 'list':
-            self.config['hosts'] = [self.config['hosts']]
+        if self.config["hosts"].__class__.__name__ != "list":
+            self.config["hosts"] = [self.config["hosts"]]
 
             # Move legacy config format to new format
-        if 'host' in self.config:
+        if "host" in self.config:
             hoststr = "%s:%s@%s:%s/%s" % (
-                self.config['user'],
-                self.config['passwd'],
-                self.config['host'],
-                self.config['port'],
-                self.config['db'],
+                self.config["user"],
+                self.config["passwd"],
+                self.config["host"],
+                self.config["port"],
+                self.config["db"],
             )
-            self.config['hosts'].append(hoststr)
+            self.config["hosts"].append(hoststr)
 
     def get_default_config_help(self):
         config_help = super(MySQLPerfCollector, self).get_default_config_help()
-        config_help.update({
-            'hosts': 'List of hosts to collect from. Format is ' +
-                     'yourusername:yourpassword@host:' +
-                     'port/performance_schema[/nickname]',
-            'slave': 'Collect Slave Replication Metrics',
-        })
+        config_help.update(
+            {
+                "hosts": "List of hosts to collect from. Format is "
+                + "yourusername:yourpassword@host:"
+                + "port/performance_schema[/nickname]",
+                "slave": "Collect Slave Replication Metrics",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(MySQLPerfCollector, self).get_default_config()
-        config.update({
-            'path':     'mysql',
-            # Connection settings
-            'hosts':    [],
+        config.update(
+            {
+                "path": "mysql",
+                # Connection settings
+                "hosts": [],
+                "slave": "False",
+            }
+        )
 
-            'slave':    'False',
-        })
         return config
 
     def connect(self, params):
-        if MySQLdb is None:
-            self.log.error('Unable to import MySQLdb')
+        if mysqlclient is None:
+            self.log.error("Unable to import mysqlclient")
             return
 
         try:
-            self.db = MySQLdb.connect(**params)
-        except MySQLError, e:
-            self.log.error('MySQLPerfCollector couldnt connect to database %s',
-                           e)
+            self.db = mysqlclient.connect(**params)
+        except MySQLError as e:
+            self.log.error("MySQLPerfCollector couldn't connect to database %s", e)
             return {}
-        self.log.debug('MySQLPerfCollector: Connected to database.')
+
+        self.log.debug("MySQLPerfCollector: Connected to database.")
 
     def query_list(self, query, params):
         cursor = self.db.cursor()
         cursor.execute(query, params)
         return list(cursor.fetchall())
 
     def slave_load(self, nickname, thread):
-        data = self.query_list("""
+        data = self.query_list(
+            """
             SELECT
                 his.event_name,
                 his.sum_timer_wait,
                 his.count_star,
                 cur.event_name,
                 UNIX_TIMESTAMP(SYSDATE())
             FROM
                 events_waits_summary_by_thread_by_event_name his
                 JOIN threads thr USING (thread_id)
                 JOIN events_waits_current cur USING (thread_id)
             WHERE
                 name = %s
             ORDER BY
                 his.event_name
-            """, (thread,))
+            """,
+            (thread,),
+        )
 
         wait_sum = sum([x[1] for x in data])
         wait_count = sum([x[2] for x in data])
         timestamp = int(time.time())
 
         if 0 in data and len(data[0]) > 5:
             cur_event_name, timestamp = data[0][3:]
@@ -180,70 +167,102 @@
         wait_delta = wait_sum - self.last_wait_sum[thread]
         time_delta = (timestamp - self.last_timestamp[thread]) * 1000000000000
 
         if time_delta == 0:
             return
 
         # Summarize a few things
-        thread_name = thread[thread.rfind('/') + 1:]
+        thread_name = thread[thread.rfind("/") + 1 :]
         data.append(
-            ['wait/synch/mutex/innodb',
-             sum([x[1] for x in data
-                  if x[0].startswith('wait/synch/mutex/innodb')])])
+            [
+                "wait/synch/mutex/innodb",
+                sum([x[1] for x in data if x[0].startswith("wait/synch/mutex/innodb")]),
+            ]
+        )
         data.append(
-            ['wait/synch/mutex',
-             sum([x[1] for x in data
-                  if (x[0].startswith('wait/synch/mutex') and
-                      x[0] not in self.monitors[thread_name])]) - data[-1][1]])
+            [
+                "wait/synch/mutex",
+                sum(
+                    [
+                        x[1]
+                        for x in data
+                        if (
+                            x[0].startswith("wait/synch/mutex")
+                            and x[0] not in self.monitors[thread_name]
+                        )
+                    ]
+                )
+                - data[-1][1],
+            ]
+        )
         data.append(
-            ['wait/synch/rwlock',
-             sum([x[1] for x in data
-                  if x[0].startswith('wait/synch/rwlock')])])
+            [
+                "wait/synch/rwlock",
+                sum([x[1] for x in data if x[0].startswith("wait/synch/rwlock")]),
+            ]
+        )
         data.append(
-            ['wait/io',
-             sum([x[1] for x in data
-                  if (x[0].startswith('wait/io') and
-                      x[0] not in self.monitors[thread_name])])])
+            [
+                "wait/io",
+                sum(
+                    [
+                        x[1]
+                        for x in data
+                        if (
+                            x[0].startswith("wait/io")
+                            and x[0] not in self.monitors[thread_name]
+                        )
+                    ]
+                ),
+            ]
+        )
 
         for d in zip(self.last_data[thread], data):
             if d[0][0] in self.monitors[thread_name]:
-                self.publish(nickname + thread_name + '.' +
-                             self.monitors[thread_name][d[0][0]],
-                             (d[1][1] - d[0][1]) / time_delta * 100)
+                self.publish(
+                    nickname + thread_name + "." + self.monitors[thread_name][d[0][0]],
+                    (d[1][1] - d[0][1]) / time_delta * 100,
+                )
 
         # Also log what's unaccounted for. This is where Actual Work gets done
-        self.publish(nickname + thread_name + '.other_work',
-                     float(time_delta - wait_delta) / time_delta * 100)
+        self.publish(
+            nickname + thread_name + ".other_work",
+            float(time_delta - wait_delta) / time_delta * 100,
+        )
 
         self.last_wait_sum[thread] = wait_sum
         self.last_wait_count[thread] = wait_count
         self.last_timestamp[thread] = timestamp
         self.last_data[thread] = data
 
     def collect(self):
-        for host in self.config['hosts']:
+        for host in self.config["hosts"]:
             matches = re.search(
-                '^([^:]*):([^@]*)@([^:]*):?([^/]*)/([^/]*)/?(.*)$', host)
+                "^([^:]*):([^@]*)@([^:]*):?([^/]*)/([^/]*)/?(.*)$", host
+            )
 
             if not matches:
                 continue
 
-            params = {}
+            params = {"host": matches.group(3)}
 
-            params['host'] = matches.group(3)
             try:
-                params['port'] = int(matches.group(4))
+                params["port"] = int(matches.group(4))
             except ValueError:
-                params['port'] = 3306
-            params['db'] = matches.group(5)
-            params['user'] = matches.group(1)
-            params['passwd'] = matches.group(2)
+                params["port"] = 3306
+
+            params["db"] = matches.group(5)
+            params["user"] = matches.group(1)
+            params["passwd"] = matches.group(2)
 
             nickname = matches.group(6)
+
             if len(nickname):
-                nickname += '.'
+                nickname += "."
 
             self.connect(params=params)
-            if self.config['slave']:
-                self.slave_load(nickname, 'thread/sql/slave_io')
-                self.slave_load(nickname, 'thread/sql/slave_sql')
+
+            if self.config["slave"]:
+                self.slave_load(nickname, "thread/sql/slave_io")
+                self.slave_load(nickname, "thread/sql/slave_sql")
+
             self.db.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/slabinfo/slabinfo.py` & `diamond-next-5.0.0/src/collectors/slabinfo/slabinfo.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,87 +6,92 @@
 
 #### Dependencies
 
  * /proc/slabinfo
 
 """
 
-import platform
 import os
+import platform
+
 import diamond.collector
 
 # Detect the architecture of the system
 # and set the counters for MAX_VALUES
 # appropriately. Otherwise, rolling over
 # counters will cause incorrect or
 # negative values.
-if platform.architecture()[0] == '64bit':
-    counter = (2 ** 64) - 1
+if platform.architecture()[0] == "64bit":
+    counter = (2**64) - 1
 else:
-    counter = (2 ** 32) - 1
+    counter = (2**32) - 1
 
 
 class SlabInfoCollector(diamond.collector.Collector):
 
-    PROC = '/proc/slabinfo'
+    PROC = "/proc/slabinfo"
 
     def get_default_config_help(self):
         config_help = super(SlabInfoCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(SlabInfoCollector, self).get_default_config()
-        config.update({
-            'path':     'slabinfo'
-        })
+        config.update({"path": "slabinfo"})
         return config
 
     def collect(self):
         """
         Collect process stat data
         """
         if not os.access(self.PROC, os.R_OK):
             return False
 
         # Open PROC file
-        file = open(self.PROC, 'r')
+        file = open(self.PROC, "r")
 
         # Get data
         for line in file:
-            if line.startswith('slabinfo'):
+            if line.startswith("slabinfo"):
                 continue
 
-            if line.startswith('#'):
+            if line.startswith("#"):
                 keys = line.split()[1:]
                 continue
 
             data = line.split()
 
-            for key in ['<active_objs>', '<num_objs>', '<objsize>',
-                        '<objperslab>', '<pagesperslab>']:
+            for key in [
+                "<active_objs>",
+                "<num_objs>",
+                "<objsize>",
+                "<objperslab>",
+                "<pagesperslab>",
+            ]:
                 i = keys.index(key)
-                metric_name = data[0] + '.' + key.replace(
-                    '<', '').replace('>', '')
+                metric_name = data[0] + "." + key.replace("<", "").replace(">", "")
                 metric_value = int(data[i])
                 self.publish(metric_name, metric_value)
 
-            for key in ['<limit>', '<batchcount>', '<sharedfactor>']:
+            for key in ["<limit>", "<batchcount>", "<sharedfactor>"]:
                 i = keys.index(key)
-                metric_name = data[0] + '.tunables.' + key.replace(
-                    '<', '').replace('>', '')
+                metric_name = (
+                    data[0] + ".tunables." + key.replace("<", "").replace(">", "")
+                )
                 metric_value = int(data[i])
                 self.publish(metric_name, metric_value)
 
-            for key in ['<active_slabs>', '<num_slabs>', '<sharedavail>']:
+            for key in ["<active_slabs>", "<num_slabs>", "<sharedavail>"]:
                 i = keys.index(key)
-                metric_name = data[0] + '.slabdata.' + key.replace(
-                    '<', '').replace('>', '')
+                metric_name = (
+                    data[0] + ".slabdata." + key.replace("<", "").replace(">", "")
+                )
                 metric_value = int(data[i])
                 self.publish(metric_name, metric_value)
 
         # Close file
         file.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/memory_lxc/memory_lxc.py` & `diamond-next-5.0.0/src/collectors/memory_lxc/memory_lxc.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,75 +4,78 @@
 Collect Memory usage and limit of LXCs
 
 #### Dependencies
  * cgroup, I guess
 
 """
 
-from diamond.collector import Collector
-import diamond.convertor
 import os
 
+import diamond.collector
+import diamond.convertor
 
-class MemoryLxcCollector(Collector):
 
+class MemoryLxcCollector(diamond.collector.Collector):
     def get_default_config_help(self):
         """
         Return help text for collector configuration.
         """
         config_help = super(MemoryLxcCollector, self).get_default_config_help()
-        config_help.update({
-            "sys_path": "Defaults to '/sys/fs/cgroup/lxc'",
-        })
+        config_help.update(
+            {
+                "sys_path": "Defaults to '/sys/fs/cgroup/lxc'",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns default settings for collector.
         """
         config = super(MemoryLxcCollector, self).get_default_config()
-        config.update({
-            "path":     "lxc",
-            "sys_path": "/sys/fs/cgroup/lxc",
-        })
+        config.update(
+            {
+                "path": "lxc",
+                "sys_path": "/sys/fs/cgroup/lxc",
+            }
+        )
         return config
 
     def collect(self):
         """
         Collect memory stats of LXCs.
         """
         lxc_metrics = ["memory.usage_in_bytes", "memory.limit_in_bytes"]
         if os.path.isdir(self.config["sys_path"]) is False:
-            self.log.debug("sys_path '%s' isn't directory.",
-                           self.config["sys_path"])
+            self.log.debug("sys_path '%s' isn't directory.", self.config["sys_path"])
             return {}
 
         collected = {}
         for item in os.listdir(self.config["sys_path"]):
             fpath = "%s/%s" % (self.config["sys_path"], item)
             if os.path.isdir(fpath) is False:
                 continue
 
             for lxc_metric in lxc_metrics:
                 filename = "%s/%s" % (fpath, lxc_metric)
                 metric_name = "%s.%s" % (
                     item.replace(".", "_"),
-                    lxc_metric.replace("_in_bytes", ""))
+                    lxc_metric.replace("_in_bytes", ""),
+                )
                 self.log.debug("Trying to collect from %s", filename)
                 collected[metric_name] = self._read_file(filename)
 
         for key in collected.keys():
             if collected[key] is None:
                 continue
 
             for unit in self.config["byte_unit"]:
                 value = diamond.convertor.binary.convert(
-                    collected[key],
-                    oldUnit="B",
-                    newUnit=unit)
+                    collected[key], old_unit="B", new_unit=unit
+                )
                 new_key = "%s_in_%ss" % (key, unit)
                 self.log.debug("Publishing '%s %s'", new_key, value)
                 self.publish(new_key, value, metric_type="GAUGE")
 
     def _read_file(self, filename):
         """
         Read contents of given file.
```

### Comparing `diamond-next-4.0.515/src/collectors/elasticsearch/elasticsearch.py` & `diamond-next-5.0.0/src/collectors/elasticsearch/elasticsearch.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,416 +5,509 @@
 
 Supports multiple instances. When using the 'instances'
 parameter the instance alias will be appended to the
 'path' parameter.
 
 #### Dependencies
 
- * urlib2
+ * urllib
 
 """
 
-import urllib2
+import base64
 import re
-from diamond.collector import str_to_bool
+import urllib.request
+
+import diamond.collector
 
 try:
     import json
 except ImportError:
     import simplejson as json
 
-import diamond.collector
-
-RE_LOGSTASH_INDEX = re.compile('^(.*)-\d\d\d\d\.\d\d\.\d\d$')
+RE_LOGSTASH_INDEX = re.compile("^(.*)-\d{4}(\.\d{2}){2,3}$")
 
 
 class ElasticSearchCollector(diamond.collector.Collector):
-
     def process_config(self):
         super(ElasticSearchCollector, self).process_config()
-        instance_list = self.config['instances']
-        if isinstance(instance_list, basestring):
+        instance_list = self.config["instances"]
+
+        if isinstance(instance_list, str):
             instance_list = [instance_list]
 
         if len(instance_list) == 0:
-            host = self.config['host']
-            port = self.config['port']
+            host = self.config["host"]
+            port = self.config["port"]
             # use empty alias to identify single-instance config
             # omitting the use of the alias in the metrics path
-            instance_list.append('@%s:%s' % (host, port))
+            instance_list.append("@%s:%s" % (host, port))
 
         self.instances = {}
+
         for instance in instance_list:
-            if '@' in instance:
-                (alias, hostport) = instance.split('@', 1)
+            if "@" in instance:
+                (alias, hostport) = instance.split("@", 1)
             else:
-                alias = 'default'
+                alias = "default"
                 hostport = instance
 
-            if ':' in hostport:
-                host, port = hostport.split(':', 1)
+            if ":" in hostport:
+                host, port = hostport.split(":", 1)
             else:
                 host = hostport
                 port = 9200
 
             self.instances[alias] = (host, int(port))
 
     def get_default_config_help(self):
-        config_help = super(ElasticSearchCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': "",
-            'port': "",
-            'instances': "List of instances. When set this overrides "
-            "the 'host' and 'port' settings. Instance format: "
-            "instance [<alias>@]<hostname>[:<port>]",
-            'scheme': "http (default) or https",
-            'cluster': "cluster/node/shard health",
-            'stats':
-                "Available stats:\n" +
-                " - jvm (JVM information)\n" +
-                " - thread_pool (Thread pool information)\n" +
-                " - indices (Individual index stats)\n",
-            'logstash_mode':
-                "If 'indices' stats are gathered, remove " +
-                "the YYYY.MM.DD suffix from the index name " +
-                "(e.g. logstash-adm-syslog-2014.01.03) and use that " +
-                "as a bucket for all 'day' index stats.",
-        })
+        config_help = super(ElasticSearchCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "",
+                "port": "",
+                "user": "Username for Basic/Shield auth",
+                "password": "Password for Basic/Shield auth",
+                "instances": "List of instances. When set this overrides "
+                "the 'host' and 'port' settings. Instance format: "
+                "instance [<alias>@]<hostname>[:<port>]",
+                "scheme": "http (default) or https",
+                "cluster": "cluster/node/shard health",
+                "stats": "Available stats:\n"
+                + " - jvm (JVM information)\n"
+                + " - thread_pool (Thread pool information)\n"
+                + " - indices (Individual index stats)\n",
+                "logstash_mode": "If 'indices' stats are gathered, remove "
+                + "the YYYY.MM.DD suffix from the index name "
+                + "(e.g. logstash-adm-syslog-2014.01.03) and use that "
+                + "as a bucket for all 'day' index stats.",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ElasticSearchCollector, self).get_default_config()
-        config.update({
-            'host':           '127.0.0.1',
-            'port':           9200,
-            'instances':      [],
-            'scheme':         'http',
-            'path':           'elasticsearch',
-            'stats':          ['jvm', 'thread_pool', 'indices'],
-            'logstash_mode': False,
-            'cluster':       False,
-        })
+        config.update(
+            {
+                "host": "127.0.0.1",
+                "port": 9200,
+                "user": "",
+                "password": "",
+                "instances": [],
+                "scheme": "http",
+                "path": "elasticsearch",
+                "stats": ["jvm", "thread_pool", "indices"],
+                "logstash_mode": False,
+                "cluster": False,
+            }
+        )
         return config
 
     def _get(self, scheme, host, port, path, assert_key=None):
         """
         Execute a ES API call. Convert response into JSON and
         optionally assert its structure.
         """
-        url = '%s://%s:%i/%s' % (scheme, host, port, path)
+        url = "%s://%s:%i/%s" % (scheme, host, port, path)
+
         try:
-            response = urllib2.urlopen(url)
-        except Exception, err:
-            self.log.error("%s: %s", url, err)
+            request = urllib.request.Request(url)
+
+            if self.config["user"] and self.config["password"]:
+                base64string = base64.b64encode(
+                    bytes(
+                        "%s:%s" % (self.config["user"], self.config["password"]),
+                        "utf-8",
+                    )
+                )
+                request.add_header("Authorization", "Basic %s" % base64string)
+
+            response = urllib.request.urlopen(request)
+        except Exception as err:
+            self.log.error("%s: %s" % (url, err))
             return False
 
         try:
             doc = json.load(response)
         except (TypeError, ValueError):
-            self.log.error("Unable to parse response from elasticsearch as a" +
-                           " json object")
+            self.log.error(
+                "Unable to parse response from elasticsearch as a json object"
+            )
+
             return False
 
         if assert_key and assert_key not in doc:
-            self.log.error("Bad response from elasticsearch, expected key "
-                           "'%s' was missing for %s" % (assert_key, url))
+            self.log.error(
+                "Bad response from elasticsearch, expected key '%s' was missing for %s"
+                % (assert_key, url)
+            )
+
             return False
+
         return doc
 
     def _copy_one_level(self, metrics, prefix, data, filter=lambda key: True):
-        for key, value in data.iteritems():
+        for key, value in iter(data.items()):
             if filter(key):
-                metric_path = '%s.%s' % (prefix, key)
+                metric_path = "%s.%s" % (prefix, key)
                 self._set_or_sum_metric(metrics, metric_path, value)
 
     def _copy_two_level(self, metrics, prefix, data, filter=lambda key: True):
-        for key1, d1 in data.iteritems():
-            self._copy_one_level(metrics, '%s.%s' % (prefix, key1), d1, filter)
+        for key1, d1 in iter(data.items()):
+            self._copy_one_level(metrics, "%s.%s" % (prefix, key1), d1, filter)
 
     def _index_metrics(self, metrics, prefix, index):
-        if self.config['logstash_mode']:
+        if self.config["logstash_mode"]:
             """Remove the YYYY.MM.DD bit from logstash indices.
             This way we keep using the same metric naming and not polute
             our metrics system (e.g. Graphite) with new metrics every day."""
             m = RE_LOGSTASH_INDEX.match(prefix)
+
             if m:
                 prefix = m.group(1)
 
                 # keep a telly of the number of indexes
-                self._set_or_sum_metric(metrics,
-                                        '%s.indexes_in_group' % prefix, 1)
+                self._set_or_sum_metric(metrics, "%s.indexes_in_group" % prefix, 1)
 
-        self._add_metric(metrics, '%s.docs.count' % prefix, index,
-                         ['docs', 'count'])
-        self._add_metric(metrics, '%s.docs.deleted' % prefix, index,
-                         ['docs', 'deleted'])
-        self._add_metric(metrics, '%s.datastore.size' % prefix, index,
-                         ['store', 'size_in_bytes'])
+        self._add_metric(metrics, "%s.docs.count" % prefix, index, ["docs", "count"])
+        self._add_metric(
+            metrics, "%s.docs.deleted" % prefix, index, ["docs", "deleted"]
+        )
+        self._add_metric(
+            metrics, "%s.datastore.size" % prefix, index, ["store", "size_in_bytes"]
+        )
 
         # publish all 'total' and 'time_in_millis' stats
         self._copy_two_level(
-            metrics, prefix, index,
-            lambda key: key.endswith('total') or key.endswith('time_in_millis') or key.endswith('in_bytes'))  # noqa
+            metrics,
+            prefix,
+            index,
+            lambda key: key.endswith("total")
+            or key.endswith("time_in_millis")
+            or key.endswith("in_bytes"),
+        )  # noqa
 
     def _add_metric(self, metrics, metric_path, data, data_path):
         """If the path specified by data_path (a list) exists in data,
         add to metrics.  Use when the data path may not be present"""
         current_item = data
+
         for path_element in data_path:
             current_item = current_item.get(path_element)
+
             if current_item is None:
                 return
 
         self._set_or_sum_metric(metrics, metric_path, current_item)
 
     def _set_or_sum_metric(self, metrics, metric_path, value):
         """If we already have a datapoint for this metric, lets add
         the value. This is used when the logstash mode is enabled."""
         if metric_path in metrics:
             metrics[metric_path] += value
         else:
             metrics[metric_path] = value
 
     def collect_instance_cluster_stats(self, scheme, host, port, metrics):
-        result = self._get(scheme, host, port, '_cluster/health')
+        result = self._get(scheme, host, port, "_cluster/health")
+
         if not result:
             return
 
-        self._add_metric(metrics, 'cluster_health.nodes.total',
-                         result, ['number_of_nodes'])
-        self._add_metric(metrics, 'cluster_health.nodes.data',
-                         result, ['number_of_data_nodes'])
-        self._add_metric(metrics, 'cluster_health.shards.active_primary',
-                         result, ['active_primary_shards'])
-        self._add_metric(metrics, 'cluster_health.shards.active',
-                         result, ['active_shards'])
-        self._add_metric(metrics, 'cluster_health.shards.relocating',
-                         result, ['relocating_shards'])
-        self._add_metric(metrics, 'cluster_health.shards.unassigned',
-                         result, ['unassigned_shards'])
-        self._add_metric(metrics, 'cluster_health.shards.initializing',
-                         result, ['initializing_shards'])
-
-        CLUSTER_STATUS = {
-            'green': 2,
-            'yellow': 1,
-            'red': 0
-        }
-        metrics['cluster_health.status'] = CLUSTER_STATUS[result['status']]
+        self._add_metric(
+            metrics, "cluster_health.nodes.total", result, ["number_of_nodes"]
+        )
+        self._add_metric(
+            metrics, "cluster_health.nodes.data", result, ["number_of_data_nodes"]
+        )
+        self._add_metric(
+            metrics,
+            "cluster_health.nodes.pending_tasks",
+            result,
+            ["number_of_pending_tasks"],
+        )
+        self._add_metric(
+            metrics,
+            "cluster_health.shards.active_primary",
+            result,
+            ["active_primary_shards"],
+        )
+        self._add_metric(
+            metrics, "cluster_health.shards.active", result, ["active_shards"]
+        )
+        self._add_metric(
+            metrics,
+            "cluster_health.shards.active_percent",
+            result,
+            ["active_shards_percent_as_number"],
+        )
+        self._add_metric(
+            metrics,
+            "cluster_health.shards.delayed_unassigned",
+            result,
+            ["delayed_unassigned_shards"],
+        )
+        self._add_metric(
+            metrics, "cluster_health.shards.relocating", result, ["relocating_shards"]
+        )
+        self._add_metric(
+            metrics, "cluster_health.shards.unassigned", result, ["unassigned_shards"]
+        )
+        self._add_metric(
+            metrics,
+            "cluster_health.shards.initializing",
+            result,
+            ["initializing_shards"],
+        )
+
+        cluster_status = {"green": 2, "yellow": 1, "red": 0}
+        metrics["cluster_health.status"] = cluster_status[result["status"]]
 
     def collect_instance_index_stats(self, scheme, host, port, metrics):
-        result = self._get(scheme, host, port, '_stats', '_all')
+        result = self._get(scheme, host, port, "_stats", "_all")
+
         if not result:
             return
 
-        _all = result['_all']
-        self._index_metrics(metrics, 'indices._all', _all['primaries'])
+        _all = result["_all"]
+        self._index_metrics(metrics, "indices._all", _all["primaries"])
 
-        if 'indices' in _all:
-            indices = _all['indices']
-        elif 'indices' in result:          # elasticsearch >= 0.90RC2
-            indices = result['indices']
+        if "indices" in _all:
+            indices = _all["indices"]
+        elif "indices" in result:  # elasticsearch >= 0.90RC2
+            indices = result["indices"]
         else:
             return
 
-        for name, index in indices.iteritems():
-            self._index_metrics(metrics, 'indices.%s' % name,
-                                index['primaries'])
+        for name, index in iter(indices.items()):
+            self._index_metrics(metrics, "indices.%s" % name, index["primaries"])
 
     def collect_instance(self, alias, scheme, host, port):
-        result = self._get(scheme, host, port, '_nodes/_local/stats', 'nodes')
+        result = self._get(scheme, host, port, "_nodes/_local/stats", "nodes")
+
         if not result:
             return
 
         metrics = {}
-        node = result['nodes'].keys()[0]
-        data = result['nodes'][node]
+        node = result["nodes"].keys()[0]
+        data = result["nodes"][node]
 
         #
         # http connections to ES
-        metrics['http.current'] = data['http']['current_open']
+        metrics["http.current"] = data["http"]["current_open"]
 
         #
         # indices
-        indices = data['indices']
-        metrics['indices.docs.count'] = indices['docs']['count']
-        metrics['indices.docs.deleted'] = indices['docs']['deleted']
-
-        metrics['indices.datastore.size'] = indices['store']['size_in_bytes']
-
-        transport = data['transport']
-        metrics['transport.rx.count'] = transport['rx_count']
-        metrics['transport.rx.size'] = transport['rx_size_in_bytes']
-        metrics['transport.tx.count'] = transport['tx_count']
-        metrics['transport.tx.size'] = transport['tx_size_in_bytes']
+        indices = data["indices"]
+        metrics["indices.docs.count"] = indices["docs"]["count"]
+        metrics["indices.docs.deleted"] = indices["docs"]["deleted"]
+
+        metrics["indices.datastore.size"] = indices["store"]["size_in_bytes"]
+
+        transport = data["transport"]
+        metrics["transport.rx.count"] = transport["rx_count"]
+        metrics["transport.rx.size"] = transport["rx_size_in_bytes"]
+        metrics["transport.tx.count"] = transport["tx_count"]
+        metrics["transport.tx.size"] = transport["tx_size_in_bytes"]
 
         # elasticsearch < 0.90RC2
-        if 'cache' in indices:
-            cache = indices['cache']
+        if "cache" in indices:
+            cache = indices["cache"]
 
-            self._add_metric(metrics, 'cache.bloom.size', cache,
-                             ['bloom_size_in_bytes'])
-            self._add_metric(metrics, 'cache.field.evictions', cache,
-                             ['field_evictions'])
-            self._add_metric(metrics, 'cache.field.size', cache,
-                             ['field_size_in_bytes'])
-            metrics['cache.filter.count'] = cache['filter_count']
-            metrics['cache.filter.evictions'] = cache['filter_evictions']
-            metrics['cache.filter.size'] = cache['filter_size_in_bytes']
-            self._add_metric(metrics, 'cache.id.size', cache,
-                             ['id_cache_size_in_bytes'])
+            self._add_metric(
+                metrics, "cache.bloom.size", cache, ["bloom_size_in_bytes"]
+            )
+            self._add_metric(
+                metrics, "cache.field.evictions", cache, ["field_evictions"]
+            )
+            self._add_metric(
+                metrics, "cache.field.size", cache, ["field_size_in_bytes"]
+            )
+            metrics["cache.filter.count"] = cache["filter_count"]
+            metrics["cache.filter.evictions"] = cache["filter_evictions"]
+            metrics["cache.filter.size"] = cache["filter_size_in_bytes"]
+            self._add_metric(
+                metrics, "cache.id.size", cache, ["id_cache_size_in_bytes"]
+            )
 
         # elasticsearch >= 0.90RC2
-        if 'filter_cache' in indices:
-            cache = indices['filter_cache']
+        if "filter_cache" in indices:
+            cache = indices["filter_cache"]
 
-            metrics['cache.filter.evictions'] = cache['evictions']
-            metrics['cache.filter.size'] = cache['memory_size_in_bytes']
-            self._add_metric(metrics, 'cache.filter.count', cache, ['count'])
+            metrics["cache.filter.evictions"] = cache["evictions"]
+            metrics["cache.filter.size"] = cache["memory_size_in_bytes"]
+            self._add_metric(metrics, "cache.filter.count", cache, ["count"])
 
         # elasticsearch >= 0.90RC2
-        if 'id_cache' in indices:
-            cache = indices['id_cache']
+        if "id_cache" in indices:
+            cache = indices["id_cache"]
 
-            self._add_metric(metrics, 'cache.id.size', cache,
-                             ['memory_size_in_bytes'])
+            self._add_metric(metrics, "cache.id.size", cache, ["memory_size_in_bytes"])
 
-        if 'query_cache' in indices:
-            cache = indices['query_cache']
+        if "query_cache" in indices:
+            cache = indices["query_cache"]
 
-            metrics['cache.query.evictions'] = cache['evictions']
-            metrics['cache.query.size'] = cache['memory_size_in_bytes']
-            self._add_metric(metrics, 'cache.query.hit_count', cache,
-                             ['hit_count'])
-            self._add_metric(metrics, 'cache.query.miss_count', cache,
-                             ['miss_count'])
+            metrics["cache.query.evictions"] = cache["evictions"]
+            metrics["cache.query.size"] = cache["memory_size_in_bytes"]
+            self._add_metric(metrics, "cache.query.hit_count", cache, ["hit_count"])
+            self._add_metric(metrics, "cache.query.miss_count", cache, ["miss_count"])
 
         # elasticsearch >= 0.90
-        if 'fielddata' in indices:
-            fielddata = indices['fielddata']
-            self._add_metric(metrics, 'fielddata.size', fielddata,
-                             ['memory_size_in_bytes'])
-            self._add_metric(metrics, 'fielddata.evictions', fielddata,
-                             ['evictions'])
-
-        if 'segments' in indices:
-            segments = indices['segments']
-            self._add_metric(metrics, 'segments.count', segments, ['count'])
-            self._add_metric(metrics, 'segments.mem.size', segments,
-                             ['memory_in_bytes'])
-            self._add_metric(metrics, 'segments.index_writer.mem.size',
-                             segments, ['index_writer_memory_in_bytes'])
-            self._add_metric(metrics, 'segments.index_writer.mem.max_size',
-                             segments, ['index_writer_max_memory_in_bytes'])
-            self._add_metric(metrics, 'segments.version_map.mem.size',
-                             segments, ['version_map_memory_in_bytes'])
-            self._add_metric(metrics, 'segments.fixed_bit_set.mem.size',
-                             segments, ['fixed_bit_set_memory_in_bytes'])
+        if "fielddata" in indices:
+            fielddata = indices["fielddata"]
+            self._add_metric(
+                metrics, "fielddata.size", fielddata, ["memory_size_in_bytes"]
+            )
+            self._add_metric(metrics, "fielddata.evictions", fielddata, ["evictions"])
+
+        if "segments" in indices:
+            segments = indices["segments"]
+            self._add_metric(metrics, "segments.count", segments, ["count"])
+            self._add_metric(
+                metrics, "segments.mem.size", segments, ["memory_in_bytes"]
+            )
+            self._add_metric(
+                metrics,
+                "segments.index_writer.mem.size",
+                segments,
+                ["index_writer_memory_in_bytes"],
+            )
+            self._add_metric(
+                metrics,
+                "segments.index_writer.mem.max_size",
+                segments,
+                ["index_writer_max_memory_in_bytes"],
+            )
+            self._add_metric(
+                metrics,
+                "segments.version_map.mem.size",
+                segments,
+                ["version_map_memory_in_bytes"],
+            )
+            self._add_metric(
+                metrics,
+                "segments.fixed_bit_set.mem.size",
+                segments,
+                ["fixed_bit_set_memory_in_bytes"],
+            )
+
+        # process mem/cpu (may not be present, depending on access restrictions)
+        self._add_metric(
+            metrics, "process.cpu.percent", data, ["process", "cpu", "percent"]
+        )
+        self._add_metric(
+            metrics,
+            "process.mem.resident",
+            data,
+            ["process", "mem", "resident_in_bytes"],
+        )
+        self._add_metric(
+            metrics, "process.mem.share", data, ["process", "mem", "share_in_bytes"]
+        )
+        self._add_metric(
+            metrics,
+            "process.mem.virtual",
+            data,
+            ["process", "mem", "total_virtual_in_bytes"],
+        )
 
-        #
-        # process mem/cpu (may not be present, depending on access
-        # restrictions)
-        self._add_metric(metrics, 'process.cpu.percent', data,
-                         ['process', 'cpu', 'percent'])
-        self._add_metric(metrics, 'process.mem.resident', data,
-                         ['process', 'mem', 'resident_in_bytes'])
-        self._add_metric(metrics, 'process.mem.share', data,
-                         ['process', 'mem', 'share_in_bytes'])
-        self._add_metric(metrics, 'process.mem.virtual', data,
-                         ['process', 'mem', 'total_virtual_in_bytes'])
-
-        #
         # filesystem (may not be present, depending on access restrictions)
-        if 'fs' in data and 'data' in data['fs'] and data['fs']['data']:
-            fs_data = data['fs']['data'][0]
-            self._add_metric(metrics, 'disk.reads.count', fs_data,
-                             ['disk_reads'])
-            self._add_metric(metrics, 'disk.reads.size', fs_data,
-                             ['disk_read_size_in_bytes'])
-            self._add_metric(metrics, 'disk.writes.count', fs_data,
-                             ['disk_writes'])
-            self._add_metric(metrics, 'disk.writes.size', fs_data,
-                             ['disk_write_size_in_bytes'])
+        if "fs" in data and "data" in data["fs"] and data["fs"]["data"]:
+            fs_data = data["fs"]["data"][0]
+            self._add_metric(metrics, "disk.reads.count", fs_data, ["disk_reads"])
+            self._add_metric(
+                metrics, "disk.reads.size", fs_data, ["disk_read_size_in_bytes"]
+            )
+            self._add_metric(metrics, "disk.writes.count", fs_data, ["disk_writes"])
+            self._add_metric(
+                metrics, "disk.writes.size", fs_data, ["disk_write_size_in_bytes"]
+            )
 
-        #
         # jvm
-        if 'jvm' in self.config['stats']:
-            jvm = data['jvm']
-            mem = jvm['mem']
-            for k in ('heap_used', 'heap_committed', 'non_heap_used',
-                      'non_heap_committed'):
-                metrics['jvm.mem.%s' % k] = mem['%s_in_bytes' % k]
-
-            if 'heap_used_percent' in mem:
-                metrics['jvm.mem.heap_used_percent'] = mem['heap_used_percent']
-
-            for pool, d in mem['pools'].iteritems():
-                pool = pool.replace(' ', '_')
-                metrics['jvm.mem.pools.%s.used' % pool] = d['used_in_bytes']
-                metrics['jvm.mem.pools.%s.max' % pool] = d['max_in_bytes']
+        if "jvm" in self.config["stats"]:
+            jvm = data["jvm"]
+            mem = jvm["mem"]
+
+            for k in (
+                "heap_used",
+                "heap_committed",
+                "non_heap_used",
+                "non_heap_committed",
+            ):
+                metrics["jvm.mem.%s" % k] = mem["%s_in_bytes" % k]
+
+            if "heap_used_percent" in mem:
+                metrics["jvm.mem.heap_used_percent"] = mem["heap_used_percent"]
+
+            for pool, d in iter(mem["pools"].items()):
+                pool = pool.replace(" ", "_")
+                metrics["jvm.mem.pools.%s.used" % pool] = d["used_in_bytes"]
+                metrics["jvm.mem.pools.%s.max" % pool] = d["max_in_bytes"]
 
-            metrics['jvm.threads.count'] = jvm['threads']['count']
+            metrics["jvm.threads.count"] = jvm["threads"]["count"]
 
-            gc = jvm['gc']
+            gc = jvm["gc"]
             collection_count = 0
             collection_time_in_millis = 0
-            for collector, d in gc['collectors'].iteritems():
-                metrics['jvm.gc.collection.%s.count' % collector] = d[
-                    'collection_count']
-                collection_count += d['collection_count']
-                metrics['jvm.gc.collection.%s.time' % collector] = d[
-                    'collection_time_in_millis']
-                collection_time_in_millis += d['collection_time_in_millis']
-            # calculate the totals, as they're absent in elasticsearch >
-            # 0.90.10
-            if 'collection_count' in gc:
-                metrics['jvm.gc.collection.count'] = gc['collection_count']
+
+            for collector, d in iter(gc["collectors"].items()):
+                metrics["jvm.gc.collection.%s.count" % collector] = d[
+                    "collection_count"
+                ]
+                collection_count += d["collection_count"]
+                metrics["jvm.gc.collection.%s.time" % collector] = d[
+                    "collection_time_in_millis"
+                ]
+                collection_time_in_millis += d["collection_time_in_millis"]
+
+            # calculate the totals, as they're absent in elasticsearch > 0.90.10
+            if "collection_count" in gc:
+                metrics["jvm.gc.collection.count"] = gc["collection_count"]
             else:
-                metrics['jvm.gc.collection.count'] = collection_count
+                metrics["jvm.gc.collection.count"] = collection_count
+
+            k = "collection_time_in_millis"
 
-            k = 'collection_time_in_millis'
             if k in gc:
-                metrics['jvm.gc.collection.time'] = gc[k]
+                metrics["jvm.gc.collection.time"] = gc[k]
             else:
-                metrics['jvm.gc.collection.time'] = collection_time_in_millis
+                metrics["jvm.gc.collection.time"] = collection_time_in_millis
 
-        #
         # thread_pool
-        if 'thread_pool' in self.config['stats']:
-            self._copy_two_level(metrics, 'thread_pool', data['thread_pool'])
+        if "thread_pool" in self.config["stats"]:
+            self._copy_two_level(metrics, "thread_pool", data["thread_pool"])
 
-        #
         # network
-        if 'network' in data:
-            self._copy_two_level(metrics, 'network', data['network'])
+        if "network" in data:
+            self._copy_two_level(metrics, "network", data["network"])
 
-        #
         # cluster (optional)
-        if str_to_bool(self.config['cluster']):
+        if diamond.collector.str_to_bool(self.config["cluster"]):
             self.collect_instance_cluster_stats(scheme, host, port, metrics)
 
-        #
         # indices (optional)
-        if 'indices' in self.config['stats']:
+        if "indices" in self.config["stats"]:
             self.collect_instance_index_stats(scheme, host, port, metrics)
 
-        #
         # all done, now publishing all metrics
         for key in metrics:
             full_key = key
-            if alias != '':
-                full_key = '%s.%s' % (alias, full_key)
+
+            if alias != "":
+                full_key = "%s.%s" % (alias, full_key)
+
             self.publish(full_key, metrics[key])
 
     def collect(self):
         if json is None:
-            self.log.error('Unable to import json')
+            self.log.error("Unable to import json")
+
             return {}
 
-        scheme = self.config['scheme']
+        scheme = self.config["scheme"]
+
         for alias in sorted(self.instances):
             (host, port) = self.instances[alias]
             self.collect_instance(alias, scheme, host, port)
```

### Comparing `diamond-next-4.0.515/src/collectors/ups/ups.py` & `diamond-next-5.0.0/src/collectors/ups/ups.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,59 +5,64 @@
 
 #### Dependencies
 
  * nut/upsc to be installed, configured and running.
 
 """
 
-import diamond.collector
 import os
 import subprocess
+
+import diamond.collector
 from diamond.collector import str_to_bool
 
 
 class UPSCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(UPSCollector, self).get_default_config_help()
-        config_help.update({
-            'ups_name':    'The name of the ups to collect data for',
-            'bin':         'The path to the upsc binary',
-            'use_sudo':    'Use sudo?',
-            'sudo_cmd':    'Path to sudo',
-        })
+        config_help.update(
+            {
+                "ups_name": "The name of the ups to collect data for",
+                "bin": "The path to the upsc binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns default collector settings.
         """
 
         config = super(UPSCollector, self).get_default_config()
-        config.update({
-            'path':             'ups',
-            'ups_name':         'cyberpower',
-            'bin':              '/bin/upsc',
-            'use_sudo':         False,
-            'sudo_cmd':         '/usr/bin/sudo',
-        })
+        config.update(
+            {
+                "path": "ups",
+                "ups_name": "cyberpower",
+                "bin": "/bin/upsc",
+                "use_sudo": False,
+                "sudo_cmd": "/usr/bin/sudo",
+            }
+        )
+
         return config
 
     def collect(self):
-        if not os.access(self.config['bin'], os.X_OK):
-            self.log.error("%s is not executable", self.config['bin'])
+        if not os.access(self.config["bin"], os.X_OK):
+            self.log.error("%s is not executable", self.config["bin"])
             return False
 
-        command = [self.config['bin'], self.config['ups_name']]
+        command = [self.config["bin"], self.config["ups_name"]]
 
-        if str_to_bool(self.config['use_sudo']):
-            command.insert(0, self.config['sudo_cmd'])
+        if str_to_bool(self.config["use_sudo"]):
+            command.insert(0, self.config["sudo_cmd"])
 
-        p = subprocess.Popen(command,
-                             stdout=subprocess.PIPE).communicate()[0]
+        p = subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0]
 
         for ln in p.strip().splitlines():
             datapoint = ln.split(": ")
 
             try:
                 val = float(datapoint[1])
             except:
```

### Comparing `diamond-next-4.0.515/src/collectors/udp/udp.py` & `diamond-next-5.0.0/src/collectors/udp/udp.py`

 * *Files 11% similar despite different names*

```diff
@@ -5,94 +5,99 @@
 
 #### Dependencies
 
  * /proc/net/snmp
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
 
-class UDPCollector(diamond.collector.Collector):
 
-    PROC = [
-        '/proc/net/snmp'
-    ]
+class UDPCollector(diamond.collector.Collector):
+    PROC = ["/proc/net/snmp"]
 
     def process_config(self):
         super(UDPCollector, self).process_config()
-        if self.config['allowed_names'] is None:
-            self.config['allowed_names'] = []
+
+        if self.config["allowed_names"] is None:
+            self.config["allowed_names"] = []
 
     def get_default_config_help(self):
         config_help = super(UDPCollector, self).get_default_config_help()
-        config_help.update({
-            'allowed_names': 'list of entries to collect, empty to collect all',
-        })
+        config_help.update(
+            {
+                "allowed_names": "list of entries to collect, empty to collect all",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(UDPCollector, self).get_default_config()
-        config.update({
-            'path':          'udp',
-            'allowed_names': 'InDatagrams, NoPorts, InErrors, ' +
-                             'OutDatagrams, RcvbufErrors, SndbufErrors'
-        })
+        config.update(
+            {
+                "path": "udp",
+                "allowed_names": "InDatagrams, NoPorts, InErrors, OutDatagrams, RcvbufErrors, SndbufErrors",
+            }
+        )
         return config
 
     def collect(self):
         metrics = {}
 
         for filepath in self.PROC:
             if not os.access(filepath, os.R_OK):
-                self.log.error('Permission to access %s denied', filepath)
+                self.log.error("Permission to access %s denied", filepath)
                 continue
 
-            header = ''
-            data = ''
+            header = ""
+            data = ""
 
             # Seek the file for the lines that start with Tcp
             file = open(filepath)
 
             if not file:
-                self.log.error('Failed to open %s', filepath)
+                self.log.error("Failed to open %s", filepath)
                 continue
 
             while True:
                 line = file.readline()
 
                 # Reached EOF?
                 if len(line) == 0:
                     break
 
                 # Line has metrics?
                 if line.startswith("Udp"):
                     header = line
                     data = file.readline()
                     break
+
             file.close()
 
             # No data from the file?
-            if header == '' or data == '':
-                self.log.error('%s has no lines with Udp', filepath)
+            if header == "" or data == "":
+                self.log.error("%s has no lines with Udp", filepath)
                 continue
 
             header = header.split()
             data = data.split()
 
-            for i in xrange(1, len(header)):
+            for i in range(1, len(header)):
                 metrics[header[i]] = data[i]
 
         for metric_name in metrics.keys():
-            if ((len(self.config['allowed_names']) > 0 and
-                 metric_name not in self.config['allowed_names'])):
+            if (
+                len(self.config["allowed_names"]) > 0
+                and metric_name not in self.config["allowed_names"]
+            ):
                 continue
 
             value = metrics[metric_name]
-            value = self.derivative(metric_name, long(value))
+            value = self.derivative(metric_name, int(value))
 
             # Publish the metric
             self.publish(metric_name, value, 0)
```

### Comparing `diamond-next-4.0.515/src/collectors/docker_collector/docker_collector.py` & `diamond-next-5.0.0/src/collectors/docker_collector/docker_collector.py`

 * *Files 11% similar despite different names*

```diff
@@ -16,83 +16,81 @@
 try:
     import docker
 except ImportError:
     docker = None
 
 
 class DockerCollector(diamond.collector.Collector):
-
     METRICS = {
         # memory stats
         "memory_stats.stats.total_rss": "RSS_byte",
         "memory_stats.stats.total_cache": "cache_byte",
         "memory_stats.stats.total_swap": "swap_byte",
         "memory_stats.stats.total_pgpgin": "pagein_count",
         "memory_stats.stats.total_pgpgout": "pageout_count",
-
         # cpu stats
         "cpu_stats.cpu_usage.total_usage": "cpu.total",
         "cpu_stats.cpu_usage.usage_in_kernelmode": "cpu.kernelmode",
         "cpu_stats.cpu_usage.usage_in_usermode": "cpu.usermode",
         "cpu_stats.system_cpu_usage": "cpu.system",
     }
 
     def get_default_config_help(self):
         return super(DockerCollector, self).get_default_config_help()
 
     def get_default_config(self):
         config = super(DockerCollector, self).get_default_config()
-        config.update({
-            'path': 'docker'
-        })
+        config.update({"path": "docker"})
         return config
 
     def get_value(self, path, dictionary):
         keys = path.split(".")
         cur = dictionary
         for key in keys:
             if not isinstance(cur, dict):
-                raise Exception("metric '{0}' does not exist".format(path))
+                raise Exception("metric '{}' does not exist".format(path))
             cur = cur.get(key)
             if cur is None:
                 break
         return cur
 
     def collect(self):
         if docker is None:
-            self.log.error('Unable to import docker')
+            self.log.error("Unable to import docker")
 
         # Collect info
         results = {}
-        client = docker.Client(version='auto')
+        client = docker.Client(version="auto")
 
         # Top level stats
         running_containers = client.containers()
-        results['containers_running_count'] = (
-            len(running_containers), 'GAUGE')
+        results["containers_running_count"] = (len(running_containers), "GAUGE")
 
         all_containers = client.containers(all=True)
-        results['containers_stopped_count'] = (
-            len(all_containers) - len(running_containers), 'GAUGE')
+        results["containers_stopped_count"] = (
+            len(all_containers) - len(running_containers),
+            "GAUGE",
+        )
 
         images_count = len(set(client.images(quiet=True)))
-        results['images_count'] = (images_count, 'GAUGE')
+        results["images_count"] = (images_count, "GAUGE")
 
-        dangling_images_count = len(set(client.images(
-            quiet=True, all=True, filters={'dangling': True})))
-        results['images_dangling_count'] = (dangling_images_count, 'GAUGE')
+        dangling_images_count = len(
+            set(client.images(quiet=True, all=True, filters={"dangling": True}))
+        )
+        results["images_dangling_count"] = (dangling_images_count, "GAUGE")
 
         # Collect memory and cpu stats
         for container in running_containers:
-            name = "containers." + "".join(container['Names'][0][1:])
+            name = "containers." + "".join(container["Names"][0][1:])
             s = client.stats(container["Id"])
             stat = json.loads(s.next())
             for path in self.METRICS:
                 val = self.get_value(path, stat)
                 if val is not None:
                     metric_key = ".".join([name, self.METRICS.get(path)])
-                    results[metric_key] = (val, 'GAUGE')
+                    results[metric_key] = (val, "GAUGE")
             s.close()
 
         for name in sorted(results.keys()):
             (value, metric_type) = results[name]
             self.publish(name, value, metric_type=metric_type)
```

### Comparing `diamond-next-4.0.515/src/collectors/supervisord/supervisord.py` & `diamond-next-5.0.0/src/collectors/supervisord/supervisord.py`

 * *Files 25% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 (github.com/Supervisor/supervisor)
 
 Supervisor runs an XML-RPC server, which this collector uses to gather a few
 basic stats on each registered process.
 
 #### Dependencies
 
- * xmlrpclib
+ * xmlrpc
  * supervisor
  * diamond
 
 #### Usage
 
 Configure supervisor's XML-RPC server (either over HTTP or Unix socket). See
 supervisord.org/configuration.html for details. In the collector configuration
@@ -23,83 +23,80 @@
 <pre>
 xmlrpc_server_protocol = unix
 xmlrpc_server_path = /var/run/supervisor.sock
 </pre>
 
 """
 
-import xmlrpclib
+import xmlrpc.client
+
+import diamond.collector
 
 try:
     import supervisor.xmlrpc
 except ImportError:
     supervisor = None
 
-import diamond.collector
-
 
 class SupervisordCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(SupervisordCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'xmlrpc_server_protocol': 'XML-RPC server protocol. Options: unix, http',  # NOQA
-            'xmlrpc_server_path': 'XML-RPC server path.'
-        })
+        config_help = super(SupervisordCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "xmlrpc_server_protocol": "XML-RPC server protocol. Options: unix, http",  # NOQA
+                "xmlrpc_server_path": "XML-RPC server path.",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         default_config = super(SupervisordCollector, self).get_default_config()
-        default_config['path'] = 'supervisor'
-        default_config['xmlrpc_server_protocol'] = 'unix'
-        default_config['xmlrpc_server_path'] = '/var/run/supervisor.sock'
-        return default_config
+        default_config["path"] = "supervisor"
+        default_config["xmlrpc_server_protocol"] = "unix"
+        default_config["xmlrpc_server_path"] = "/var/run/supervisor.sock"
 
-    def getAllProcessInfo(self):
+        return default_config
 
+    def get_all_process_info(self):
         server = None
-        protocol = self.config['xmlrpc_server_protocol']
-        path = self.config['xmlrpc_server_path']
-        uri = '{0}://{1}'.format(protocol, path)
-
-        self.log.debug(
-            'Attempting to connect to XML-RPC server "%s"', uri)
-
-        if protocol == 'unix':
-            server = xmlrpclib.ServerProxy(
-                'http://127.0.0.1',
-                supervisor.xmlrpc.SupervisorTransport(None, None, uri)
+        protocol = self.config["xmlrpc_server_protocol"]
+        path = self.config["xmlrpc_server_path"]
+        uri = "{}://{}".format(protocol, path)
+
+        self.log.debug('Attempting to connect to XML-RPC server "%s"', uri)
+
+        if protocol == "unix":
+            server = xmlrpc.client.ServerProxy(
+                "http://127.0.0.1",
+                supervisor.xmlrpc.SupervisorTransport(None, None, uri),
             ).supervisor
-
-        elif protocol == 'http':
-            server = xmlrpclib.Server(uri).supervisor
-
+        elif protocol == "http":
+            server = xmlrpc.client.Server(uri).supervisor
         else:
             self.log.debug(
-                'Invalid xmlrpc_server_protocol config setting "%s"',
-                protocol)
+                'Invalid xmlrpc_server_protocol config setting "%s"', protocol
+            )
+
             return None
 
-        return server.getAllProcessInfo()
+        return server.get_all_process_info()
 
     def collect(self):
+        processes = self.get_all_process_info()
 
-        processes = self.getAllProcessInfo()
-
-        self.log.debug('Found %s supervisord processes', len(processes))
+        self.log.debug("Found %s supervisord processes", len(processes))
 
         for process in processes:
-            statPrefix = "%s.%s" % (process["group"], process["name"])
+            stat_prefix = "%s.%s" % (process["group"], process["name"])
 
             # state
 
-            self.publish(statPrefix + ".state", process["state"])
+            self.publish(stat_prefix + ".state", process["state"])
 
             # uptime
 
             uptime = 0
 
             if process["statename"] == "RUNNING":
                 uptime = process["now"] - process["start"]
 
-            self.publish(statPrefix + ".uptime", uptime)
+            self.publish(stat_prefix + ".uptime", uptime)
```

### Comparing `diamond-next-4.0.515/src/collectors/slony/slony.py` & `diamond-next-5.0.0/src/collectors/slony/slony.py`

 * *Files 15% similar despite different names*

```diff
@@ -34,112 +34,121 @@
 """
 
 import diamond.collector
 
 try:
     import psycopg2
     import psycopg2.extensions
+
     psycopg2  # workaround for pyflakes issue #13
 except ImportError:
     psycopg2 = None
 
 
 class SlonyCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(SlonyCollector, self).get_default_config_help()
-        config_help.update({
-            'host': 'Hostname',
-            'user': 'Username',
-            'password': 'Password',
-            'port': 'Port number',
-            'slony_node_string': 'Regex for SQL SUBSTRING to extract ' +
-                                 'the hostname from sl_node.no_comment',
-            'instances': 'Subcategory of slony instances that includes the ' +
-                         'slony database, and slony schema to be monitored. ' +
-                         'Optionally, user, password and slony_node_string ' +
-                         'maybe overridden per instance (see example).'
-        })
+        config_help.update(
+            {
+                "host": "Hostname",
+                "user": "Username",
+                "password": "Password",
+                "port": "Port number",
+                "slony_node_string": "Regex for SQL SUBSTRING to extract "
+                + "the hostname from sl_node.no_comment",
+                "instances": "Subcategory of slony instances that includes the "
+                + "slony database, and slony schema to be monitored. "
+                + "Optionally, user, password and slony_node_string "
+                + "maybe overridden per instance (see example).",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Return default config.
         """
         config = super(SlonyCollector, self).get_default_config()
-        config.update({
-            'path': 'postgres',
-            'host': 'localhost',
-            'user': 'postgres',
-            'password': 'postgres',
-            'port': 5432,
-            'slony_node_string': 'Node [0-9]+ - postgres@localhost',
-            'method': 'Threaded',
-            'instances': {},
-        })
+        config.update(
+            {
+                "path": "postgres",
+                "host": "localhost",
+                "user": "postgres",
+                "password": "postgres",
+                "port": 5432,
+                "slony_node_string": "Node [0-9]+ - postgres@localhost",
+                "method": "Threaded",
+                "instances": {},
+            }
+        )
         return config
 
     def collect(self):
         if psycopg2 is None:
-            self.log.error('Unable to import module psycopg2')
+            self.log.error("Unable to import module psycopg2")
             return {}
 
-        instances = self.config['instances']
+        instances = self.config["instances"]
+
         # HACK: setting default with subcategory messes up merging of configs,
         # so we only set the default if one wasn't provided.
         if not instances:
             instances = {
-                'default': {
-                    'slony_db': 'postgres',
-                    'slony_schema': '_postgres',
+                "default": {
+                    "slony_db": "postgres",
+                    "slony_schema": "_postgres",
                 }
             }
 
-        for name, instance in instances.iteritems():
-            host = self.config['host']
-            port = self.config['port']
-            user = instance.get('user') or self.config['user']
-            password = instance.get('password') or self.config['password']
-            slony_node_string = instance.get('slony_node_string') or \
-                self.config['slony_node_string']
-            slony_db = instance['slony_db']
-            slony_schema = instance['slony_schema']
+        for name, instance in iter(instances.items()):
+            host = self.config["host"]
+            port = self.config["port"]
+            user = instance.get("user") or self.config["user"]
+            password = instance.get("password") or self.config["password"]
+            slony_node_string = (
+                instance.get("slony_node_string") or self.config["slony_node_string"]
+            )
+            slony_db = instance["slony_db"]
+            slony_schema = instance["slony_schema"]
 
             stats = self._get_stats_by_database(
-                host, port, user, password, slony_db,
-                slony_schema, slony_node_string
+                host, port, user, password, slony_db, slony_schema, slony_node_string
             )
             [self.publish(metric, value) for metric, value in stats]
 
-    def _get_stats_by_database(self, host, port, user,
-                               password, db, schema, node_string):
+    def _get_stats_by_database(
+        self, host, port, user, password, db, schema, node_string
+    ):
         path = "slony.%(datname)s.%(metric)s.lag_events"
         conn = psycopg2.connect(
-            host=host,
-            user=user,
-            password=password,
-            port=port,
-            database=db)
+            host=host, user=user, password=password, port=port, database=db
+        )
 
         # Avoid using transactions, set isolation level to autocommit
         conn.set_isolation_level(0)
 
         query = """
-            SELECT SUBSTRING(sl.no_comment FROM %(node_extractor)s) AS node,
-                   st.st_lag_num_events AS lag_events
+            SELECT SUBSTRING(sl.no_comment FROM %(node_extractor)s) AS node, st.st_lag_num_events AS lag_events
             FROM %(schema)s.sl_status AS st, %(schema)s.sl_node AS sl
             WHERE sl.no_id = st.st_received
         """
         cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
-        cursor.execute(query, {
-            'node_extractor': node_string,
-            'schema': psycopg2.extensions.AsIs(schema),
-        })
+        cursor.execute(
+            query,
+            {
+                "node_extractor": node_string,
+                "schema": psycopg2.extensions.AsIs(schema),
+            },
+        )
 
         metrics = []
+
         for row in cursor.fetchall():
             stats = row.copy()
-            metrics.append((
-                path % {'datname': db, 'metric': stats.get('node')},
-                stats.get('lag_events')
-            ))
+            metrics.append(
+                (
+                    path % {"datname": db, "metric": stats.get("node")},
+                    stats.get("lag_events"),
+                )
+            )
+
         return metrics
```

### Comparing `diamond-next-4.0.515/src/collectors/memcached/memcached.py` & `diamond-next-5.0.0/src/collectors/memcached/memcached.py`

 * *Files 11% similar despite different names*

```diff
@@ -21,153 +21,174 @@
 TO use a unix socket, set a host string like this
 
 ```
     hosts = /path/to/blah.sock, app-1@/path/to/bleh.sock,
 ```
 """
 
-import diamond.collector
-import socket
 import re
+import socket
+
+import diamond.collector
 
 
 class MemcachedCollector(diamond.collector.Collector):
     GAUGES = [
-        'bytes',
-        'connection_structures',
-        'curr_connections',
-        'curr_items',
-        'threads',
-        'reserved_fds',
-        'limit_maxbytes',
-        'hash_power_level',
-        'hash_bytes',
-        'hash_is_expanding',
-        'uptime'
+        "bytes",
+        "connection_structures",
+        "curr_connections",
+        "curr_items",
+        "threads",
+        "reserved_fds",
+        "limit_maxbytes",
+        "hash_power_level",
+        "hash_bytes",
+        "hash_is_expanding",
+        "uptime",
     ]
 
     def get_default_config_help(self):
         config_help = super(MemcachedCollector, self).get_default_config_help()
-        config_help.update({
-            'publish':
-                "Which rows of 'status' you would like to publish." +
-                " Telnet host port' and type stats and hit enter to see the" +
-                " list of possibilities. Leave unset to publish all.",
-            'hosts':
-                "List of hosts, and ports to collect. Set an alias by " +
-                " prefixing the host:port with alias@",
-        })
+        config_help.update(
+            {
+                "publish": "Which rows of 'status' you would like to publish."
+                + " Telnet host port' and type stats and hit enter to see the"
+                + " list of possibilities. Leave unset to publish all.",
+                "hosts": "List of hosts, and ports to collect. Set an alias by "
+                + " prefixing the host:port with alias@",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(MemcachedCollector, self).get_default_config()
-        config.update({
-            'path':     'memcached',
-
-            # Which rows of 'status' you would like to publish.
-            # 'telnet host port' and type stats and hit enter to see the list of
-            # possibilities.
-            # Leave unset to publish all
-            # 'publish': ''
-
-            # Connection settings
-            'hosts': ['localhost:11211']
-        })
+        config.update(
+            {
+                "path": "memcached",
+                # Which rows of 'status' you would like to publish.
+                # 'telnet host port' and type stats and hit enter to see the list of
+                # possibilities.
+                # Leave unset to publish all
+                # 'publish': ''
+                # Connection settings
+                "hosts": ["localhost:11211"],
+            }
+        )
         return config
 
     def get_raw_stats(self, host, port):
-        data = ''
+        data = ""
         # connect
         try:
             if port is None:
                 sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                 sock.connect(host)
             else:
                 sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                 sock.connect((host, int(port)))
+
+            # give up after a reasonable amount of time
+            sock.settimeout(3)
+
             # request stats
-            sock.send('stats\n')
-            # something big enough to get whatever is sent back
-            data = sock.recv(4096)
+            sock.send("stats\n")
+
+            # stats can be sent across multiple packets, so make sure we've
+            # read up until the END marker
+            while True:
+                received = sock.recv(4096)
+                if not received:
+                    break
+                data += received
+                if data.endswith("END\r\n"):
+                    break
         except socket.error:
-            self.log.exception('Failed to get stats from %s:%s',
-                               host, port)
+            self.log.exception("Failed to get stats from %s:%s", host, port)
+
+        sock.close()
+
         return data
 
     def get_stats(self, host, port):
         # stuff that's always ignored, aren't 'stats'
-        ignored = ('libevent', 'pointer_size', 'time', 'version',
-                   'repcached_version', 'replication', 'accepting_conns',
-                   'pid')
+        ignored = (
+            "libevent",
+            "pointer_size",
+            "time",
+            "version",
+            "repcached_version",
+            "replication",
+            "accepting_conns",
+            "pid",
+        )
         pid = None
 
         stats = {}
         data = self.get_raw_stats(host, port)
 
         # parse stats
         for line in data.splitlines():
-            pieces = line.split(' ')
-            if pieces[0] != 'STAT' or pieces[1] in ignored:
+            pieces = line.split(" ")
+            if pieces[0] != "STAT" or pieces[1] in ignored:
                 continue
-            elif pieces[1] == 'pid':
+            elif pieces[1] == "pid":
                 pid = pieces[2]
                 continue
-            if '.' in pieces[2]:
+            if "." in pieces[2]:
                 stats[pieces[1]] = float(pieces[2])
             else:
                 stats[pieces[1]] = int(pieces[2])
 
         # get max connection limit
-        self.log.debug('pid %s', pid)
+        self.log.debug("pid %s", pid)
         try:
             cmdline = "/proc/%s/cmdline" % pid
-            f = open(cmdline, 'r')
+            f = open(cmdline, "r")
             m = re.search("-c\x00(\d+)", f.readline())
             if m is not None:
-                self.log.debug('limit connections %s', m.group(1))
-                stats['limit_maxconn'] = m.group(1)
+                self.log.debug("limit connections %s", m.group(1))
+                stats["limit_maxconn"] = m.group(1)
             f.close()
         except:
             self.log.debug("Cannot parse command line options for memcached")
 
         return stats
 
     def collect(self):
-        hosts = self.config.get('hosts')
+        hosts = self.config.get("hosts")
 
         # Convert a string config value to be an array
-        if isinstance(hosts, basestring):
+        if isinstance(hosts, str):
             hosts = [hosts]
 
         for host in hosts:
-            matches = re.search('((.+)\@)?([^:]+)(:(\d+))?', host)
+            matches = re.search("((.+)\@)?([^:]+)(:(\d+))?", host)
             alias = matches.group(2)
             hostname = matches.group(3)
             port = matches.group(5)
 
             if alias is None:
                 alias = hostname
 
             stats = self.get_stats(hostname, port)
 
             # figure out what we're configured to get, defaulting to everything
-            desired = self.config.get('publish', stats.keys())
+            desired = self.config.get("publish", stats.keys())
 
             # for everything we want
             for stat in desired:
                 if stat in stats:
-
                     # we have it
                     if stat in self.GAUGES:
                         self.publish_gauge(alias + "." + stat, stats[stat])
                     else:
                         self.publish_counter(alias + "." + stat, stats[stat])
-
                 else:
-
                     # we don't, must be somehting configured in publish so we
                     # should log an error about it
-                    self.log.error("No such key '%s' available, issue 'stats' "
-                                   "for a full list", stat)
+                    self.log.error(
+                        "No such key '%s' available, issue 'stats' " "for a full list",
+                        stat,
+                    )
```

### Comparing `diamond-next-4.0.515/src/collectors/postgres/postgres.py` & `diamond-next-5.0.0/src/collectors/postgres/postgres.py`

 * *Files 11% similar despite different names*

```diff
@@ -6,15 +6,14 @@
 #### Dependencies
 
  * psycopg2
 
 """
 
 import diamond.collector
-from diamond.collector import str_to_bool
 
 try:
     import psycopg2
     import psycopg2.extras
 except ImportError:
     psycopg2 = None
 
@@ -24,96 +23,104 @@
     PostgreSQL collector class
     """
 
     def get_default_config_help(self):
         """
         Return help text for collector
         """
-        config_help = super(PostgresqlCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': 'Hostname',
-            'dbname': 'DB to connect to in order to get list of DBs in PgSQL',
-            'user': 'Username',
-            'password': 'Password',
-            'port': 'Port number',
-            'password_provider': "Whether to auth with supplied password or"
-            " .pgpass file  <password|pgpass>",
-            'sslmode': 'Whether to use SSL - <disable|allow|require|...>',
-            'underscore': 'Convert _ to .',
-            'extended': 'Enable collection of extended database stats.',
-            'metrics': 'List of enabled metrics to collect',
-            'pg_version': "The version of postgres that you'll be monitoring"
-            " eg. in format 9.2",
-            'has_admin': 'Admin privileges are required to execute some'
-            ' queries.',
-        })
+        config_help = super(PostgresqlCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "Hostname",
+                "dbname": "DB to connect to in order to get list of DBs in PgSQL",
+                "user": "Username",
+                "password": "Password",
+                "port": "Port number",
+                "password_provider": "Whether to auth with supplied password or"
+                " .pgpass file  <password|pgpass>",
+                "sslmode": "Whether to use SSL - <disable|allow|require|...>",
+                "underscore": "Convert _ to .",
+                "extended": "Enable collection of extended database stats.",
+                "metrics": "List of enabled metrics to collect",
+                "pg_version": "The version of postgres that you'll be monitoring"
+                " eg. in format 9.2",
+                "has_admin": "Admin privileges are required to execute some"
+                " queries.",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Return default config.
         """
         config = super(PostgresqlCollector, self).get_default_config()
-        config.update({
-            'path': 'postgres',
-            'host': 'localhost',
-            'dbname': 'postgres',
-            'user': 'postgres',
-            'password': 'postgres',
-            'port': 5432,
-            'password_provider': 'password',
-            'sslmode': 'disable',
-            'underscore': False,
-            'extended': False,
-            'metrics': [],
-            'pg_version': 9.2,
-            'has_admin': True,
-        })
+        config.update(
+            {
+                "path": "postgres",
+                "host": "localhost",
+                "dbname": "postgres",
+                "user": "postgres",
+                "password": "postgres",
+                "port": 5432,
+                "password_provider": "password",
+                "sslmode": "disable",
+                "underscore": False,
+                "extended": False,
+                "metrics": [],
+                "pg_version": 9.2,
+                "has_admin": True,
+            }
+        )
         return config
 
     def collect(self):
         """
         Do pre-flight checks, get list of db names, collect metrics, publish
         """
         if psycopg2 is None:
-            self.log.error('Unable to import module psycopg2')
+            self.log.error("Unable to import module psycopg2")
             return {}
 
         # Get list of databases
         dbs = self._get_db_names()
+
         if len(dbs) == 0:
             self.log.error("I have 0 databases!")
+
             return {}
 
-        if self.config['metrics']:
-            metrics = self.config['metrics']
-        elif str_to_bool(self.config['extended']):
-            metrics = registry['extended']
-            if str_to_bool(self.config['has_admin']) \
-                    and 'WalSegmentStats' not in metrics:
-                metrics.append('WalSegmentStats')
+        if self.config["metrics"]:
+            metrics = self.config["metrics"]
+        elif diamond.collector.str_to_bool(self.config["extended"]):
+            metrics = registry["extended"]
+
+            if (
+                diamond.collector.str_to_bool(self.config["has_admin"])
+                and "WalSegmentStats" not in metrics
+            ):
+                metrics.append("WalSegmentStats")
 
         else:
-            metrics = registry['basic']
+            metrics = registry["basic"]
 
         # Iterate every QueryStats class
         for metric_name in set(metrics):
             if metric_name not in metrics_registry:
                 self.log.error(
-                    'metric_name %s not found in metric registry' % metric_name)
+                    "metric_name %s not found in metric registry" % metric_name
+                )
                 continue
 
             for dbase in dbs:
                 conn = self._connect(database=dbase)
                 try:
                     klass = metrics_registry[metric_name]
-                    stat = klass(dbase, conn,
-                                 underscore=self.config['underscore'])
-                    stat.fetch(self.config['pg_version'])
+                    stat = klass(dbase, conn, underscore=self.config["underscore"])
+                    stat.fetch(self.config["pg_version"])
                     for metric, value in stat:
                         if value is not None:
                             self.publish(metric, value)
 
                     # Setting multi_db to True will run this query on all known
                     # databases. This is bad for queries that hit views like
                     # pg_database, which are shared across databases.
@@ -130,51 +137,51 @@
         Try to get a list of db names
         """
         query = """
             SELECT datname FROM pg_database
             WHERE datallowconn AND NOT datistemplate
             AND NOT datname='postgres' AND NOT datname='rdsadmin' ORDER BY 1
         """
-        conn = self._connect(self.config['dbname'])
+        conn = self._connect(self.config["dbname"])
         cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
         cursor.execute(query)
-        datnames = [d['datname'] for d in cursor.fetchall()]
+        datnames = [d["datname"] for d in cursor.fetchall()]
         conn.close()
 
         # Exclude `postgres` database list, unless it is the
         # only database available (required for querying pg_stat_database)
         if not datnames:
-            datnames = ['postgres']
+            datnames = ["postgres"]
 
         return datnames
 
     def _connect(self, database=None):
         """
         Connect to given database
         """
         conn_args = {
-            'host': self.config['host'],
-            'user': self.config['user'],
-            'password': self.config['password'],
-            'port': self.config['port'],
-            'sslmode': self.config['sslmode'],
+            "host": self.config["host"],
+            "user": self.config["user"],
+            "password": self.config["password"],
+            "port": self.config["port"],
+            "sslmode": self.config["sslmode"],
         }
 
         if database:
-            conn_args['database'] = database
+            conn_args["database"] = database
         else:
-            conn_args['database'] = 'postgres'
+            conn_args["database"] = "postgres"
 
         # libpq will use ~/.pgpass only if no password supplied
-        if self.config['password_provider'] == 'pgpass':
-            del conn_args['password']
+        if self.config["password_provider"] == "pgpass":
+            del conn_args["password"]
 
         try:
             conn = psycopg2.connect(**conn_args)
-        except Exception, e:
+        except Exception as e:
             self.log.error(e)
             raise e
 
         # Avoid using transactions, set isolation level to autocommit
         conn.set_isolation_level(0)
         return conn
 
@@ -195,64 +202,79 @@
         Replace '_' with '.'
         """
         if self.underscore:
             datname = datname.replace("_", ".")
         return datname
 
     def fetch(self, pg_version):
-        if float(pg_version) >= 9.2 and hasattr(self, 'post_92_query'):
+        if float(pg_version) >= 10.0 and hasattr(self, "post_100_query"):
+            q = self.post_100_query
+        elif float(pg_version) >= 9.6 and hasattr(self, "post_96_query"):
+            q = self.post_96_query
+        elif float(pg_version) >= 9.2 and hasattr(self, "post_92_query"):
             q = self.post_92_query
         else:
             q = self.query
 
         cursor = self.conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
         try:
             cursor.execute(q, self.parameters)
             rows = cursor.fetchall()
             for row in rows:
                 # If row is length 2, assume col1, col2 forms key: value
                 if len(row) == 2:
-                    self.data.append({
-                        'datname': self._translate_datname(self.dbname),
-                        'metric': row[0],
-                        'value': row[1],
-                    })
+                    self.data.append(
+                        {
+                            "datname": self._translate_datname(self.dbname),
+                            "metric": row[0],
+                            "value": row[1],
+                        }
+                    )
 
                 # If row > length 2, assume each column name maps to
                 # key => value
                 else:
-                    for key, value in row.iteritems():
-                        if key in ('datname', 'schemaname', 'relname',
-                                   'indexrelname', 'funcname',):
+                    for key, value in iter(row.items()):
+                        if key in (
+                            "datname",
+                            "schemaname",
+                            "relname",
+                            "indexrelname",
+                            "funcname",
+                        ):
                             continue
 
-                        self.data.append({
-                            'datname': self._translate_datname(row.get(
-                                'datname', self.dbname)),
-                            'schemaname': row.get('schemaname', None),
-                            'relname': row.get('relname', None),
-                            'indexrelname': row.get('indexrelname', None),
-                            'funcname': row.get('funcname', None),
-                            'metric': key,
-                            'value': value,
-                        })
+                        self.data.append(
+                            {
+                                "datname": self._translate_datname(
+                                    row.get("datname", self.dbname)
+                                ),
+                                "schemaname": row.get("schemaname", None),
+                                "relname": row.get("relname", None),
+                                "indexrelname": row.get("indexrelname", None),
+                                "funcname": row.get("funcname", None),
+                                "metric": key,
+                                "value": value,
+                            }
+                        )
 
         # Clean up
         finally:
             cursor.close()
 
     def __iter__(self):
         for data_point in self.data:
-            yield (self.path % data_point, data_point['value'])
+            yield self.path % data_point, data_point["value"]
 
 
 class DatabaseStats(QueryStats):
     """
     Database-level summary stats
     """
+
     path = "database.%(datname)s.%(metric)s"
     multi_db = False
     post_92_query = """
         SELECT pg_stat_database.datname as datname,
                pg_stat_database.numbackends as numbackends,
                pg_stat_database.xact_commit as xact_commit,
                pg_stat_database.xact_rollback as xact_rollback,
@@ -267,18 +289,16 @@
         FROM pg_database
         JOIN pg_stat_database
         ON pg_database.datname = pg_stat_database.datname
         WHERE pg_stat_database.datname
         NOT IN ('template0','template1','postgres', 'rdsadmin')
     """
     query = post_92_query.replace(
-        'pg_stat_database.temp_files as temp_files,',
-        '').replace(
-        'pg_stat_database.temp_bytes as temp_bytes,',
-        '')
+        "pg_stat_database.temp_files as temp_files,", ""
+    ).replace("pg_stat_database.temp_bytes as temp_bytes,", "")
 
 
 class UserFunctionStats(QueryStats):
     # http://www.pateldenish.com/2010/11/postgresql-track-functions-to-tune.html
     path = "%(datname)s.functions.%(funcname)s.%(metric)s"
     multi_db = True
     query = """
@@ -307,16 +327,17 @@
                n_live_tup,
                n_dead_tup
         FROM pg_stat_user_tables
     """
 
 
 class UserIndexStats(QueryStats):
-    path = "%(datname)s.indexes.%(schemaname)s.%(relname)s." \
-           "%(indexrelname)s.%(metric)s"
+    path = (
+        "%(datname)s.indexes.%(schemaname)s.%(relname)s." "%(indexrelname)s.%(metric)s"
+    )
     multi_db = True
     query = """
         SELECT relname,
                schemaname,
                indexrelname,
                idx_scan,
                idx_tup_read,
@@ -336,16 +357,17 @@
                idx_blks_read,
                idx_blks_hit
         FROM pg_statio_user_tables
     """
 
 
 class UserIndexIOStats(QueryStats):
-    path = "%(datname)s.indexes.%(schemaname)s.%(relname)s." \
-           "%(indexrelname)s.%(metric)s"
+    path = (
+        "%(datname)s.indexes.%(schemaname)s.%(relname)s." "%(indexrelname)s.%(metric)s"
+    )
     multi_db = True
     query = """
         SELECT relname,
                schemaname,
                indexrelname,
                idx_blks_read,
                idx_blks_hit
@@ -414,14 +436,41 @@
                              WHEN query = '<insufficient privilege>'
                                  THEN 'unknown'
                              ELSE 'active'
                         END
              ) AS tmp2
         ON tmp.mstate=tmp2.mstate ORDER BY 1
     """
+    post_96_query = """
+        SELECT tmp.state AS key,COALESCE(count,0) FROM
+               (VALUES ('active'),
+                       ('waiting'),
+                       ('idle'),
+                       ('idletransaction'),
+                       ('unknown')
+                ) AS tmp(state)
+        LEFT JOIN
+             (SELECT CASE WHEN wait_event IS NOT NULL THEN 'waiting'
+                          WHEN state= 'idle' THEN 'idle'
+                          WHEN state= 'idle in transaction'
+                          THEN 'idletransaction'
+                          WHEN state = 'active' THEN 'active'
+                          ELSE 'unknown' END AS state,
+                     count(*) AS count
+               FROM pg_stat_activity
+               WHERE pid != pg_backend_pid()
+               GROUP BY CASE WHEN wait_event IS NOT NULL THEN 'waiting'
+                          WHEN state= 'idle' THEN 'idle'
+                          WHEN state= 'idle in transaction'
+                          THEN 'idletransaction'
+                          WHEN state = 'active' THEN 'active'
+                          ELSE 'unknown' END
+             ) AS tmp2
+        ON tmp.state=tmp2.state ORDER BY 1
+    """
 
 
 class LockStats(QueryStats):
     path = "%(datname)s.locks.%(metric)s"
     multi_db = False
     query = """
         SELECT lower(mode) AS key,
@@ -463,18 +512,23 @@
         FROM pg_stat_bgwriter
     """
 
 
 class WalSegmentStats(QueryStats):
     path = "wals.%(metric)s"
     multi_db = False
+    post_100_query = """
+        SELECT count(*) AS segments
+        FROM pg_ls_dir('pg_wal') t(fn)
+        WHERE fn ~ '^[0-9A-Z]{24}$'
+    """
     query = """
         SELECT count(*) AS segments
         FROM pg_ls_dir('pg_xlog') t(fn)
-        WHERE fn ~ '^[0-9A-Z]{{24}}\$'
+        WHERE fn ~ '^[0-9A-Z]{24}$'
     """
 
 
 class TransactionCount(QueryStats):
     path = "transactions.%(metric)s"
     multi_db = False
     query = """
@@ -495,16 +549,16 @@
         SELECT 'idle_in_transactions',
                max(COALESCE(ROUND(EXTRACT(epoch FROM now()-query_start)),0))
                    AS idle_in_transaction
         FROM pg_stat_activity
         WHERE %s
         GROUP BY 1
     """
-    query = base_query % ("current_query = '<IDLE> in transaction'", )
-    post_92_query = base_query % ("state LIKE 'idle in transaction%'", )
+    query = base_query % ("current_query = '<IDLE> in transaction'",)
+    post_92_query = base_query % ("state LIKE 'idle in transaction%'",)
 
 
 class LongestRunningQueries(QueryStats):
     path = "%(datname)s.longest_running.%(metric)s"
     multi_db = True
     base_query = """
         SELECT 'query',
@@ -513,30 +567,30 @@
         WHERE %s
         UNION ALL
         SELECT 'transaction',
             COALESCE(max(extract(epoch FROM CURRENT_TIMESTAMP-xact_start)),0)
         FROM pg_stat_activity
         WHERE 1=1
     """
-    query = base_query % ("current_query NOT LIKE '<IDLE%'", )
-    post_92_query = base_query % ("state NOT LIKE 'idle%'", )
+    query = base_query % ("current_query NOT LIKE '<IDLE%'",)
+    post_92_query = base_query % ("state NOT LIKE 'idle%'",)
 
 
 class UserConnectionCount(QueryStats):
     path = "%(datname)s.user_connections.%(metric)s"
     multi_db = True
     query = """
         SELECT usename,
                count(*) as count
         FROM pg_stat_activity
         WHERE procpid != pg_backend_pid()
         GROUP BY usename
         ORDER BY 1
     """
-    post_92_query = query.replace('procpid', 'pid')
+    post_92_query = query.replace("procpid", "pid")
 
 
 class DatabaseConnectionCount(QueryStats):
     path = "database.%(metric)s.connections"
     multi_db = False
     query = """
         SELECT datname,
@@ -586,56 +640,56 @@
     query = """
         SELECT datname, age(datfrozenxid) AS age
         FROM pg_database WHERE datallowconn = TRUE
     """
 
 
 metrics_registry = {
-    'DatabaseStats': DatabaseStats,
-    'DatabaseConnectionCount': DatabaseConnectionCount,
-    'UserFunctionStats': UserFunctionStats,
-    'UserTableStats': UserTableStats,
-    'UserIndexStats': UserIndexStats,
-    'UserTableIOStats': UserTableIOStats,
-    'UserIndexIOStats': UserIndexIOStats,
-    'ConnectionStateStats': ConnectionStateStats,
-    'LockStats': LockStats,
-    'RelationSizeStats': RelationSizeStats,
-    'BackgroundWriterStats': BackgroundWriterStats,
-    'WalSegmentStats': WalSegmentStats,
-    'TransactionCount': TransactionCount,
-    'IdleInTransactions': IdleInTransactions,
-    'LongestRunningQueries': LongestRunningQueries,
-    'UserConnectionCount': UserConnectionCount,
-    'TableScanStats': TableScanStats,
-    'TupleAccessStats': TupleAccessStats,
-    'DatabaseReplicationStats': DatabaseReplicationStats,
-    'DatabaseXidAge': DatabaseXidAge,
+    "DatabaseStats": DatabaseStats,
+    "DatabaseConnectionCount": DatabaseConnectionCount,
+    "UserFunctionStats": UserFunctionStats,
+    "UserTableStats": UserTableStats,
+    "UserIndexStats": UserIndexStats,
+    "UserTableIOStats": UserTableIOStats,
+    "UserIndexIOStats": UserIndexIOStats,
+    "ConnectionStateStats": ConnectionStateStats,
+    "LockStats": LockStats,
+    "RelationSizeStats": RelationSizeStats,
+    "BackgroundWriterStats": BackgroundWriterStats,
+    "WalSegmentStats": WalSegmentStats,
+    "TransactionCount": TransactionCount,
+    "IdleInTransactions": IdleInTransactions,
+    "LongestRunningQueries": LongestRunningQueries,
+    "UserConnectionCount": UserConnectionCount,
+    "TableScanStats": TableScanStats,
+    "TupleAccessStats": TupleAccessStats,
+    "DatabaseReplicationStats": DatabaseReplicationStats,
+    "DatabaseXidAge": DatabaseXidAge,
 }
 
 registry = {
-    'basic': [
-        'DatabaseStats',
-        'DatabaseConnectionCount',
+    "basic": [
+        "DatabaseStats",
+        "DatabaseConnectionCount",
     ],
-    'extended': [
-        'DatabaseStats',
-        'DatabaseConnectionCount',
-        'DatabaseReplicationStats',
-        'DatabaseXidAge',
-        'UserFunctionStats',
-        'UserTableStats',
-        'UserIndexStats',
-        'UserTableIOStats',
-        'UserIndexIOStats',
-        'ConnectionStateStats',
-        'LockStats',
-        'RelationSizeStats',
-        'BackgroundWriterStats',
-        'TransactionCount',
-        'IdleInTransactions',
-        'LongestRunningQueries',
-        'UserConnectionCount',
-        'TableScanStats',
-        'TupleAccessStats',
+    "extended": [
+        "DatabaseStats",
+        "DatabaseConnectionCount",
+        "DatabaseReplicationStats",
+        "DatabaseXidAge",
+        "UserFunctionStats",
+        "UserTableStats",
+        "UserIndexStats",
+        "UserTableIOStats",
+        "UserIndexIOStats",
+        "ConnectionStateStats",
+        "LockStats",
+        "RelationSizeStats",
+        "BackgroundWriterStats",
+        "TransactionCount",
+        "IdleInTransactions",
+        "LongestRunningQueries",
+        "UserConnectionCount",
+        "TableScanStats",
+        "TupleAccessStats",
     ],
 }
```

### Comparing `diamond-next-4.0.515/src/collectors/ceph/ceph.py` & `diamond-next-5.0.0/src/collectors/ceph/ceph.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,146 +1,155 @@
 # coding=utf-8
 
 """
 The CephCollector collects utilization info from the Ceph storage system.
 
 Documentation for ceph perf counters:
-http://ceph.com/docs/master/dev/perf_counters/
+http://docs.ceph.com/docs/master/dev/perf_counters/
 
 #### Dependencies
 
  * ceph [http://ceph.com/]
 
 """
 
-try:
-    import json
-except ImportError:
-    import simplejson as json
-
 import glob
 import os
 import subprocess
 
 import diamond.collector
 
+try:
+    import json
+except ImportError:
+    import simplejson as json
+
 
-def flatten_dictionary(input, sep='.', prefix=None):
+def flatten_dictionary(input, sep=".", prefix=None):
     """Produces iterator of pairs where the first value is
     the joined key names and the second value is the value
     associated with the lowest level key. For example::
 
       {'a': {'b': 10},
        'c': 20,
        }
 
     produces::
 
       [('a.b', 10), ('c', 20)]
     """
     for name, value in sorted(input.items()):
         fullname = sep.join(filter(None, [prefix, name]))
+
         if isinstance(value, dict):
             for result in flatten_dictionary(value, sep, fullname):
                 yield result
         else:
             yield (fullname, value)
 
 
 class CephCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(CephCollector, self).get_default_config_help()
-        config_help.update({
-            'socket_path': 'The location of the ceph monitoring sockets.'
-                           ' Defaults to "/var/run/ceph"',
-            'socket_prefix': 'The first part of all socket names.'
-                             ' Defaults to "ceph-"',
-            'socket_ext': 'Extension for socket filenames.'
-                          ' Defaults to "asok"',
-            'ceph_binary': 'Path to "ceph" executable. '
-                           'Defaults to /usr/bin/ceph.',
-        })
+        config_help.update(
+            {
+                "socket_path": "The location of the ceph monitoring sockets."
+                ' Defaults to "/var/run/ceph"',
+                "socket_prefix": "The first part of all socket names."
+                ' Defaults to "ceph-"',
+                "socket_ext": "Extension for socket filenames." ' Defaults to "asok"',
+                "ceph_binary": 'Path to "ceph" executable. '
+                "Defaults to /usr/bin/ceph.",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(CephCollector, self).get_default_config()
-        config.update({
-            'socket_path': '/var/run/ceph',
-            'socket_prefix': 'ceph-',
-            'socket_ext': 'asok',
-            'ceph_binary': '/usr/bin/ceph',
-        })
+        config.update(
+            {
+                "socket_path": "/var/run/ceph",
+                "socket_prefix": "ceph-",
+                "socket_ext": "asok",
+                "ceph_binary": "/usr/bin/ceph",
+            }
+        )
+
         return config
 
     def _get_socket_paths(self):
         """Return a sequence of paths to sockets for communicating
         with ceph daemons.
         """
-        socket_pattern = os.path.join(self.config['socket_path'],
-                                      (self.config['socket_prefix'] +
-                                       '*.' + self.config['socket_ext']))
+        socket_pattern = os.path.join(
+            self.config["socket_path"],
+            (self.config["socket_prefix"] + "*." + self.config["socket_ext"]),
+        )
+
         return glob.glob(socket_pattern)
 
     def _get_counter_prefix_from_socket_name(self, name):
         """Given the name of a UDS socket, return the prefix
         for counters coming from that source.
         """
         base = os.path.splitext(os.path.basename(name))[0]
-        if base.startswith(self.config['socket_prefix']):
-            base = base[len(self.config['socket_prefix']):]
-        return 'ceph.' + base
+
+        if base.startswith(self.config["socket_prefix"]):
+            base = base[len(self.config["socket_prefix"]) :]
+
+        return "ceph." + base
 
     def _get_stats_from_socket(self, name):
         """Return the parsed JSON data returned when ceph is told to
         dump the stats from the named socket.
 
         In the event of an error error, the exception is logged, and
         an empty result set is returned.
         """
         try:
             json_blob = subprocess.check_output(
-                [self.config['ceph_binary'],
-                 '--admin-daemon',
-                 name,
-                 'perf',
-                 'dump',
-                 ])
-        except subprocess.CalledProcessError, err:
-            self.log.info('Could not get stats from %s: %s',
-                          name, err)
-            self.log.exception('Could not get stats from %s' % name)
+                [
+                    self.config["ceph_binary"],
+                    "--admin-daemon",
+                    name,
+                    "perf",
+                    "dump",
+                ]
+            )
+        except subprocess.CalledProcessError as err:
+            self.log.info("Could not get stats from %s: %s", name, err)
+            self.log.exception("Could not get stats from %s" % name)
+
             return {}
 
         try:
             json_data = json.loads(json_blob)
-        except Exception, err:
-            self.log.info('Could not parse stats from %s: %s',
-                          name, err)
-            self.log.exception('Could not parse stats from %s' % name)
+        except Exception as err:
+            self.log.info("Could not parse stats from %s: %s", name, err)
+            self.log.exception("Could not parse stats from %s" % name)
+
             return {}
 
         return json_data
 
     def _publish_stats(self, counter_prefix, stats):
         """Given a stats dictionary from _get_stats_from_socket,
         publish the individual values.
         """
-        for stat_name, stat_value in flatten_dictionary(
-            stats,
-            prefix=counter_prefix,
-        ):
+        for stat_name, stat_value in flatten_dictionary(stats, prefix=counter_prefix):
             self.publish_gauge(stat_name, stat_value)
 
     def collect(self):
         """
         Collect stats
         """
         for path in self._get_socket_paths():
-            self.log.debug('checking %s', path)
+            self.log.debug("checking %s", path)
             counter_prefix = self._get_counter_prefix_from_socket_name(path)
             stats = self._get_stats_from_socket(path)
             self._publish_stats(counter_prefix, stats)
+
         return
```

### Comparing `diamond-next-4.0.515/src/collectors/filestat/filestat.py` & `diamond-next-5.0.0/src/collectors/filestat/filestat.py`

 * *Files 10% similar despite different names*

```diff
@@ -30,238 +30,275 @@
 #### Dependencies
 
  * /proc/sys/fs/file-nr
  * /usr/sbin/lsof
 
 """
 
-import diamond.collector
-import re
 import os
+import re
 
-_RE = re.compile(r'(\d+)\s+(\d+)\s+(\d+)')
+import diamond.collector
 
+_RE = re.compile(r"(\d+)\s+(\d+)\s+(\d+)")
 
-class FilestatCollector(diamond.collector.Collector):
 
-    PROC = '/proc/sys/fs/file-nr'
+class FilestatCollector(diamond.collector.Collector):
+    PROC = "/proc/sys/fs/file-nr"
 
     def get_default_config_help(self):
         config_help = super(FilestatCollector, self).get_default_config_help()
-        config_help.update({
-            'user_include': "This is list of users to collect data for."
-                            " If this is left empty, its a wildcard"
-                            " to collector for all users"
-                            " (default = None)",
-            'user_exclude': "This is a list of users to exclude"
-                            " from collecting data. If this is left empty,"
-                            " no specific users will be excluded"
-                            " (default = None)",
-            'group_include': "This is a list of groups to include"
-                             " in data collection. This DOES NOT"
-                             " override user_exclude."
-                             " (default = None)",
-            'group_exclude': "This is a list of groups to exclude"
-                             " from collecting data. It DOES NOT override"
-                             " user_include. (default = None)",
-            'uid_min': "This creates a floor for the user's uid."
-                       " This means that it WILL NOT collect data"
-                       " for any user with a uid LOWER"
-                       " than the specified minimum,"
-                       " unless the user is told to be included"
-                       " by user_include (default = 0)",
-            'uid_max': "This creates a ceiling for the user's uid."
-                       " This means that it WILL NOT collect data"
-                       " for any user with a uid HIGHER"
-                       " than the specified maximum,"
-                       " unless the user is told to be included"
-                       " by user_include (default = 65536)",
-            'type_include': "This is a list of file types to collect"
-                            " ('REG', 'DIR', 'FIFO', etc). If left empty,"
-                            " will collect for all file types."
-                            "(Note: it's suggested to not leave"
-                            " type_include empty,"
-                            " as it would add significant load"
-                            " to your graphite box(es) (default = None)",
-            'type_exclude': "This is a list of tile types to exclude"
-                            " from being collected for. If left empty,"
-                            " no file types will be excluded. (default = None)",
-            'collect_user_data': "This enables or disables"
-                                 " the collection of user specific"
-                                 " file handles. (default = False)"
-        })
+        config_help.update(
+            {
+                "user_include": "This is list of users to collect data for."
+                " If this is left empty, its a wildcard"
+                " to collector for all users"
+                " (default = None)",
+                "user_exclude": "This is a list of users to exclude"
+                " from collecting data. If this is left empty,"
+                " no specific users will be excluded"
+                " (default = None)",
+                "group_include": "This is a list of groups to include"
+                " in data collection. This DOES NOT"
+                " override user_exclude."
+                " (default = None)",
+                "group_exclude": "This is a list of groups to exclude"
+                " from collecting data. It DOES NOT override"
+                " user_include. (default = None)",
+                "uid_min": "This creates a floor for the user's uid."
+                " This means that it WILL NOT collect data"
+                " for any user with a uid LOWER"
+                " than the specified minimum,"
+                " unless the user is told to be included"
+                " by user_include (default = 0)",
+                "uid_max": "This creates a ceiling for the user's uid."
+                " This means that it WILL NOT collect data"
+                " for any user with a uid HIGHER"
+                " than the specified maximum,"
+                " unless the user is told to be included"
+                " by user_include (default = 65536)",
+                "type_include": "This is a list of file types to collect"
+                " ('REG', 'DIR', 'FIFO', etc). If left empty,"
+                " will collect for all file types."
+                "(Note: it's suggested to not leave"
+                " type_include empty,"
+                " as it would add significant load"
+                " to your graphite box(es) (default = None)",
+                "type_exclude": "This is a list of tile types to exclude"
+                " from being collected for. If left empty,"
+                " no file types will be excluded. (default = None)",
+                "collect_user_data": "This enables or disables"
+                " the collection of user specific"
+                " file handles. (default = False)",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(FilestatCollector, self).get_default_config()
-        config.update({
-            'path':     'files',
-            'user_include': None,
-            'user_exclude': None,
-            'group_include': None,
-            'group_exclude': None,
-            'uid_min': 0,
-            'uid_max': 65536,
-            'type_include': None,
-            'type_exclude': None,
-            'collect_user_data': False
-        })
+        config.update(
+            {
+                "path": "files",
+                "user_include": None,
+                "user_exclude": None,
+                "group_include": None,
+                "group_exclude": None,
+                "uid_min": 0,
+                "uid_max": 65536,
+                "type_include": None,
+                "type_exclude": None,
+                "collect_user_data": False,
+            }
+        )
+
         return config
 
     def get_userlist(self):
         """
         This collects all the users with open files on the system, and filters
         based on the variables user_include and user_exclude
         """
-    # convert user/group  lists to arrays if strings
-        if isinstance(self.config['user_include'], basestring):
-            self.config['user_include'] = self.config['user_include'].split()
-        if isinstance(self.config['user_exclude'], basestring):
-            self.config['user_exclude'] = self.config['user_exclude'].split()
-        if isinstance(self.config['group_include'], basestring):
-            self.config['group_include'] = self.config['group_include'].split()
-        if isinstance(self.config['group_exclude'], basestring):
-            self.config['group_exclude'] = self.config['group_exclude'].split()
+        # convert user/group lists to arrays if strings
+        if isinstance(self.config["user_include"], str):
+            self.config["user_include"] = self.config["user_include"].split()
+
+        if isinstance(self.config["user_exclude"], str):
+            self.config["user_exclude"] = self.config["user_exclude"].split()
+
+        if isinstance(self.config["group_include"], str):
+            self.config["group_include"] = self.config["group_include"].split()
 
-        rawusers = os.popen("lsof | awk '{ print $3 }' | sort | uniq -d"
-                            ).read().split()
+        if isinstance(self.config["group_exclude"], str):
+            self.config["group_exclude"] = self.config["group_exclude"].split()
+
+        rawusers = os.popen("lsof | awk '{ print $3 }' | sort | uniq -d").read().split()
         userlist = []
 
         # remove any not on the user include list
-        if ((self.config['user_include'] is None or
-             len(self.config['user_include']) == 0)):
+        if self.config["user_include"] is None or len(self.config["user_include"]) == 0:
             userlist = rawusers
         else:
             # only work with specified include list, which is added at the end
             userlist = []
 
         # add any user in the group include list
-        addedByGroup = []
-        if ((self.config['group_include'] is not None and
-             len(self.config['group_include']) > 0)):
+        added_by_group = []
+
+        if (
+            self.config["group_include"] is not None
+            and len(self.config["group_include"]) > 0
+        ):
             for u in rawusers:
                 self.log.info(u)
+
                 # get list of groups of user
-                user_groups = os.popen("id -Gn %s" % (u)).read().split()
-                for gi in self.config['group_include']:
+                user_groups = os.popen("id -Gn %s" % u).read().split()
+
+                for gi in self.config["group_include"]:
                     if gi in user_groups and u not in userlist:
                         userlist.append(u)
-                        addedByGroup.append(u)
+                        added_by_group.append(u)
                         break
 
         # remove any user in the exclude group list
-        if ((self.config['group_exclude'] is not None and
-             len(self.config['group_exclude']) > 0)):
+        if (
+            self.config["group_exclude"] is not None
+            and len(self.config["group_exclude"]) > 0
+        ):
             # create tmp list to iterate over while editing userlist
             tmplist = userlist[:]
+
             for u in tmplist:
                 # get list of groups of user
-                groups = os.popen("id -Gn %s" % (u)).read().split()
-                for gi in self.config['group_exclude']:
+                groups = os.popen("id -Gn %s" % u).read().split()
+
+                for gi in self.config["group_exclude"]:
                     if gi in groups:
                         userlist.remove(u)
                         break
 
-        # remove any that aren't within the uid limits
-        # make sure uid_min/max are ints
-        self.config['uid_min'] = int(self.config['uid_min'])
-        self.config['uid_max'] = int(self.config['uid_max'])
+        # remove any that aren't within the uid limits make sure uid_min/max are ints
+        self.config["uid_min"] = int(self.config["uid_min"])
+        self.config["uid_max"] = int(self.config["uid_max"])
         tmplist = userlist[:]
+
         for u in tmplist:
-            if ((self.config['user_include'] is None or
-                 u not in self.config['user_include'])):
-                if u not in addedByGroup:
-                    uid = int(os.popen("id -u %s" % (u)).read())
-                    if ((uid < self.config['uid_min'] and
-                         self.config['uid_min'] is not None and
-                         u in userlist)):
+            if (
+                self.config["user_include"] is None
+                or u not in self.config["user_include"]
+            ):
+                if u not in added_by_group:
+                    uid = int(os.popen("id -u %s" % u).read())
+
+                    if (
+                        uid < self.config["uid_min"]
+                        and self.config["uid_min"] is not None
+                        and u in userlist
+                    ):
                         userlist.remove(u)
-                    if ((uid > self.config['uid_max'] and
-                         self.config['uid_max'] is not None and
-                         u in userlist)):
+
+                    if (
+                        uid > self.config["uid_max"]
+                        and self.config["uid_max"] is not None
+                        and u in userlist
+                    ):
                         userlist.remove(u)
 
         # add users that are in the users include list
-        if ((self.config['user_include'] is not None and
-             len(self.config['user_include']) > 0)):
-            for u in self.config['user_include']:
+        if (
+            self.config["user_include"] is not None
+            and len(self.config["user_include"]) > 0
+        ):
+            for u in self.config["user_include"]:
                 if u in rawusers and u not in userlist:
                     userlist.append(u)
 
         # remove any that is on the user exclude list
-        if ((self.config['user_exclude'] is not None and
-             len(self.config['user_exclude']) > 0)):
-            for u in self.config['user_exclude']:
+        if (
+            self.config["user_exclude"] is not None
+            and len(self.config["user_exclude"]) > 0
+        ):
+            for u in self.config["user_exclude"]:
                 if u in userlist:
                     userlist.remove(u)
 
         return userlist
 
     def get_typelist(self):
         """
         This collects all avaliable types and applies include/exclude filters
         """
         typelist = []
 
         # convert type list into arrays if strings
-        if isinstance(self.config['type_include'], basestring):
-            self.config['type_include'] = self.config['type_include'].split()
-        if isinstance(self.config['type_exclude'], basestring):
-            self.config['type_exclude'] = self.config['type_exclude'].split()
+        if isinstance(self.config["type_include"], str):
+            self.config["type_include"] = self.config["type_include"].split()
+
+        if isinstance(self.config["type_exclude"], str):
+            self.config["type_exclude"] = self.config["type_exclude"].split()
 
         # remove any not in include list
-        if self.config['type_include'] is None or len(
-                self.config['type_include']) == 0:
-            typelist = os.popen("lsof | awk '{ print $5 }' | sort | uniq -d"
-                                ).read().split()
+        if self.config["type_include"] is None or len(self.config["type_include"]) == 0:
+            typelist = (
+                os.popen("lsof | awk '{ print $5 }' | sort | uniq -d").read().split()
+            )
         else:
-            typelist = self.config['type_include']
+            typelist = self.config["type_include"]
 
         # remove any in the exclude list
-        if self.config['type_exclude'] is not None and len(
-                self.config['type_include']) > 0:
-            for t in self.config['type_exclude']:
+        if (
+            self.config["type_exclude"] is not None
+            and len(self.config["type_include"]) > 0
+        ):
+            for t in self.config["type_exclude"]:
                 if t in typelist:
                     typelist.remove(t)
 
         return typelist
 
     def process_lsof(self, users, types):
         """
         Get the list of users and file types to collect for and collect the
         data from lsof
         """
         d = {}
+
         for u in users:
             d[u] = {}
-            tmp = os.popen("lsof -wbu %s | awk '{ print $5 }'" % (
-                u)).read().split()
+            tmp = os.popen("lsof -wbu %s | awk '{ print $5 }'" % u).read().split()
+
             for t in types:
                 d[u][t] = tmp.count(t)
+
         return d
 
     def collect(self):
         if not os.access(self.PROC, os.R_OK):
             return None
 
         # collect total open files
         file = open(self.PROC)
+
         for line in file:
             match = _RE.match(line)
+
             if match:
-                self.publish('assigned', int(match.group(1)))
-                self.publish('unused',   int(match.group(2)))
-                self.publish('max',      int(match.group(3)))
+                self.publish("assigned", int(match.group(1)))
+                self.publish("unused", int(match.group(2)))
+                self.publish("max", int(match.group(3)))
+
         file.close()
 
         # collect open files per user per type
-        if self.config['collect_user_data']:
+        if self.config["collect_user_data"]:
             data = self.process_lsof(self.get_userlist(), self.get_typelist())
-            for ukey in data.iterkeys():
-                for tkey in data[ukey].iterkeys():
-                    self.log.debug('files.user.%s.%s %s' % (
-                        ukey, tkey, int(data[ukey][tkey])))
-                    self.publish('user.%s.%s' % (ukey, tkey),
-                                 int(data[ukey][tkey]))
+
+            for ukey in iter(data.keys()):
+                for tkey in iter(data[ukey].keys()):
+                    self.log.debug(
+                        "files.user.%s.%s %s" % (ukey, tkey, int(data[ukey][tkey]))
+                    )
+                    self.publish("user.%s.%s" % (ukey, tkey), int(data[ukey][tkey]))
```

### Comparing `diamond-next-4.0.515/src/collectors/ntp/ntp.py` & `diamond-next-5.0.0/src/collectors/ntp/ntp.py`

 * *Files 14% similar despite different names*

```diff
@@ -18,82 +18,83 @@
 #### Dependencies
 
     * /usr/sbin/ntpdate
     * subprocess
 
 """
 
-import subprocess
-
 import diamond.collector
-from diamond.collector import str_to_bool
-from diamond import convertor
+import diamond.convertor
 
 
 class NtpCollector(diamond.collector.ProcessCollector):
-
     def get_default_config_help(self):
         config_help = super(NtpCollector, self).get_default_config_help()
-        config_help.update({
-            'bin':      'Path to ntpdate binary',
-            'ntp_pool': 'NTP Pool address',
-            'precision': 'Number of decimal places to report to',
-            'time_scale': 'Time unit to report offset in',
-        })
+        config_help.update(
+            {
+                "bin": "Path to ntpdate binary",
+                "ntp_pool": "NTP Pool address",
+                "precision": "Number of decimal places to report to",
+                "time_scale": "Time unit to report offset in",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(NtpCollector, self).get_default_config()
-        config.update({
-            'bin':      self.find_binary('/usr/sbin/ntpdate'),
-            'ntp_pool': 'pool.ntp.org',
-            'path':     'ntp',
-            'precision': 0,
-            'time_scale': 'milliseconds',
-        })
+        config.update(
+            {
+                "bin": self.find_binary("/usr/sbin/ntpdate"),
+                "ntp_pool": "pool.ntp.org",
+                "path": "ntp",
+                "precision": 0,
+                "time_scale": "milliseconds",
+            }
+        )
         return config
 
     def get_ntpdate_stats(self):
-        output = self.run_command(['-q', self.config['ntp_pool']])
+        output = self.run_command(["-q", self.config["ntp_pool"]])
 
-        data = {}
-        data['server.count'] = {'val': 0, 'precision': 0}
+        data = {"server.count": {"val": 0, "precision": 0}}
 
         for line in output[0].splitlines():
             # Only care about best choice not all servers
-            if line.startswith('server'):
-                data['server.count']['val'] += 1
+            if line.startswith("server"):
+                data["server.count"]["val"] += 1
                 continue
 
             parts = line.split()
 
             # Make sure we have the correct output
             # Sample of line: 31 Apr 12:00:00 ntpdate[12345]: adjust time \
             #   server 123.456.789.2 offset -0.000123 sec
             if len(parts) != 11:
-                self.log.error('NtpCollector: Output of ntpdate was %s words '
-                               'long but was expected to be 11' % len(parts))
-                self.log.debug('NtpCollector: ntpdate output was %s' % parts)
+                self.log.error(
+                    "NtpCollector: Output of ntpdate was %s words long but was expected to be 11"
+                    % len(parts)
+                )
+                self.log.debug("NtpCollector: ntpdate output was %s" % parts)
                 continue
 
             # offset is in seconds, convert is to nanoseconds and miliseconds
             offset_in_s = float(parts[9])
 
             # Convert to the requested time unit
-            offset = convertor.time.convert(offset_in_s,
-                                            's',
-                                            self.config['time_scale'])
+            offset = diamond.convertor.time.convert(
+                offset_in_s, "s", self.config["time_scale"]
+            )
 
             # Determine metric namespace based on given time unit
-            metric_name = 'offset.%s' % self.config['time_scale']
+            metric_name = "offset.%s" % self.config["time_scale"]
 
-            data[metric_name] = {'val': offset,
-                                 'precision': self.config['precision']}
+            data[metric_name] = {"val": offset, "precision": self.config["precision"]}
 
         return data.items()
 
     def collect(self):
         for stat, v in self.get_ntpdate_stats():
-            self.publish(stat, v['val'], precision=v['precision'])
+            self.publish(stat, v["val"], precision=v["precision"])
```

### Comparing `diamond-next-4.0.515/src/collectors/bind/bind.py` & `diamond-next-5.0.0/src/collectors/bind/bind.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,150 +6,156 @@
 #### Dependencies
 
  * [bind 9.5](http://www.isc.org/software/bind/new-features/9.5)
     configured with libxml2 and statistics-channels
 
 """
 
-import diamond.collector
-import sys
-import urllib2
+import urllib.request
+import xml.etree.cElementTree as ElementTree
 
-if sys.version_info >= (2, 5):
-    import xml.etree.cElementTree as ElementTree
-else:
-    import cElementTree as ElementTree
+import diamond.collector
 
 
 class BindCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(BindCollector, self).get_default_config_help()
-        config_help.update({
-            'host': "",
-            'port': "",
-            'publish': "Available stats:\n" +
-            " - resolver (Per-view resolver and cache statistics)\n" +
-            " - server (Incoming requests and their answers)\n" +
-            " - zonemgmt (Zone management requests/responses)\n" +
-            " - sockets (Socket statistics)\n" +
-            " - memory (Global memory usage)\n",
-            'publish_view_bind': "",
-            'publish_view_meta': "",
-        })
+        config_help.update(
+            {
+                "host": "",
+                "port": "",
+                "publish": "Available stats:\n"
+                + " - resolver (Per-view resolver and cache statistics)\n"
+                + " - server (Incoming requests and their answers)\n"
+                + " - zonemgmt (Zone management requests/responses)\n"
+                + " - sockets (Socket statistics)\n"
+                + " - memory (Global memory usage)\n",
+                "publish_view_bind": "",
+                "publish_view_meta": "",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(BindCollector, self).get_default_config()
-        config.update({
-            'host': 'localhost',
-            'port': 8080,
-            'path': 'bind',
-            # Available stats:
-            # - resolver (Per-view resolver and cache statistics)
-            # - server (Incoming requests and their answers)
-            # - zonemgmt (Requests/responses related to zone management)
-            # - sockets (Socket statistics)
-            # - memory (Global memory usage)
-            'publish': [
-                'resolver',
-                'server',
-                'zonemgmt',
-                'sockets',
-                'memory',
-            ],
-            # By default we don't publish these special views
-            'publish_view_bind': False,
-            'publish_view_meta': False,
-        })
+        config.update(
+            {
+                "host": "localhost",
+                "port": 8080,
+                "path": "bind",
+                # Available stats:
+                # - resolver (Per-view resolver and cache statistics)
+                # - server (Incoming requests and their answers)
+                # - zonemgmt (Requests/responses related to zone management)
+                # - sockets (Socket statistics)
+                # - memory (Global memory usage)
+                "publish": [
+                    "resolver",
+                    "server",
+                    "zonemgmt",
+                    "sockets",
+                    "memory",
+                ],
+                # By default we don't publish these special views
+                "publish_view_bind": False,
+                "publish_view_meta": False,
+            }
+        )
         return config
 
     def clean_counter(self, name, value):
         value = self.derivative(name, value)
+
         if value < 0:
             value = 0
+
         self.publish(name, value)
 
     def collect(self):
         try:
-            req = urllib2.urlopen('http://%s:%d/' % (
-                self.config['host'], int(self.config['port'])))
-        except Exception, e:
-            self.log.error('Couldnt connect to bind: %s', e)
+            req = urllib.request.urlopen(
+                "http://%s:%d/" % (self.config["host"], int(self.config["port"]))
+            )
+        except Exception as e:
+            self.log.error("Could not connect to bind: %s", e)
+
             return {}
 
         tree = ElementTree.parse(req)
 
         if not tree:
             raise ValueError("Corrupt XML file, no statistics found")
 
-        root = tree.find('bind/statistics')
+        root = tree.find("bind/statistics")
+
+        if "resolver" in self.config["publish"]:
+            for view in root.findall("views/view"):
+                name = view.find("name").text
 
-        if 'resolver' in self.config['publish']:
-            for view in root.findall('views/view'):
-                name = view.find('name').text
-                if name == '_bind' and not self.config['publish_view_bind']:
+                if name == "_bind" and not self.config["publish_view_bind"]:
                     continue
-                if name == '_meta' and not self.config['publish_view_meta']:
+
+                if name == "_meta" and not self.config["publish_view_meta"]:
                     continue
-                nzones = len(view.findall('zones/zone'))
-                self.publish('view.%s.zones' % name, nzones)
-                for counter in view.findall('rdtype'):
+
+                nzones = len(view.findall("zones/zone"))
+                self.publish("view.%s.zones" % name, nzones)
+
+                for counter in view.findall("rdtype"):
                     self.clean_counter(
-                        'view.%s.query.%s' % (name,
-                                              counter.find('name').text),
-                        int(counter.find('counter').text)
+                        "view.%s.query.%s" % (name, counter.find("name").text),
+                        int(counter.find("counter").text),
                     )
-                for counter in view.findall('resstat'):
+
+                for counter in view.findall("resstat"):
                     self.clean_counter(
-                        'view.%s.resstat.%s' % (name,
-                                                counter.find('name').text),
-                        int(counter.find('counter').text)
+                        "view.%s.resstat.%s" % (name, counter.find("name").text),
+                        int(counter.find("counter").text),
                     )
-                for counter in view.findall('cache/rrset'):
+
+                for counter in view.findall("cache/rrset"):
                     self.clean_counter(
-                        'view.%s.cache.%s' % (
-                            name, counter.find('name').text.replace('!',
-                                                                    'NOT_')),
-                        int(counter.find('counter').text)
+                        "view.%s.cache.%s"
+                        % (name, counter.find("name").text.replace("!", "NOT_")),
+                        int(counter.find("counter").text),
                     )
 
-        if 'server' in self.config['publish']:
-            for counter in root.findall('server/requests/opcode'):
+        if "server" in self.config["publish"]:
+            for counter in root.findall("server/requests/opcode"):
                 self.clean_counter(
-                    'requests.%s' % counter.find('name').text,
-                    int(counter.find('counter').text)
+                    "requests.%s" % counter.find("name").text,
+                    int(counter.find("counter").text),
                 )
-            for counter in root.findall('server/queries-in/rdtype'):
+
+            for counter in root.findall("server/queries-in/rdtype"):
                 self.clean_counter(
-                    'queries.%s' % counter.find('name').text,
-                    int(counter.find('counter').text)
+                    "queries.%s" % counter.find("name").text,
+                    int(counter.find("counter").text),
                 )
-            for counter in root.findall('server/nsstat'):
+
+            for counter in root.findall("server/nsstat"):
                 self.clean_counter(
-                    'nsstat.%s' % counter.find('name').text,
-                    int(counter.find('counter').text)
+                    "nsstat.%s" % counter.find("name").text,
+                    int(counter.find("counter").text),
                 )
 
-        if 'zonemgmt' in self.config['publish']:
-            for counter in root.findall('server/zonestat'):
+        if "zonemgmt" in self.config["publish"]:
+            for counter in root.findall("server/zonestat"):
                 self.clean_counter(
-                    'zonestat.%s' % counter.find('name').text,
-                    int(counter.find('counter').text)
+                    "zonestat.%s" % counter.find("name").text,
+                    int(counter.find("counter").text),
                 )
 
-        if 'sockets' in self.config['publish']:
-            for counter in root.findall('server/sockstat'):
+        if "sockets" in self.config["publish"]:
+            for counter in root.findall("server/sockstat"):
                 self.clean_counter(
-                    'sockstat.%s' % counter.find('name').text,
-                    int(counter.find('counter').text)
+                    "sockstat.%s" % counter.find("name").text,
+                    int(counter.find("counter").text),
                 )
 
-        if 'memory' in self.config['publish']:
-            for counter in root.find('memory/summary').getchildren():
-                self.publish(
-                    'memory.%s' % counter.tag,
-                    int(counter.text)
-                )
+        if "memory" in self.config["publish"]:
+            for counter in list(root.find("memory/summary").iter()):
+                self.publish("memory.%s" % counter.tag, int(counter.text))
```

### Comparing `diamond-next-4.0.515/src/collectors/hadoop/hadoop.py` & `diamond-next-5.0.0/src/collectors/hadoop/hadoop.py`

 * *Files 27% similar despite different names*

```diff
@@ -7,115 +7,143 @@
 
 #### Dependencies
 
  * hadoop
 
 """
 
-from diamond.metric import Metric
-import diamond.collector
-from diamond.collector import str_to_bool
 import glob
-import re
 import os
+import re
+
+import diamond.collector
+import diamond.metric
 
 
 class HadoopCollector(diamond.collector.Collector):
 
-    re_log = re.compile(r'^(?P<timestamp>\d+) (?P<name>\S+): (?P<metrics>.*)$')
+    re_log = re.compile(r"^(?P<timestamp>\d+) (?P<name>\S+): (?P<metrics>.*)$")
 
     def get_default_config_help(self):
         config_help = super(HadoopCollector, self).get_default_config_help()
-        config_help.update({
-            'metrics':  "List of paths to process metrics from",
-            'truncate': "Truncate the metrics files after reading them.",
-        })
+        config_help.update(
+            {
+                "metrics": "List of paths to process metrics from",
+                "truncate": "Truncate the metrics files after reading them.",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(HadoopCollector, self).get_default_config()
-        config.update({
-            'path':      'hadoop',
-            'metrics':   ['/var/log/hadoop/*-metrics.out'],
-            'truncate':  False,
-        })
+        config.update(
+            {
+                "path": "hadoop",
+                "metrics": ["/var/log/hadoop/*-metrics.out"],
+                "truncate": False,
+            }
+        )
         return config
 
     def collect(self):
-        for pattern in self.config['metrics']:
+        metrics = self.config["metrics"]
+        if not isinstance(metrics, list):
+            metrics = [str(metrics)]
+
+        for pattern in metrics:
             for filename in glob.glob(pattern):
                 self.collect_from(filename)
 
     def collect_from(self, filename):
         if not os.access(filename, os.R_OK):
             self.log.error('HadoopCollector unable to read "%s"', filename)
             return False
 
-        if self.config['truncate']:
-            fd = open(filename, 'r+')
+        if self.config["truncate"]:
+            fd = open(filename, "r+")
         else:
-            fd = open(filename, 'r')
+            fd = open(filename, "r")
 
         for line in fd:
             match = self.re_log.match(line)
             if not match:
                 continue
 
             metrics = {}
 
             data = match.groupdict()
-            for metric in data['metrics'].split(','):
+            for metric in data["metrics"].split(","):
                 metric = metric.strip()
-                if '=' in metric:
-                    key, value = metric.split('=', 1)
+                if "=" in metric:
+                    key, value = metric.split("=", 1)
                     metrics[key] = value
 
             for metric in metrics.keys():
                 try:
 
-                    if data['name'] == 'jvm.metrics':
-                        path = self.get_metric_path('.'.join([
-                            data['name'],
-                            metrics['hostName'].replace('.', '_'),
-                            metrics['processName'].replace(' ', '_'),
-                            metric, ]))
-
-                    elif data['name'] == 'mapred.job':
-                        path = self.get_metric_path('.'.join([
-                            data['name'],
-                            metrics['hostName'].replace('.', '_'),
-                            metrics['group'].replace(' ', '_'),
-                            metrics['counter'].replace(' ', '_'),
-                            metric, ]))
+                    if data["name"] == "jvm.metrics":
+                        path = self.get_metric_path(
+                            ".".join(
+                                [
+                                    data["name"],
+                                    metrics["hostName"].replace(".", "_"),
+                                    metrics["processName"].replace(" ", "_"),
+                                    metric,
+                                ]
+                            )
+                        )
+
+                    elif data["name"] == "mapred.job":
+                        path = self.get_metric_path(
+                            ".".join(
+                                [
+                                    data["name"],
+                                    metrics["hostName"].replace(".", "_"),
+                                    metrics["group"].replace(" ", "_"),
+                                    metrics["counter"].replace(" ", "_"),
+                                    metric,
+                                ]
+                            )
+                        )
 
-                    elif data['name'] == 'rpc.metrics':
+                    elif data["name"] == "rpc.metrics":
 
-                        if metric == 'port':
+                        if metric == "port":
                             continue
 
-                        path = self.get_metric_path('.'.join([
-                            data['name'],
-                            metrics['hostName'].replace('.', '_'),
-                            metrics['port'],
-                            metric, ]))
+                        path = self.get_metric_path(
+                            ".".join(
+                                [
+                                    data["name"],
+                                    metrics["hostName"].replace(".", "_"),
+                                    metrics["port"],
+                                    metric,
+                                ]
+                            )
+                        )
 
                     else:
-                        path = self.get_metric_path('.'.join([
-                            data['name'],
-                            metric, ]))
+                        path = self.get_metric_path(
+                            ".".join(
+                                [
+                                    data["name"],
+                                    metric,
+                                ]
+                            )
+                        )
 
                     value = float(metrics[metric])
 
                     self.publish_metric(
-                        Metric(path,
-                               value,
-                               timestamp=int(data['timestamp']) / 1000))
-
+                        diamond.metric.Metric(
+                            path, value, timestamp=int(data["timestamp"]) / 1000
+                        )
+                    )
                 except ValueError:
                     pass
-        if str_to_bool(self.config['truncate']):
+        if diamond.collector.str_to_bool(self.config["truncate"]):
             fd.seek(0)
             fd.truncate()
         fd.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/postqueue/postqueue.py` & `diamond-next-5.0.0/src/collectors/postqueue/postqueue.py`

 * *Files 23% similar despite different names*

```diff
@@ -6,56 +6,58 @@
 #### Dependencies
 
  * subprocess
 
 """
 
 import subprocess
+
 import diamond.collector
-from diamond.collector import str_to_bool
 
 
 class PostqueueCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(PostqueueCollector, self).get_default_config_help()
-        config_help.update({
-            'bin':         'The path to the postqueue binary',
-            'use_sudo':    'Use sudo?',
-            'sudo_cmd':    'Path to sudo',
-        })
+        config_help.update(
+            {
+                "bin": "The path to the postqueue binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(PostqueueCollector, self).get_default_config()
-        config.update({
-            'path':             'postqueue',
-            'bin':              '/usr/bin/postqueue',
-            'use_sudo':         False,
-            'sudo_cmd':         '/usr/bin/sudo',
-        })
+        config.update(
+            {
+                "path": "postqueue",
+                "bin": "/usr/bin/postqueue",
+                "use_sudo": False,
+                "sudo_cmd": "/usr/bin/sudo",
+            }
+        )
         return config
 
     def get_postqueue_output(self):
         try:
-            command = [self.config['bin'], '-p']
+            command = [self.config["bin"], "-p"]
 
-            if str_to_bool(self.config['use_sudo']):
-                command.insert(0, self.config['sudo_cmd'])
+            if diamond.collector.str_to_bool(self.config["use_sudo"]):
+                command.insert(0, self.config["sudo_cmd"])
 
-            return subprocess.Popen(command,
-                                    stdout=subprocess.PIPE).communicate()[0]
+            return subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0]
         except OSError:
             return ""
 
     def collect(self):
         output = self.get_postqueue_output()
 
         try:
             postqueue_count = int(output.strip().split("\n")[-1].split()[-2])
         except:
             postqueue_count = 0
 
-        self.publish('count', postqueue_count)
+        self.publish("count", postqueue_count)
```

### Comparing `diamond-next-4.0.515/src/collectors/iodrivesnmp/iodrivesnmp.py` & `diamond-next-5.0.0/src/collectors/iodrivesnmp/iodrivesnmp.py`

 * *Files 12% similar despite different names*

```diff
@@ -20,116 +20,113 @@
 [[iodrive]]
 host = my_host
 port = 161
 community = mycommunitystring
 
 """
 
-import sys
 import os
-import time
 import struct
 
+import sys
+import time
+
+import diamond.metric
+
 # Fix Path for locating the SNMPCollector
-sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
-                                             '../',
-                                             'snmp',
-                                             )))
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../", "snmp")))
 
-from diamond.metric import Metric
 from snmp import SNMPCollector as parent_SNMPCollector
 
 
 class IODriveSNMPCollector(parent_SNMPCollector):
     """
     SNMPCollector for a single Fusion IO Drive
     """
 
     IODRIVE_STATS = {
-
         "InternalTemp": "1.3.6.1.4.1.30018.1.2.1.1.1.24.5",
-
         "MilliVolts": "1.3.6.1.4.1.30018.1.2.1.1.1.32.5",
         "MilliWatts": "1.3.6.1.4.1.30018.1.2.1.1.1.35.5",
         "MilliAmps": "1.3.6.1.4.1.30018.1.2.1.1.1.37.5",
     }
 
     IODRIVE_BYTE_STATS = {
-
         "BytesReadU": "1.3.6.1.4.1.30018.1.2.2.1.1.12.5",
         "BytesReadL": "1.3.6.1.4.1.30018.1.2.2.1.1.13.5",
-
         "BytesWrittenU": "1.3.6.1.4.1.30018.1.2.2.1.1.14.5",
         "BytesWrittenL": "1.3.6.1.4.1.30018.1.2.2.1.1.15.5",
-
     }
 
     MAX_VALUE = 18446744073709551615
 
     def get_default_config_help(self):
-        config_help = super(IODriveSNMPCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': 'Host address',
-            'port': 'SNMP port to collect snmp data',
-            'community': 'SNMP community',
-        })
+        config_help = super(IODriveSNMPCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "Host address",
+                "port": "SNMP port to collect snmp data",
+                "community": "SNMP community",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(IODriveSNMPCollector, self).get_default_config()
-        config.update({
-            'path':     'iodrive',
-            'timeout':  15,
-        })
+        config.update(
+            {
+                "path": "iodrive",
+                "timeout": 15,
+            }
+        )
         return config
 
     def get_string_index_oid(self, s):
         """Turns a string into an oid format is length of name followed by
         name chars in ascii"""
-        return (len(self.get_bytes(s)), ) + self.get_bytes(s)
+        return (len(self.get_bytes(s)),) + self.get_bytes(s)
 
     def get_bytes(self, s):
         """Turns a string into a list of byte values"""
-        return struct.unpack('%sB' % len(s), s)
+        return struct.unpack("%sB" % len(s), s)
 
     def collect_snmp(self, device, host, port, community):
         """
         Collect Fusion IO Drive SNMP stats from device
         host and device are from the conf file. In the future device should be
         changed to be what IODRive device it being checked.
         i.e. fioa, fiob.
         """
 
         # Set timestamp
         timestamp = time.time()
 
         for k, v in self.IODRIVE_STATS.items():
             # Get Metric Name and Value
-            metricName = '.'.join([k])
-            metricValue = int(self.get(v, host, port, community)[v])
+            metric_name = ".".join([k])
+            metric_value = int(self.get(v, host, port, community)[v])
 
             # Get Metric Path
-            metricPath = '.'.join(['servers', host, device, metricName])
+            metric_path = ".".join(["servers", host, device, metric_name])
 
             # Create Metric
-            metric = Metric(metricPath, metricValue, timestamp, 0)
+            metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 0)
 
             # Publish Metric
             self.publish_metric(metric)
 
         for k, v in self.IODRIVE_BYTE_STATS.items():
             # Get Metric Name and Value
-            metricName = '.'.join([k])
-            metricValue = int(self.get(v, host, port, community)[v])
+            metric_name = ".".join([k])
+            metric_value = int(self.get(v, host, port, community)[v])
 
             # Get Metric Path
-            metricPath = '.'.join(['servers', host, device, metricName])
+            metric_path = ".".join(["servers", host, device, metric_name])
 
             # Create Metric
-            metric = Metric(metricPath, metricValue, timestamp, 0)
+            metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 0)
 
             # Publish Metric
             self.publish_metric(metric)
```

### Comparing `diamond-next-4.0.515/src/collectors/darner/darner.py` & `diamond-next-5.0.0/src/collectors/darner/darner.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,144 +17,149 @@
 TO use a unix socket, set a host string like this
 
 ```
     hosts = /path/to/blah.sock, app-1@/path/to/bleh.sock,
 ```
 """
 
-import diamond.collector
-import socket
 import re
-from diamond.collector import str_to_bool
+import socket
+
+import diamond.collector
 
 
 class DarnerCollector(diamond.collector.Collector):
-    GAUGES = [
-        'curr_connections',
-        'curr_items',
-        'uptime'
-    ]
+    GAUGES = ["curr_connections", "curr_items", "uptime"]
 
     def get_default_config_help(self):
         config_help = super(DarnerCollector, self).get_default_config_help()
-        config_help.update({
-            'publish':
-                "Which rows of 'status' you would like to publish." +
-                " Telnet host port' and type stats and hit enter to see " +
-                " the list of possibilities. Leave unset to publish all.",
-            'hosts':
-                "List of hosts, and ports to collect. Set an alias by " +
-                " prefixing the host:port with alias@",
-            'publish_queues':
-                "Publish queue stats (defaults to True)",
-        })
+        config_help.update(
+            {
+                "publish": "Which rows of 'status' you would like to publish."
+                + " Telnet host port' and type stats and hit enter to see "
+                + " the list of possibilities. Leave unset to publish all.",
+                "hosts": "List of hosts, and ports to collect. Set an alias by "
+                + " prefixing the host:port with alias@",
+                "publish_queues": "Publish queue stats (defaults to True)",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(DarnerCollector, self).get_default_config()
-        config.update({
-            'path':     'darner',
+        config.update(
+            {
+                "path": "darner",
+                # Which rows of 'status' you would like to publish.
+                # 'telnet host port' and type stats and hit enter to see the list
+                # of possibilities.
+                # Leave unset to publish all
+                # 'publish': ''
+                "publish_queues": True,
+                # Connection settings
+                "hosts": ["localhost:22133"],
+            }
+        )
 
-            # Which rows of 'status' you would like to publish.
-            # 'telnet host port' and type stats and hit enter to see the list
-            # of possibilities.
-            # Leave unset to publish all
-            # 'publish': ''
-            'publish_queues': True,
-
-            # Connection settings
-            'hosts': ['localhost:22133']
-        })
         return config
 
     def get_raw_stats(self, host, port):
-        data = ''
+        data = ""
+
         # connect
         try:
             if port is None:
                 sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                 sock.connect(host)
             else:
                 sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                 sock.connect((host, int(port)))
+
             # request stats
-            sock.send('stats\n')
+            sock.send("stats\n")
+
             # something big enough to get whatever is sent back
             data = sock.recv(4096)
         except socket.error:
-            self.log.exception('Failed to get stats from %s:%s',
-                               host, port)
+            self.log.exception("Failed to get stats from %s:%s", host, port)
+
         return data
 
     def get_stats(self, host, port):
         # stuff that's always ignored, aren't 'stats'
-        ignored = ('time', 'version')
+        ignored = ("time", "version")
 
         stats = {}
         queues = {}
         data = self.get_raw_stats(host, port)
 
         # parse stats
         for line in data.splitlines():
-            pieces = line.split(' ')
-            if pieces[0] != 'STAT' or pieces[1] in ignored:
+            pieces = line.split(" ")
+
+            if pieces[0] != "STAT" or pieces[1] in ignored:
                 continue
-            if re.match(r'^queue', pieces[1]):
+
+            if re.match(r"^queue", pieces[1]):
                 queue_match = re.match(
-                    r'^queue_(.*)_(items|waiters|open_transactions)$',
-                    pieces[1])
-                queue_name = queue_match.group(1).replace('.', '_')
+                    r"^queue_(.*)_(items|waiters|open_transactions)$", pieces[1]
+                )
+                queue_name = queue_match.group(1).replace(".", "_")
+
                 if queue_name not in queues:
                     queues[queue_name] = {}
+
                 queues[queue_name][queue_match.group(2)] = int(pieces[2])
             else:
                 stats[pieces[1]] = int(pieces[2])
+
         return stats, queues
 
     def collect(self):
-        hosts = self.config.get('hosts')
+        hosts = self.config.get("hosts")
 
         # Convert a string config value to be an array
-        if isinstance(hosts, basestring):
+        if isinstance(hosts, str):
             hosts = [hosts]
 
         for host in hosts:
-            matches = re.search('((.+)\@)?([^:]+)(:(\d+))?', host)
+            matches = re.search("((.+)\@)?([^:]+)(:(\d+))?", host)
             alias = matches.group(2)
             hostname = matches.group(3)
             port = matches.group(5)
 
             if alias is None:
                 alias = hostname
 
             stats, queues = self.get_stats(hostname, port)
 
             # Publish queue stats if configured
-            if str_to_bool(self.config['publish_queues']):
+            if diamond.collector.str_to_bool(self.config["publish_queues"]):
                 for queue in queues:
                     for queue_stat in queues[queue]:
                         self.publish_gauge(
                             alias + ".queues." + queue + "." + queue_stat,
-                            queues[queue][queue_stat])
+                            queues[queue][queue_stat],
+                        )
 
             # figure out what we're configured to get, defaulting to everything
-            desired = self.config.get('publish', stats.keys())
+            desired = self.config.get("publish", stats.keys())
 
             # for everything we want
             for stat in desired:
                 if stat in stats:
 
                     # we have it
                     if stat in self.GAUGES:
                         self.publish_gauge(alias + "." + stat, stats[stat])
                     else:
                         self.publish_counter(alias + "." + stat, stats[stat])
-
                 else:
-
-                    # we don't, must be something configured in publish so we
-                    # should log an error about it
-                    self.log.error("No such key '%s' available, issue 'stats' "
-                                   "for a full list", stat)
+                    # we don't, must be something configured in publish so we should log an error about it
+                    self.log.error(
+                        "No such key '%s' available, issue 'stats' for a full list",
+                        stat,
+                    )
```

### Comparing `diamond-next-4.0.515/src/collectors/websitemonitor/websitemonitor.py` & `diamond-next-5.0.0/src/collectors/websitemonitor/websitemonitor.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,78 +1,86 @@
 # coding=utf-8
 """
 Gather HTTP Response code and Duration of HTTP request
 
 #### Dependencies
-  * urllib2
-
+  * urllib
 """
 
-import urllib2
+import datetime
+import urllib.error
+import urllib.request
+
 import time
-from datetime import datetime
+
 import diamond.collector
 
 
 class WebsiteMonitorCollector(diamond.collector.Collector):
     """
     Gather HTTP response code and Duration of HTTP request
     """
 
     def get_default_config_help(self):
-        config_help = super(WebsiteMonitorCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'URL': "FQDN of HTTP endpoint to test",
+        config_help = super(WebsiteMonitorCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "URL": "FQDN of HTTP endpoint to test",
+            }
+        )
 
-        })
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
-        default_config = super(WebsiteMonitorCollector,
-                               self).get_default_config()
-        default_config['URL'] = ''
-        default_config['path'] = 'websitemonitor'
+        default_config = super(WebsiteMonitorCollector, self).get_default_config()
+        default_config["URL"] = ""
+        default_config["path"] = "websitemonitor"
+
         return default_config
 
     def collect(self):
-        req = urllib2.Request('%s' % (self.config['URL']))
+        req = urllib.request.Request("%s" % (self.config["URL"]))
+
+        # time in seconds since epoch as a floating number
+        start_time = time.time()
 
         try:
-            # time in seconds since epoch as a floating number
-            start_time = time.time()
             # human-readable time e.g November 25, 2013 18:15:56
-            st = datetime.fromtimestamp(start_time
-                                        ).strftime('%B %d, %Y %H:%M:%S')
-            self.log.debug('Start time: %s' % (st))
+            st = datetime.datetime.fromtimestamp(start_time).strftime(
+                "%B %d, %Y %H:%M:%S"
+            )
+            self.log.debug("Start time: %s" % st)
+
+            resp = urllib.request.urlopen(req)
 
-            resp = urllib2.urlopen(req)
             # time in seconds since epoch as a floating number
             end_time = time.time()
+
             # human-readable end time e.eg. November 25, 2013 18:15:56
-            et = datetime.fromtimestamp(end_time).strftime('%B %d, %Y %H:%M%S')
-            self.log.debug('End time: %s' % (et))
+            et = datetime.datetime.fromtimestamp(end_time).strftime("%B %d, %Y %H:%M%S")
+            self.log.debug("End time: %s" % et)
+
             # Response time in milliseconds
-            rt = int(format((end_time - start_time) * 1000, '.0f'))
+            rt = int(format((end_time - start_time) * 1000, ".0f"))
+
             # Publish metrics
-            self.publish('response_time.%s' % (resp.code), rt,
-                         metric_type='COUNTER')
-        # urllib2 will puke on non HTTP 200/OK URLs
-        except urllib2.URLError, e:
+            self.publish("response_time.%s" % resp.code, rt, metric_type="COUNTER")
+        # urllib will puke on non HTTP 200/OK URLs
+        except urllib.error.URLError as e:
             if e.code != 200:
                 # time in seconds since epoch as a floating number
                 end_time = time.time()
+
                 # Response time in milliseconds
-                rt = int(format((end_time - start_time) * 1000, '.0f'))
-                # Publish metrics -- this is recording a failure, rt will
-                # likely be 0 but does capture HTTP Status Code
-                self.publish('response_time.%s' % (e.code), rt,
-                             metric_type='COUNTER')
+                rt = int(format((end_time - start_time) * 1000, ".0f"))
+
+                # Publish metrics -- this is recording a failure, rt will likely be 0 but does capture HTTP Status Code
+                self.publish("response_time.%s" % e.code, rt, metric_type="COUNTER")
 
-        except IOError, e:
-            self.log.error('Unable to open %s' % (self.config['URL']))
+        except IOError:
+            self.log.error("Unable to open %s" % (self.config["URL"]))
 
-        except Exception, e:
+        except Exception as e:
             self.log.error("Unknown error opening url: %s", e)
```

### Comparing `diamond-next-4.0.515/src/collectors/interrupt/soft.py` & `diamond-next-5.0.0/src/collectors/interrupt/soft.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,78 +6,73 @@
 
 #### Dependencies
 
  * /proc/stat
 
 """
 
-import platform
 import os
+import platform
+
 import diamond.collector
 
 # Detect the architecture of the system
 # and set the counters for MAX_VALUES
 # appropriately. Otherwise, rolling over
 # counters will cause incorrect or
 # negative values.
-if platform.architecture()[0] == '64bit':
-    counter = (2 ** 64) - 1
+if platform.architecture()[0] == "64bit":
+    counter = (2**64) - 1
 else:
-    counter = (2 ** 32) - 1
+    counter = (2**32) - 1
 
 
 class SoftInterruptCollector(diamond.collector.Collector):
-
-    PROC = '/proc/stat'
+    PROC = "/proc/stat"
 
     def get_default_config_help(self):
-        config_help = super(SoftInterruptCollector,
-                            self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(SoftInterruptCollector, self).get_default_config_help()
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(SoftInterruptCollector, self).get_default_config()
-        config.update({
-            'path':     'softirq'
-        })
+        config.update({"path": "softirq"})
+
         return config
 
     def collect(self):
         """
         Collect interrupt data
         """
         if not os.access(self.PROC, os.R_OK):
             return False
 
         # Open PROC file
-        file = open(self.PROC, 'r')
+        file = open(self.PROC, "r")
 
         # Get data
         for line in file:
-
-            if not line.startswith('softirq'):
+            if not line.startswith("softirq"):
                 continue
 
             data = line.split()
 
-            metric_name = 'total'
+            metric_name = "total"
             metric_value = int(data[1])
-            metric_value = int(self.derivative(
-                metric_name,
-                long(metric_value), counter))
+            metric_value = int(self.derivative(metric_name, int(metric_value), counter))
             self.publish(metric_name, metric_value)
 
             for i in range(2, len(data)):
                 metric_name = str(i - 2)
                 metric_value = int(data[i])
-                metric_value = int(self.derivative(
-                    metric_name,
-                    long(metric_value), counter))
+                metric_value = int(
+                    self.derivative(metric_name, int(metric_value), counter)
+                )
                 self.publish(metric_name, metric_value)
 
         # Close file
         file.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/interrupt/interrupt.py` & `diamond-next-5.0.0/src/collectors/interrupt/interrupt.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,97 +6,100 @@
 
 #### Dependencies
 
  * /proc/interrupts
 
 """
 
-import platform
 import os
+import platform
+
 import diamond.collector
 
 # Detect the architecture of the system
 # and set the counters for MAX_VALUES
 # appropriately. Otherwise, rolling over
 # counters will cause incorrect or
 # negative values.
-if platform.architecture()[0] == '64bit':
-    counter = (2 ** 64) - 1
+if platform.architecture()[0] == "64bit":
+    counter = (2**64) - 1
 else:
-    counter = (2 ** 32) - 1
+    counter = (2**32) - 1
 
 
 class InterruptCollector(diamond.collector.Collector):
-
-    PROC = '/proc/interrupts'
+    PROC = "/proc/interrupts"
 
     def get_default_config_help(self):
         config_help = super(InterruptCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(InterruptCollector, self).get_default_config()
-        config.update({
-            'path':     'interrupts'
-        })
+        config.update({"path": "interrupts"})
         return config
 
     def collect(self):
         """
         Collect interrupt data
         """
         if not os.access(self.PROC, os.R_OK):
             return False
 
         # Open PROC file
-        file = open(self.PROC, 'r')
+        file = open(self.PROC, "r")
+
         # Get data
-        cpuCount = None
+        cpu_count = None
+
         for line in file:
-            if not cpuCount:
-                cpuCount = len(line.split())
+            if not cpu_count:
+                cpu_count = len(line.split())
             else:
-                data = line.strip().split(None, cpuCount + 2)
-                data[0] = data[0].replace(':', '')
+                data = line.strip().split(None, cpu_count + 2)
+                data[0] = data[0].replace(":", "")
 
                 if len(data) == 2:
                     metric_name = data[0]
                     metric_value = data[1]
-                    self.publish(metric_name,
-                                 self.derivative(metric_name,
-                                                 long(metric_value),
-                                                 counter))
+                    self.publish(
+                        metric_name,
+                        self.derivative(metric_name, int(metric_value), counter),
+                    )
                 else:
-                    if len(data[0]) == cpuCount + 1:
-                        metric_name = data[0] + '.'
+                    if len(data[0]) == cpu_count + 1:
+                        metric_name = data[0] + "."
                     elif len(data[0]) == 3:
                         metric_name = (
-                            ((data[-2] + ' ' +
-                              data[-1]).replace(' ', '_')) + '.')
+                            (data[-2] + " " + data[-1]).replace(" ", "_")
+                        ) + "."
                     else:
                         metric_name = (
-                            ((data[-2]).replace(' ', '_')) +
-                            '.' +
-                            ((data[-1]).replace(', ', '-').replace(' ', '_')) +
-                            '.' + data[0] + '.')
+                            ((data[-2]).replace(" ", "_"))
+                            + "."
+                            + ((data[-1]).replace(", ", "-").replace(" ", "_"))
+                            + "."
+                            + data[0]
+                            + "."
+                        )
                     total = 0
                     for index, value in enumerate(data):
-                        if index == 0 or index >= cpuCount + 1:
+                        if index == 0 or index >= cpu_count + 1:
                             continue
 
-                        metric_name_node = metric_name + 'CPU' + str(index - 1)
-                        value = int(self.derivative(metric_name_node,
-                                                    long(value), counter))
+                        metric_name_node = metric_name + "CPU" + str(index - 1)
+                        value = int(
+                            self.derivative(metric_name_node, int(value), counter)
+                        )
                         total += value
                         self.publish(metric_name_node, value)
 
                     # Roll up value
-                    metric_name_node = metric_name + 'total'
+                    metric_name_node = metric_name + "total"
                     self.publish(metric_name_node, total)
 
         # Close file
         file.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/cpu/cpu.py` & `diamond-next-5.0.0/src/collectors/cpu/cpu.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,263 +5,309 @@
 
 #### Dependencies
 
  * /proc/stat
 
 """
 
-import diamond.collector
 import os
+
 import time
-from diamond.collector import str_to_bool
+
+import diamond.collector
 
 try:
     import psutil
 except ImportError:
     psutil = None
 
 
 class CPUCollector(diamond.collector.Collector):
+    PROC = "/proc/stat"
 
-    PROC = '/proc/stat'
     INTERVAL = 1
 
     MAX_VALUES = {
-        'user': diamond.collector.MAX_COUNTER,
-        'nice': diamond.collector.MAX_COUNTER,
-        'system': diamond.collector.MAX_COUNTER,
-        'idle': diamond.collector.MAX_COUNTER,
-        'iowait': diamond.collector.MAX_COUNTER,
-        'irq': diamond.collector.MAX_COUNTER,
-        'softirq': diamond.collector.MAX_COUNTER,
-        'steal': diamond.collector.MAX_COUNTER,
-        'guest': diamond.collector.MAX_COUNTER,
-        'guest_nice': diamond.collector.MAX_COUNTER,
+        "user": diamond.collector.MAX_COUNTER,
+        "nice": diamond.collector.MAX_COUNTER,
+        "system": diamond.collector.MAX_COUNTER,
+        "idle": diamond.collector.MAX_COUNTER,
+        "iowait": diamond.collector.MAX_COUNTER,
+        "irq": diamond.collector.MAX_COUNTER,
+        "softirq": diamond.collector.MAX_COUNTER,
+        "steal": diamond.collector.MAX_COUNTER,
+        "guest": diamond.collector.MAX_COUNTER,
+        "guest_nice": diamond.collector.MAX_COUNTER,
     }
 
     def get_default_config_help(self):
         config_help = super(CPUCollector, self).get_default_config_help()
-        config_help.update({
-            'percore':  'Collect metrics per cpu core or just total',
-            'simple':   'only return aggregate CPU% metric',
-            'normalize': 'for cpu totals, divide by the number of CPUs',
-        })
+        config_help.update(
+            {
+                "percore": "Collect metrics per cpu core or just total",
+                "simple": "only return aggregate CPU% metric",
+                "normalize": "for cpu totals, divide by the number of CPUs",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(CPUCollector, self).get_default_config()
-        config.update({
-            'path':     'cpu',
-            'percore':  'True',
-            'xenfix':   None,
-            'simple':   'False',
-            'normalize': 'False',
-        })
+        config.update(
+            {
+                "path": "cpu",
+                "percore": "True",
+                "xenfix": None,
+                "simple": "False",
+                "normalize": "False",
+            }
+        )
+
         return config
 
     def collect(self):
         """
         Collector cpu stats
         """
 
         def cpu_time_list():
             """
             get cpu time list
             """
 
-            statFile = open(self.PROC, "r")
-            timeList = statFile.readline().split(" ")[2:6]
-            for i in range(len(timeList)):
-                timeList[i] = int(timeList[i])
-            statFile.close()
-            return timeList
+            stat_file = open(self.PROC, "r")
+            time_list = stat_file.readline().split(" ")[2:6]
+
+            for i in range(len(time_list)):
+                time_list[i] = int(time_list[i])
+
+            stat_file.close()
+
+            return time_list
 
         def cpu_delta_time(interval):
             """
             Get before and after cpu times for usage calc
             """
             pre_check = cpu_time_list()
             time.sleep(interval)
             post_check = cpu_time_list()
+
             for i in range(len(pre_check)):
                 post_check[i] -= pre_check[i]
+
             return post_check
 
         if os.access(self.PROC, os.R_OK):
-
             # If simple only return aggregate CPU% metric
-            if str_to_bool(self.config['simple']):
+            if diamond.collector.str_to_bool(self.config["simple"]):
                 dt = cpu_delta_time(self.INTERVAL)
-                cpuPct = 100 - (dt[len(dt) - 1] * 100.00 / sum(dt))
-                self.publish('percent', str('%.4f' % cpuPct))
+                cpu_pct = 100 - (dt[len(dt) - 1] * 100.00 / sum(dt))
+                self.publish("percent", str("%.4f" % cpu_pct))
+
                 return True
 
             results = {}
+
             # Open file
             file = open(self.PROC)
 
             ncpus = -1  # dont want to count the 'cpu'(total) cpu.
+
             for line in file:
-                if not line.startswith('cpu'):
+                if not line.startswith("cpu"):
                     continue
 
                 ncpus += 1
                 elements = line.split()
-
                 cpu = elements[0]
 
-                if cpu == 'cpu':
-                    cpu = 'total'
-                elif not str_to_bool(self.config['percore']):
+                if cpu == "cpu":
+                    cpu = "total"
+                elif not diamond.collector.str_to_bool(self.config["percore"]):
                     continue
 
                 results[cpu] = {}
 
                 if len(elements) >= 2:
-                    results[cpu]['user'] = elements[1]
+                    results[cpu]["user"] = elements[1]
+
                 if len(elements) >= 3:
-                    results[cpu]['nice'] = elements[2]
+                    results[cpu]["nice"] = elements[2]
+
                 if len(elements) >= 4:
-                    results[cpu]['system'] = elements[3]
+                    results[cpu]["system"] = elements[3]
+
                 if len(elements) >= 5:
-                    results[cpu]['idle'] = elements[4]
+                    results[cpu]["idle"] = elements[4]
+
                 if len(elements) >= 6:
-                    results[cpu]['iowait'] = elements[5]
+                    results[cpu]["iowait"] = elements[5]
+
                 if len(elements) >= 7:
-                    results[cpu]['irq'] = elements[6]
+                    results[cpu]["irq"] = elements[6]
+
                 if len(elements) >= 8:
-                    results[cpu]['softirq'] = elements[7]
+                    results[cpu]["softirq"] = elements[7]
+
                 if len(elements) >= 9:
-                    results[cpu]['steal'] = elements[8]
+                    results[cpu]["steal"] = elements[8]
+
                 if len(elements) >= 10:
-                    results[cpu]['guest'] = elements[9]
+                    results[cpu]["guest"] = elements[9]
+
                 if len(elements) >= 11:
-                    results[cpu]['guest_nice'] = elements[10]
+                    results[cpu]["guest_nice"] = elements[10]
 
             # Close File
             file.close()
 
-            metrics = {}
-            metrics['cpu_count'] = ncpus
+            metrics = {"cpu_count": ncpus}
 
             for cpu in results.keys():
                 stats = results[cpu]
+
                 for s in stats.keys():
                     # Get Metric Name
-                    metric_name = '.'.join([cpu, s])
+                    metric_name = ".".join([cpu, s])
+
                     # Get actual data
-                    if ((str_to_bool(self.config['normalize']) and
-                         cpu == 'total' and
-                         ncpus > 0)):
-                        metrics[metric_name] = self.derivative(
-                            metric_name,
-                            long(stats[s]),
-                            self.MAX_VALUES[s]) / ncpus
+                    if (
+                        diamond.collector.str_to_bool(self.config["normalize"])
+                        and cpu == "total"
+                        and ncpus > 0
+                    ):
+                        metrics[metric_name] = (
+                            self.derivative(
+                                metric_name, int(stats[s]), self.MAX_VALUES[s]
+                            )
+                            / ncpus
+                        )
                     else:
                         metrics[metric_name] = self.derivative(
-                            metric_name,
-                            long(stats[s]),
-                            self.MAX_VALUES[s])
+                            metric_name, int(stats[s]), self.MAX_VALUES[s]
+                        )
 
             # Check for a bug in xen where the idle time is doubled for guest
             # See https://bugzilla.redhat.com/show_bug.cgi?id=624756
-            if self.config['xenfix'] is None or self.config['xenfix'] is True:
-                if os.path.isdir('/proc/xen'):
+            if self.config["xenfix"] is None or self.config["xenfix"] is True:
+                if os.path.isdir("/proc/xen"):
                     total = 0
+
                     for metric_name in metrics.keys():
-                        if 'cpu0.' in metric_name:
+                        if "cpu0." in metric_name:
                             total += int(metrics[metric_name])
+
                     if total > 110:
-                        self.config['xenfix'] = True
+                        self.config["xenfix"] = True
+
                         for mname in metrics.keys():
-                            if '.idle' in mname:
+                            if ".idle" in mname:
                                 metrics[mname] = float(metrics[mname]) / 2
                     elif total > 0:
-                        self.config['xenfix'] = False
+                        self.config["xenfix"] = False
                 else:
-                    self.config['xenfix'] = False
+                    self.config["xenfix"] = False
 
             # Publish Metric Derivative
             for metric_name in metrics.keys():
-                self.publish(metric_name,
-                             metrics[metric_name],
-                             precision=2)
+                self.publish(metric_name, metrics[metric_name], precision=2)
+
             return True
 
         else:
             if not psutil:
-                self.log.error('Unable to import psutil')
-                self.log.error('No cpu metrics retrieved')
+                self.log.error("Unable to import psutil")
+                self.log.error("No cpu metrics retrieved")
+
                 return None
 
             cpu_time = psutil.cpu_times(True)
             cpu_count = len(cpu_time)
             total_time = psutil.cpu_times()
+
             for i in range(0, len(cpu_time)):
-                metric_name = 'cpu' + str(i)
+                metric_name = "cpu" + str(i)
                 self.publish(
-                    metric_name + '.user',
-                    self.derivative(metric_name + '.user',
-                                    cpu_time[i].user,
-                                    self.MAX_VALUES['user']),
-                    precision=2)
+                    metric_name + ".user",
+                    self.derivative(
+                        metric_name + ".user", cpu_time[i].user, self.MAX_VALUES["user"]
+                    ),
+                    precision=2,
+                )
 
-                if hasattr(cpu_time[i], 'nice'):
+                if hasattr(cpu_time[i], "nice"):
                     self.publish(
-                        metric_name + '.nice',
-                        self.derivative(metric_name + '.nice',
-                                        cpu_time[i].nice,
-                                        self.MAX_VALUES['nice']),
-                        precision=2)
+                        metric_name + ".nice",
+                        self.derivative(
+                            metric_name + ".nice",
+                            cpu_time[i].nice,
+                            self.MAX_VALUES["nice"],
+                        ),
+                        precision=2,
+                    )
 
                 self.publish(
-                    metric_name + '.system',
-                    self.derivative(metric_name + '.system',
-                                    cpu_time[i].system,
-                                    self.MAX_VALUES['system']),
-                    precision=2)
+                    metric_name + ".system",
+                    self.derivative(
+                        metric_name + ".system",
+                        cpu_time[i].system,
+                        self.MAX_VALUES["system"],
+                    ),
+                    precision=2,
+                )
 
                 self.publish(
-                    metric_name + '.idle',
-                    self.derivative(metric_name + '.idle',
-                                    cpu_time[i].idle,
-                                    self.MAX_VALUES['idle']),
-                    precision=2)
+                    metric_name + ".idle",
+                    self.derivative(
+                        metric_name + ".idle", cpu_time[i].idle, self.MAX_VALUES["idle"]
+                    ),
+                    precision=2,
+                )
 
-            metric_name = 'total'
+            metric_name = "total"
             self.publish(
-                metric_name + '.user',
-                self.derivative(metric_name + '.user',
-                                total_time.user,
-                                self.MAX_VALUES['user']) / cpu_count,
-                precision=2)
+                metric_name + ".user",
+                self.derivative(
+                    metric_name + ".user", total_time.user, self.MAX_VALUES["user"]
+                )
+                / cpu_count,
+                precision=2,
+            )
 
-            if hasattr(total_time, 'nice'):
+            if hasattr(total_time, "nice"):
                 self.publish(
-                    metric_name + '.nice',
-                    self.derivative(metric_name + '.nice',
-                                    total_time.nice,
-                                    self.MAX_VALUES['nice']) / cpu_count,
-                    precision=2)
+                    metric_name + ".nice",
+                    self.derivative(
+                        metric_name + ".nice", total_time.nice, self.MAX_VALUES["nice"]
+                    )
+                    / cpu_count,
+                    precision=2,
+                )
 
             self.publish(
-                metric_name + '.system',
-                self.derivative(metric_name + '.system',
-                                total_time.system,
-                                self.MAX_VALUES['system']) / cpu_count,
-                precision=2)
+                metric_name + ".system",
+                self.derivative(
+                    metric_name + ".system",
+                    total_time.system,
+                    self.MAX_VALUES["system"],
+                )
+                / cpu_count,
+                precision=2,
+            )
 
             self.publish(
-                metric_name + '.idle',
-                self.derivative(metric_name + '.idle',
-                                total_time.idle,
-                                self.MAX_VALUES['idle']) / cpu_count,
-                precision=2)
+                metric_name + ".idle",
+                self.derivative(
+                    metric_name + ".idle", total_time.idle, self.MAX_VALUES["idle"]
+                )
+                / cpu_count,
+                precision=2,
+            )
 
-            self.publish('cpu_count', psutil.cpu_count())
+            self.publish("cpu_count", psutil.cpu_count())
 
             return True
-
-        return None
```

### Comparing `diamond-next-4.0.515/src/collectors/celerymon/celerymon.py` & `diamond-next-5.0.0/src/collectors/celerymon/celerymon.py`

 * *Files 13% similar despite different names*

```diff
@@ -13,84 +13,94 @@
 enabled=True
 host=celerymon.example.com
 port=16379
 ```
 
 """
 
-import diamond.collector
-import urllib2
+import urllib.request
+
 import time
 
+import diamond.collector
+
 try:
     import json
 except ImportError:
     import simplejson as json
 
 
 class CelerymonCollector(diamond.collector.Collector):
-
     LastCollectTime = None
 
     def get_default_config_help(self):
         config_help = super(CelerymonCollector, self).get_default_config_help()
-        config_help.update({
-            'path': 'celerymon',
-            'host': 'A single hostname to get metrics from',
-            'port': 'The celerymon port'
+        config_help.update(
+            {
+                "path": "celerymon",
+                "host": "A single hostname to get metrics from",
+                "port": "The celerymon port",
+            }
+        )
 
-        })
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(CelerymonCollector, self).get_default_config()
-        config.update({
-            'host':     'localhost',
-            'port':     '8989'
-        })
+        config.update({"host": "localhost", "port": "8989"})
         return config
 
     def collect(self):
         """
         Overrides the Collector.collect method
         """
 
         # Handle collection time intervals correctly
-        CollectTime = int(time.time())
-        time_delta = float(self.config['interval'])
+        collect_time = int(time.time())
+        time_delta = float(self.config["interval"])
+
         if not self.LastCollectTime:
-            self.LastCollectTime = CollectTime - time_delta
+            self.LastCollectTime = collect_time - time_delta
 
-        host = self.config['host']
-        port = self.config['port']
+        host = self.config["host"]
+        port = self.config["port"]
 
         celerymon_url = "http://%s:%s/api/task/?since=%i" % (
-            host, port, self.LastCollectTime)
-        response = urllib2.urlopen(celerymon_url)
+            host,
+            port,
+            self.LastCollectTime,
+        )
+        response = urllib.request.urlopen(celerymon_url)
         body = response.read()
         celery_data = json.loads(body)
 
         results = dict()
         total_messages = 0
+
         for data in celery_data:
-            name = str(data[1]['name'])
+            name = str(data[1]["name"])
+
             if name not in results:
                 results[name] = dict()
-            state = str(data[1]['state'])
+
+            state = str(data[1]["state"])
+
             if state not in results[name]:
                 results[name][state] = 1
             else:
                 results[name][state] += 1
+
             total_messages += 1
 
         # Publish Metric
-        self.publish('total_messages', total_messages)
+        self.publish("total_messages", total_messages)
+
         for result in results:
             for state in results[result]:
                 metric_value = results[result][state]
                 metric_name = "%s.%s" % (result, state)
                 self.publish(metric_name, metric_value)
 
-        self.LastCollectTime = CollectTime
+        self.LastCollectTime = collect_time
```

### Comparing `diamond-next-4.0.515/src/collectors/s3/s3.py` & `diamond-next-5.0.0/src/collectors/s3/s3.py`

 * *Files 13% similar despite different names*

```diff
@@ -5,70 +5,74 @@
 
 #### Dependencies
 
   * boto (https://github.com/boto/boto)
 """
 
 import diamond.collector
+import diamond.convertor
+
 try:
     import boto
+
     boto
     from boto.s3.connection import S3Connection
 except ImportError:
     boto = None
 
 
 class S3BucketCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(S3BucketCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(S3BucketCollector, self).get_default_config()
-        config.update({
-            'path':      'aws.s3',
-            'byte_unit': 'byte'
-        })
+        config.update({"path": "aws.s3", "byte_unit": "byte"})
+
         return config
 
     def getBucket(self, aws_access, aws_secret, bucket_name):
-        self.log.info("S3: Open Bucket, %s, %s, %s" % (bucket_name, aws_access,
-                                                       aws_secret))
+        self.log.info(
+            "S3: Open Bucket, %s, %s, %s" % (bucket_name, aws_access, aws_secret)
+        )
         s3 = S3Connection(aws_access, aws_secret)
+
         return s3.lookup(bucket_name)
 
     def getBucketSize(self, bucket):
         total_bytes = 0
+
         for key in bucket:
             total_bytes += key.size
+
         return total_bytes
 
     def collect(self):
         """
         Collect s3 bucket stats
         """
         if boto is None:
             self.log.error("Unable to import boto python module")
             return {}
-        for s3instance in self.config['s3']:
-            self.log.info("S3: byte_unit: %s" % self.config['byte_unit'])
-            aws_access = self.config['s3'][s3instance]['aws_access_key']
-            aws_secret = self.config['s3'][s3instance]['aws_secret_key']
-            for bucket_name in self.config['s3'][s3instance]['buckets']:
+
+        for s3instance in self.config["s3"]:
+            self.log.info("S3: byte_unit: %s" % self.config["byte_unit"])
+            aws_access = self.config["s3"][s3instance]["aws_access_key"]
+            aws_secret = self.config["s3"][s3instance]["aws_secret_key"]
+
+            for bucket_name in self.config["s3"][s3instance]["buckets"]:
                 bucket = self.getBucket(aws_access, aws_secret, bucket_name)
 
                 # collect bucket size
                 total_size = self.getBucketSize(bucket)
-                for byte_unit in self.config['byte_unit']:
+
+                for byte_unit in self.config["byte_unit"]:
                     new_size = diamond.convertor.binary.convert(
-                        value=total_size,
-                        oldUnit='byte',
-                        newUnit=byte_unit
+                        value=total_size, old_unit="byte", new_unit=byte_unit
                     )
-                    self.publish("%s.size.%s" % (bucket_name, byte_unit),
-                                 new_size)
+                    self.publish("%s.size.%s" % (bucket_name, byte_unit), new_size)
```

### Comparing `diamond-next-4.0.515/src/collectors/snmpraw/snmpraw.py` & `diamond-next-5.0.0/src/collectors/snmpraw/snmpraw.py`

 * *Files 16% similar despite different names*

```diff
@@ -50,138 +50,157 @@
 #### Dependencies
 
  * pysmnp (which depends on pyasn1 0.1.7 and pycrypto)
 
 """
 
 import os
+
 import sys
 import time
 
-sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)),
-                                'snmp'))
+import diamond.metric
+
+sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), "snmp"))
+
 from snmp import SNMPCollector as parent_SNMPCollector
-from diamond.metric import Metric
 
 
 class SNMPRawCollector(parent_SNMPCollector):
-
     def process_config(self):
         super(SNMPRawCollector, self).process_config()
         # list to save non-existing oid's per device, to avoid repetition of
         # errors in logging. Signal HUP to diamond/collector to flush this
         self.skip_list = []
 
     def get_default_config(self):
         """
         Override SNMPCollector.get_default_config method to provide
         default_config for the SNMPInterfaceCollector
         """
-        default_config = super(SNMPRawCollector,
-                               self).get_default_config()
-        default_config.update({
-            'oids': {},
-            'path_prefix': 'servers',
-            'path_suffix': 'snmp',
-        })
+        default_config = super(SNMPRawCollector, self).get_default_config()
+        default_config.update(
+            {
+                "oids": {},
+                "path_prefix": "servers",
+                "path_suffix": "snmp",
+            }
+        )
+
         return default_config
 
     def _precision(self, value):
         """
         Return the precision of the number
         """
         value = str(value)
-        decimal = value.rfind('.')
+        decimal = value.rfind(".")
+
         if decimal == -1:
             return 0
+
         return len(value) - decimal - 1
 
     def _skip(self, device, oid, reason=None):
         self.skip_list.append((device, oid))
+
         if reason is not None:
-            self.log.warn('Muted \'{0}\' on \'{1}\', because: {2}'.format(
-                oid, device, reason))
+            self.log.warn("Muted '{}' on '{}', because: {}".format(oid, device, reason))
 
     def _get_value_walk(self, device, oid, host, port, community):
         data = self.walk(oid, host, port, community)
 
         if data is None:
-            self._skip(device, oid, 'device down (#2)')
+            self._skip(device, oid, "device down (#2)")
             return
 
-        self.log.debug('Data received from WALK \'{0}\': [{1}]'.format(
-            device, data))
+        self.log.debug("Data received from WALK '{}': [{}]".format(device, data))
 
         if len(data) != 1:
             self._skip(
                 device,
                 oid,
-                'unexpected response, data has {0} entries'.format(
-                    len(data)))
+                "unexpected response, data has {} entries".format(len(data)),
+            )
             return
 
         # because we only allow 1-key dicts, we can pick with absolute index
         value = data.items()[0][1]
         return value
 
     def _get_value(self, device, oid, host, port, community):
         data = self.get(oid, host, port, community)
 
         if data is None:
-            self._skip(device, oid, 'device down (#1)')
+            self._skip(device, oid, "device down (#1)")
             return
 
-        self.log.debug('Data received from GET \'{0}\': [{1}]'.format(
-            device, data))
+        self.log.debug("Data received from GET '{}': [{}]".format(device, data))
 
         if len(data) == 0:
-            self._skip(device, oid, 'empty response, device down?')
+            self._skip(device, oid, "empty response, device down?")
             return
 
         if oid not in data:
             # oid is not even in hierarchy, happens when using 9.9.9.9
             # but not when using 1.9.9.9
-            self._skip(device, oid, 'no object at OID (#1)')
+            self._skip(device, oid, "no object at OID (#1)")
             return
 
         value = data[oid]
-        if value == 'No Such Object currently exists at this OID':
-            self._skip(device, oid, 'no object at OID (#2)')
+
+        if value == "No Such Object currently exists at this OID":
+            self._skip(device, oid, "no object at OID (#2)")
             return
 
-        if value == 'No Such Instance currently exists at this OID':
+        if value == "No Such Instance currently exists at this OID":
             return self._get_value_walk(device, oid, host, port, community)
 
         return value
 
     def collect_snmp(self, device, host, port, community):
         """
         Collect SNMP interface data from device
         """
-        self.log.debug(
-            'Collecting raw SNMP statistics from device \'{0}\''.format(device))
+        self.log.debug("Collecting raw SNMP statistics from device '{}'".format(device))
 
-        dev_config = self.config['devices'][device]
-        if 'oids' in dev_config:
-            for oid, metricName in dev_config['oids'].items():
+        dev_config = self.config["devices"][device]
+
+        if "oids" in dev_config:
+            for oid, metricName in dev_config["oids"].items():
 
                 if (device, oid) in self.skip_list:
                     self.log.debug(
-                        'Skipping OID \'{0}\' ({1}) on device \'{2}\''.format(
-                            oid, metricName, device))
+                        "Skipping OID '{}' ({}) on device '{}'".format(
+                            oid, metricName, device
+                        )
+                    )
                     continue
 
                 timestamp = time.time()
                 value = self._get_value(device, oid, host, port, community)
+
                 if value is None:
                     continue
 
                 self.log.debug(
-                    '\'{0}\' ({1}) on device \'{2}\' - value=[{3}]'.format(
-                        oid, metricName, device, value))
-
-                path = '.'.join([self.config['path_prefix'], device,
-                                 self.config['path_suffix'], metricName])
-                metric = Metric(path=path, value=value, timestamp=timestamp,
-                                precision=self._precision(value),
-                                metric_type='GAUGE')
+                    "'{}' ({}) on device '{}' - value=[{}]".format(
+                        oid, metricName, device, value
+                    )
+                )
+
+                path = ".".join(
+                    [
+                        self.config["path_prefix"],
+                        device,
+                        self.config["path_suffix"],
+                        metricName,
+                    ]
+                )
+                metric = diamond.metric.Metric(
+                    path=path,
+                    value=value,
+                    timestamp=timestamp,
+                    precision=self._precision(value),
+                    metric_type="GAUGE",
+                )
                 self.publish_metric(metric)
```

### Comparing `diamond-next-4.0.515/src/collectors/openstackswift/openstackswift.py` & `diamond-next-5.0.0/src/collectors/openstackswift/openstackswift.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,86 +10,100 @@
    diamond also get an idea of the runtime of a swift-dispersion-report call
    and make sure the collect interval is high enough to avoid contention.
  * swift commandline tool (for container_metrics)
 
 both of these should come installed with swift
 """
 
+import subprocess
+
 import diamond.collector
-from subprocess import Popen, PIPE
 
 try:
     import json
 except ImportError:
     import simplejson as json
 
 
 class OpenstackSwiftCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(OpenstackSwiftCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'enable_dispersion_report': 'gather swift-dispersion-report ' +
-                                        'metrics (default False)',
-            'enable_container_metrics': 'gather containers metrics ' +
-                                        '(# objects, bytes used, ' +
-                                        'x_timestamp. default True)',
-            'auth_url': 'authentication url (for enable_container_metrics)',
-            'account': 'swift auth account (for enable_container_metrics)',
-            'user': 'swift auth user (for enable_container_metrics)',
-            'password': 'swift auth password (for enable_container_metrics)',
-            'containers': 'containers on which to count number of objects, ' +
-                          'space separated list (for enable_container_metrics)'
-        })
+        config_help = super(OpenstackSwiftCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "enable_dispersion_report": "gather swift-dispersion-report metrics (default False)",
+                "enable_container_metrics": "gather containers metrics (# objects, bytes used, x_timestamp. default True)",
+                "auth_url": "authentication url (for enable_container_metrics)",
+                "account": "swift auth account (for enable_container_metrics)",
+                "user": "swift auth user (for enable_container_metrics)",
+                "password": "swift auth password (for enable_container_metrics)",
+                "containers": "containers on which to count number of objects, space separated list (for enable_container_metrics)",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(OpenstackSwiftCollector, self).get_default_config()
-        config.update({
-            'path': 'openstackswift',
-            'enable_dispersion_report': False,
-            'enable_container_metrics': True,
-            # don't use the threaded model with this one.
-            # for some reason it crashes.
-            'interval': 1200,  # by default, every 20 minutes
-        })
+        config.update(
+            {
+                "path": "openstackswift",
+                "enable_dispersion_report": False,
+                "enable_container_metrics": True,
+                # don't use the threaded model with this one.
+                # for some reason it crashes.
+                "interval": 1200,  # by default, every 20 minutes
+            }
+        )
+
         return config
 
     def collect(self):
         # dispersion report.  this can take easily >60s. beware!
-        if (self.config['enable_dispersion_report']):
-            p = Popen(
-                ['swift-dispersion-report', '-j'],
-                stdout=PIPE,
-                stderr=PIPE)
+        if self.config["enable_dispersion_report"]:
+            p = subprocess.Popen(
+                ["swift-dispersion-report", "-j"],
+                stdout=subprocess.PIPE,
+                stderr=subprocess.PIPE,
+            )
             stdout, stderr = p.communicate()
-            self.publish('dispersion.errors', len(stderr.split('\n')) - 1)
+            self.publish("dispersion.errors", len(stderr.split("\n")) - 1)
             data = json.loads(stdout)
-            for t in ('object', 'container'):
+
+            for t in ("object", "container"):
                 for (k, v) in data[t].items():
-                    self.publish('dispersion.%s.%s' % (t, k), v)
+                    self.publish("dispersion.%s.%s" % (t, k), v)
 
         # container metrics returned by stat <container>
-        if(self.config['enable_container_metrics']):
-            account = '%s:%s' % (self.config['account'], self.config['user'])
-            for container in self.config['containers'].split(','):
-                cmd = ['swift', '-A', self.config['auth_url'],
-                       '-U', account,
-                       '-K', self.config['password'],
-                       'stat', container]
-                p = Popen(cmd, stdout=PIPE, stderr=PIPE)
+        if self.config["enable_container_metrics"]:
+            account = "%s:%s" % (self.config["account"], self.config["user"])
+
+            for container in self.config["containers"].split(","):
+                cmd = [
+                    "swift",
+                    "-A",
+                    self.config["auth_url"],
+                    "-U",
+                    account,
+                    "-K",
+                    self.config["password"],
+                    "stat",
+                    container,
+                ]
+                p = subprocess.Popen(
+                    cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
+                )
                 stdout, stderr = p.communicate()
                 stats = {}
+
                 # stdout is some lines in 'key   : val' format
-                for line in stdout.split('\n'):
+                for line in stdout.split("\n"):
                     if line:
-                        line = line.split(':', 2)
+                        line = line.split(":", 2)
                         stats[line[0].strip()] = line[1].strip()
-                key = 'container_metrics.%s.%s' % (self.config['account'],
-                                                   container)
-                self.publish('%s.objects' % key, stats['Objects'])
-                self.publish('%s.bytes' % key, stats['Bytes'])
-                self.publish('%s.x_timestamp' % key, stats['X-Timestamp'])
+
+                key = "container_metrics.%s.%s" % (self.config["account"], container)
+                self.publish("%s.objects" % key, stats["Objects"])
+                self.publish("%s.bytes" % key, stats["Bytes"])
+                self.publish("%s.x_timestamp" % key, stats["X-Timestamp"])
```

### Comparing `diamond-next-4.0.515/src/collectors/amavis/amavis.py` & `diamond-next-5.0.0/src/collectors/amavis/amavis.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,99 +6,120 @@
 #### Dependencies
 
 * amavisd-agent must be present in PATH
 
 """
 
 import os
-import subprocess
 import re
+import subprocess
 
 import diamond.collector
-import diamond.convertor
-from diamond.collector import str_to_bool
 
 
 class AmavisCollector(diamond.collector.Collector):
     # From the source of amavisd-agent and it seems like the three interesting
     # formats are these:  ("x y/h", "xMB yMB/h", "x s y s/msg"),
     # so this, ugly as it is to hardcode it this way, it should be right.
     #
     # The other option would be to directly read and decode amavis' berkeley
     # db, and I don't even want to get there
 
     matchers = [
-        re.compile(r'^\s*(?P<name>sysUpTime)\s+TimeTicks\s+(?P<time>\d+)\s+'
-                   r'\([\w:\., ]+\)\s*$'),
-        re.compile(r'^\s*(?P<name>[\w]+)\s+(?P<time>[\d]+) s\s+'
-                   r'(?P<frequency>[\d.]+) s/msg\s+\([\w]+\)\s*$'),
-        re.compile(r'^\s*(?P<name>[\w.-]+)\s+(?P<count>[\d]+)\s+'
-                   r'(?P<frequency>[\d.]+)/h\s+(?P<percentage>[\d.]+) %'
-                   r'\s\([\w]+\)\s*$'),
-        re.compile(r'^\s*(?P<name>[\w.-]+)\s+(?P<size>[\d]+)MB\s+'
-                   r'(?P<frequency>[\d.]+)MB/h\s+(?P<percentage>[\d.]+) %'
-                   r'\s\([\w]+\)\s*$'),
+        re.compile(
+            r"^\s*(?P<name>[\w]+)\s+(?P<time>[\d]+) s\s+"
+            r"(?P<frequency>[\d.]+) s/msg\s+\([\w]+\)\s*$"
+        ),
+        re.compile(
+            r"^\s*(?P<name>[\w.-]+)\s+(?P<count>[\d]+)\s+"
+            r"(?P<frequency>[\d.]+)/h\s+(?P<percentage>[\d.]+) %"
+            r"\s\([\w]+\)\s*$"
+        ),
+        re.compile(
+            r"^\s*(?P<name>[\w.-]+)\s+(?P<size>[\d]+)MB\s+"
+            r"(?P<frequency>[\d.]+)MB/h\s+(?P<percentage>[\d.]+) %"
+            r"\s\([\w]+\)\s*$"
+        ),
     ]
 
     def get_default_config_help(self):
         config_help = super(AmavisCollector, self).get_default_config_help()
-        config_help.update({
-            'amavisd_exe': 'The path to amavisd-agent',
-            'use_sudo': 'Call amavisd-agent using sudo',
-            'sudo_exe': 'The path to sudo',
-            'sudo_user': 'The user to use if using sudo',
-        })
+        config_help.update(
+            {
+                "amavisd_exe": "The path to amavisd-agent",
+                "use_sudo": "Call amavisd-agent using sudo",
+                "sudo_exe": "The path to sudo",
+                "sudo_user": "The user to use if using sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         config = super(AmavisCollector, self).get_default_config()
-        config.update({
-            'path': 'amavis',
-            'amavisd_exe': '/usr/sbin/amavisd-agent',
-            'use_sudo': False,
-            'sudo_exe': '/usr/bin/sudo',
-            'sudo_user': 'amavis',
-        })
+        config.update(
+            {
+                "path": "amavis",
+                "amavisd_exe": "/usr/sbin/amavisd-agent",
+                "use_sudo": False,
+                "sudo_exe": "/usr/bin/sudo",
+                "sudo_user": "amavis",
+            }
+        )
+
         return config
 
     def collect(self):
         """
         Collect memory stats
         """
         try:
-            if str_to_bool(self.config['use_sudo']):
+            if diamond.collector.str_to_bool(self.config["use_sudo"]):
                 # Use -u instead of --user as the former is more portable. Not
                 # all versions of sudo support the long form --user.
                 cmdline = [
-                    self.config['sudo_exe'], '-u', self.config['sudo_user'],
-                    '--', self.config['amavisd_exe'], '-c', '1'
+                    self.config["sudo_exe"],
+                    "-u",
+                    self.config["sudo_user"],
+                    "--",
+                    self.config["amavisd_exe"],
+                    "-c",
+                    "1",
                 ]
             else:
-                cmdline = [self.config['amavisd_exe'], '-c', '1']
+                cmdline = [self.config["amavisd_exe"], "-c", "1"]
+
             agent = subprocess.Popen(cmdline, stdout=subprocess.PIPE)
             agent_out = agent.communicate()[0]
             lines = agent_out.strip().split(os.linesep)
+
             for line in lines:
                 for rex in self.matchers:
                     res = rex.match(line)
+
                     if res:
                         groups = res.groupdict()
-                        name = groups['name']
+                        name = groups["name"]
+
                         for metric, value in groups.items():
-                            if metric == 'name':
+                            if metric == "name":
                                 continue
-                            mtype = 'GAUGE'
+
+                            mtype = "GAUGE"
                             precision = 2
-                            if metric in ('count', 'time'):
-                                mtype = 'COUNTER'
+
+                            if metric in ("count", "time"):
+                                mtype = "COUNTER"
                                 precision = 0
-                            self.publish("{0}.{1}".format(name, metric),
-                                         value, metric_type=mtype,
-                                         precision=precision)
+
+                            self.publish(
+                                "{}.{}".format(name, metric),
+                                value,
+                                metric_type=mtype,
+                                precision=precision,
+                            )
 
         except OSError as err:
-            self.log.error("Could not run %s: %s",
-                           self.config['amavisd_exe'],
-                           err)
+            self.log.error("Could not run %s: %s", self.config["amavisd_exe"], err)
             return None
 
         return True
```

### Comparing `diamond-next-4.0.515/src/collectors/memory/memory.py` & `diamond-next-5.0.0/src/collectors/memory/memory.py`

 * *Files 23% similar despite different names*

```diff
@@ -9,136 +9,172 @@
 
 #### Dependencies
 
 * /proc/meminfo or psutil
 
 """
 
+import decimal
+import os
+
 import diamond.collector
 import diamond.convertor
-import os
 
 try:
     import psutil
+
     psutil  # workaround for pyflakes issue #13
 except ImportError:
     psutil = None
 
 _KEY_MAPPING = [
-    'MemTotal',
-    'MemFree',
-    'MemAvailable',  # needs kernel 3.14
-    'Buffers',
-    'Cached',
-    'Active',
-    'Dirty',
-    'Inactive',
-    'Shmem',
-    'SwapTotal',
-    'SwapFree',
-    'SwapCached',
-    'VmallocTotal',
-    'VmallocUsed',
-    'VmallocChunk',
-    'Committed_AS',
+    "MemTotal",
+    "MemFree",
+    "MemAvailable",  # needs kernel 3.14
+    "Buffers",
+    "Cached",
+    "Active",
+    "Dirty",
+    "Inactive",
+    "Shmem",
+    "SwapTotal",
+    "SwapFree",
+    "SwapCached",
+    "VmallocTotal",
+    "VmallocUsed",
+    "VmallocChunk",
+    "Committed_AS",
 ]
 
 
 class MemoryCollector(diamond.collector.Collector):
 
-    PROC = '/proc/meminfo'
+    PROC = "/proc/meminfo"
 
     def get_default_config_help(self):
         config_help = super(MemoryCollector, self).get_default_config_help()
-        config_help.update({
-            'detailed': 'Set to True to Collect all the nodes',
-        })
+        config_help.update(
+            {
+                "detailed": "Set to True to Collect all the nodes",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(MemoryCollector, self).get_default_config()
-        config.update({
-            'path': 'memory',
-            'method': 'Threaded',
-            'force_psutil': 'False'
-            # Collect all the nodes or just a few standard ones?
-            # Uncomment to enable
-            # 'detailed': 'True'
-        })
+        config.update(
+            {
+                "path": "memory",
+                "method": "Threaded",
+                "force_psutil": "False"
+                # Collect all the nodes or just a few standard ones?
+                # Uncomment to enable
+                # 'detailed': 'True'
+            }
+        )
         return config
 
     def collect(self):
         """
         Collect memory stats
         """
-        if ((os.access(self.PROC, os.R_OK) and
-             self.config.get('force_psutil') != 'True')):
+        if os.access(self.PROC, os.R_OK) and self.config.get("force_psutil") != "True":
             file = open(self.PROC)
             data = file.read()
             file.close()
 
+            memory_total = None
+            memory_available = None
+
             for line in data.splitlines():
                 try:
                     name, value, units = line.split()
-                    name = name.rstrip(':')
+                    name = name.rstrip(":")
                     value = int(value)
 
-                    if ((name not in _KEY_MAPPING and
-                         'detailed' not in self.config)):
+                    if name not in _KEY_MAPPING and "detailed" not in self.config:
                         continue
 
-                    for unit in self.config['byte_unit']:
-                        value = diamond.convertor.binary.convert(value=value,
-                                                                 oldUnit=units,
-                                                                 newUnit=unit)
-                        self.publish(name, value, metric_type='GAUGE')
+                    if name in "MemTotal":
+                        memory_total = value
+                    elif name in "MemAvailable":
+                        memory_available = value
+
+                    for unit in self.config["byte_unit"]:
+                        value = diamond.convertor.binary.convert(
+                            value=value, old_unit=units, new_unit=unit
+                        )
+                        self.publish(name, value, metric_type="GAUGE")
 
                         # TODO: We only support one unit node here. Fix it!
                         break
 
                 except ValueError:
                     continue
+
+            if memory_total is not None and memory_available is not None:
+                memory_used = memory_total - memory_available
+                memory_used_percent = decimal.Decimal(
+                    str(100.0 * memory_used / memory_total)
+                )
+                self.publish(
+                    "MemUsedPercentage",
+                    round(memory_used_percent, 2),
+                    metric_type="GAUGE",
+                )
+
             return True
         else:
             if not psutil:
-                self.log.error('Unable to import psutil')
-                self.log.error('No memory metrics retrieved')
-                return None
-
-            # psutil.phymem_usage() and psutil.virtmem_usage() are deprecated.
-            if hasattr(psutil, "phymem_usage"):
-                phymem_usage = psutil.phymem_usage()
-                virtmem_usage = psutil.virtmem_usage()
-            else:
-                phymem_usage = psutil.virtual_memory()
-                virtmem_usage = psutil.swap_memory()
-
-            units = 'B'
+                self.log.error("Unable to import psutil")
+                self.log.error("No memory metrics retrieved")
 
-            for unit in self.config['byte_unit']:
-                value = diamond.convertor.binary.convert(
-                    value=phymem_usage.total, oldUnit=units, newUnit=unit)
-                self.publish('MemTotal', value, metric_type='GAUGE')
+                return None
 
-                value = diamond.convertor.binary.convert(
-                    value=phymem_usage.available, oldUnit=units, newUnit=unit)
-                self.publish('MemAvailable', value, metric_type='GAUGE')
+            phymem_usage = psutil.virtual_memory()
+            virtmem_usage = psutil.swap_memory()
+            units = "B"
+
+            for unit in self.config["byte_unit"]:
+                memory_total = value = diamond.convertor.binary.convert(
+                    value=phymem_usage.total, old_unit=units, new_unit=unit
+                )
+                self.publish("MemTotal", value, metric_type="GAUGE")
+
+                memory_available = value = diamond.convertor.binary.convert(
+                    value=phymem_usage.available, old_unit=units, new_unit=unit
+                )
+                self.publish("MemAvailable", value, metric_type="GAUGE")
+
+                memory_used = memory_total - memory_available
+
+                memory_used_percent = decimal.Decimal(
+                    str(100.0 * memory_used / memory_total)
+                )
+                self.publish(
+                    "MemUsedPercentage",
+                    round(memory_used_percent, 2),
+                    metric_type="GAUGE",
+                )
 
                 value = diamond.convertor.binary.convert(
-                    value=phymem_usage.free, oldUnit=units, newUnit=unit)
-                self.publish('MemFree', value, metric_type='GAUGE')
+                    value=phymem_usage.free, old_unit=units, new_unit=unit
+                )
+                self.publish("MemFree", value, metric_type="GAUGE")
 
                 value = diamond.convertor.binary.convert(
-                    value=virtmem_usage.total, oldUnit=units, newUnit=unit)
-                self.publish('SwapTotal', value, metric_type='GAUGE')
+                    value=virtmem_usage.total, old_unit=units, new_unit=unit
+                )
+                self.publish("SwapTotal", value, metric_type="GAUGE")
 
                 value = diamond.convertor.binary.convert(
-                    value=virtmem_usage.free, oldUnit=units, newUnit=unit)
-                self.publish('SwapFree', value, metric_type='GAUGE')
+                    value=virtmem_usage.free, old_unit=units, new_unit=unit
+                )
+                self.publish("SwapFree", value, metric_type="GAUGE")
 
                 # TODO: We only support one unit node here. Fix it!
                 break
 
             return True
```

### Comparing `diamond-next-4.0.515/src/collectors/cpuacct_cgroup/cpuacct_cgroup.py` & `diamond-next-5.0.0/src/collectors/fluentd/fluentd.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,69 +1,67 @@
 # coding=utf-8
 
 """
-The CpuAcctCGroupCollector collects CPU Acct metric for cgroups
+The FlutendCollector monitors fluentd and data about the kinesis stream.
 
 #### Dependencies
 
-A mounted cgroup fs. Defaults to /sys/fs/cgroup/cpuacct/
+ * flutend
+
+#### Example config
+
+```
+    enabled = True
+    host = localhost
+    port = 24220
+    [[[collect]]]
+    kinesis = buffer_queue_length, buffer_total_queued_size, retry_count
+```
 
 """
 
+import json
+import urllib.request
+
 import diamond.collector
-import os
 
 
-class CpuAcctCgroupCollector(diamond.collector.Collector):
+class FluentdCollector(diamond.collector.Collector):
+    API_PATH = "/api/plugins.json"
 
     def get_default_config_help(self):
-        config_help = super(
-            CpuAcctCgroupCollector, self).get_default_config_help()
-        config_help.update({
-            'path': """Directory path to where cpuacct is located,
-defaults to /sys/fs/cgroup/cpuacct/. Redhat/CentOS/SL use /cgroup"""
-        })
+        config_help = super(FluentdCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "Fluentd host",
+                "port": "Fluentd port",
+                "collect": "Plugins and their metrics to collect",
+            }
+        )
         return config_help
 
     def get_default_config(self):
-        """
-        Returns the default collector settings
-        """
-        config = super(CpuAcctCgroupCollector, self).get_default_config()
-        config.update({
-            'path':     '/sys/fs/cgroup/cpuacct/'
-        })
+        config = super(FluentdCollector, self).get_default_config()
+        config.update(
+            {"host": "localhost", "port": "24220", "path": "fluentd", "collect": {}}
+        )
         return config
 
     def collect(self):
-        # find all cpuacct.stat files
-        matches = []
-        for root, dirnames, filenames in os.walk(self.config['path']):
-            for filename in filenames:
-                if filename == 'cpuacct.stat':
-                    # matches will contain a tuple contain path to cpuacct.stat
-                    # and the parent of the stat
-                    parent = root.replace(self.config['path'],
-                                          "").replace("/", ".")
-                    if parent == '':
-                        parent = 'system'
-                    # If the parent starts with a dot, remove it
-                    if parent[0] == '.':
-                        parent = parent[1:]
-                    matches.append((parent, os.path.join(root, filename)))
-
-        # Read utime and stime from cpuacct files
-        results = {}
-        for match in matches:
-            results[match[0]] = {}
-            stat_file = open(match[1])
-            elements = [line.split() for line in stat_file]
-            for el in elements:
-                results[match[0]][el[0]] = el[1]
-                stat_file.close()
-
-        # create metrics from collected utimes and stimes for cgroups
-        for parent, cpuacct in results.iteritems():
-            for key, value in cpuacct.iteritems():
-                metric_name = '.'.join([parent, key])
-                self.publish(metric_name, value, metric_type='GAUGE')
-        return True
+        params = (self.config["host"], self.config["port"], self.API_PATH)
+        url = "http://%s:%s/%s" % params
+
+        res = urllib.request.urlopen(url)
+        data = json.load(res)
+
+        result = self.parse_api_output(data)
+        for r in result:
+            self.publish(r[0], r[1])
+
+    def parse_api_output(self, status):
+        result = []
+        for p in status.get("plugins"):
+            if p["type"] in self.config["collect"].keys():
+                for m in self.config["collect"].get(p["type"]):
+                    tag = ".".join([p["type"], m])
+                    result.append((tag, p.get(m)))
+        return result
```

### Comparing `diamond-next-4.0.515/src/collectors/conntrack/conntrack.py` & `diamond-next-5.0.0/src/collectors/conntrack/conntrack.py`

 * *Files 5% similar despite different names*

```diff
@@ -6,84 +6,91 @@
 
 #### Dependencies
 
  * nf_conntrack/ip_conntrack kernel module
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
+
 
 class ConnTrackCollector(diamond.collector.Collector):
     """
     Collector of number of conntrack connections
     """
 
     def get_default_config_help(self):
         """
         Return help text for collector configuration
         """
         config_help = super(ConnTrackCollector, self).get_default_config_help()
-        config_help.update({
-            "dir":      "Directories with files of interest, comma seperated",
-            "files":    "List of files to collect statistics from",
-        })
+        config_help.update(
+            {
+                "dir": "Directories with files of interest, comma seperated",
+                "files": "List of files to collect statistics from",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ConnTrackCollector, self).get_default_config()
-        config.update({
-            "path":  "conntrack",
-            "dir":   "/proc/sys/net/ipv4/netfilter,/proc/sys/net/netfilter",
-            "files": "ip_conntrack_count,ip_conntrack_max,"
-                     "nf_conntrack_count,nf_conntrack_max",
-        })
+        config.update(
+            {
+                "path": "conntrack",
+                "dir": "/proc/sys/net/ipv4/netfilter,/proc/sys/net/netfilter",
+                "files": "ip_conntrack_count,ip_conntrack_max,"
+                "nf_conntrack_count,nf_conntrack_max",
+            }
+        )
         return config
 
     def collect(self):
         """
         Collect metrics
         """
         collected = {}
+        dirs = []
         files = []
 
-        if isinstance(self.config['dir'], basestring):
-            dirs = [d.strip() for d in self.config['dir'].split(',')]
-        elif isinstance(self.config['dir'], list):
-            dirs = self.config['dir']
-
-        if isinstance(self.config['files'], basestring):
-            files = [f.strip() for f in self.config['files'].split(',')]
-        elif isinstance(self.config['files'], list):
-            files = self.config['files']
+        if isinstance(self.config["dir"], str):
+            dirs = [d.strip() for d in self.config["dir"].split(",")]
+        elif isinstance(self.config["dir"], list):
+            dirs = self.config["dir"]
+
+        if isinstance(self.config["files"], str):
+            files = [f.strip() for f in self.config["files"].split(",")]
+        elif isinstance(self.config["files"], list):
+            files = self.config["files"]
 
         for sdir in dirs:
             for sfile in files:
-                if sfile.endswith('conntrack_count'):
-                    metric_name = 'ip_conntrack_count'
-                elif sfile.endswith('conntrack_max'):
-                    metric_name = 'ip_conntrack_max'
+                if sfile.endswith("conntrack_count"):
+                    metric_name = "ip_conntrack_count"
+                elif sfile.endswith("conntrack_max"):
+                    metric_name = "ip_conntrack_max"
                 else:
-                    self.log.error('Unknown file for collection: %s', sfile)
+                    self.log.error("Unknown file for collection: %s", sfile)
                     continue
+
                 fpath = os.path.join(sdir, sfile)
+
                 if not os.path.exists(fpath):
                     continue
+
                 try:
                     with open(fpath, "r") as fhandle:
                         metric = float(fhandle.readline().rstrip("\n"))
                         collected[metric_name] = metric
                 except Exception as exception:
-                    self.log.error("Failed to collect from '%s': %s",
-                                   fpath,
-                                   exception)
+                    self.log.error("Failed to collect from '%s': %s", fpath, exception)
         if not collected:
-            self.log.error('No metric was collected, looks like '
-                           'nf_conntrack/ip_conntrack kernel module was '
-                           'not loaded')
+            self.log.error(
+                "No metric was collected, looks like nf_conntrack/ip_conntrack kernel module was not loaded"
+            )
         else:
             for key in collected.keys():
                 self.publish(key, collected[key])
```

### Comparing `diamond-next-4.0.515/src/collectors/elb/elb.py` & `diamond-next-5.0.0/src/collectors/elb/elb.py`

 * *Files 14% similar despite different names*

```diff
@@ -34,27 +34,28 @@
 ```
 
 #### Dependencies
 
  * boto
 
 """
+
 import calendar
-import cPickle
+import collections
 import datetime
 import functools
+import pickle
 import re
-import time
+import string
 import threading
-from collections import namedtuple
-from string import Template
+
+import time
 
 import diamond.collector
-from diamond.collector import str_to_bool
-from diamond.metric import Metric
+import diamond.metric
 
 try:
     import boto.ec2.elb
     from boto.ec2 import cloudwatch
 except ImportError:
     cloudwatch = False
 
@@ -72,15 +73,15 @@
 
     def __init__(self, func):
         self.func = func
         self.cache = {}
 
     def __call__(self, *args, **kwargs):
         # If the function args cannot be used as a cache hash key, fail fast
-        key = cPickle.dumps((args, kwargs))
+        key = pickle.dumps((args, kwargs))
         try:
             return self.cache[key]
         except KeyError:
             value = self.func(*args, **kwargs)
             self.cache[key] = value
             return value
 
@@ -104,60 +105,63 @@
     assert utc_dt.resolution >= datetime.timedelta(microseconds=1)
     return local_dt.replace(microsecond=utc_dt.microsecond)
 
 
 @memoized
 def get_zones(region, auth_kwargs):
     """
+    :param auth_kwargs:
     :param region: region to get the availability zones for
     :return: list of availability zones
     """
     ec2_conn = boto.ec2.connect_to_region(region, **auth_kwargs)
     return [zone.name for zone in ec2_conn.get_all_zones()]
 
 
 class ElbCollector(diamond.collector.Collector):
 
     # default_to_zero means if cloudwatch does not return a stat for the
     # given metric, then just default it to zero.
-    MetricInfo = namedtuple(
-        'MetricInfo',
-        'name aws_type diamond_type precision default_to_zero')
+    MetricInfo = collections.namedtuple(
+        "MetricInfo", "name aws_type diamond_type precision default_to_zero"
+    )
 
     # AWS metrics for ELBs
     metrics = [
-        MetricInfo('HealthyHostCount', 'Average', 'GAUGE', 0, False),
-        MetricInfo('UnHealthyHostCount', 'Average', 'GAUGE', 0, False),
-        MetricInfo('RequestCount', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('Latency', 'Average', 'GAUGE', 4, False),
-        MetricInfo('HTTPCode_ELB_4XX', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('HTTPCode_ELB_5XX', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('HTTPCode_Backend_2XX', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('HTTPCode_Backend_3XX', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('HTTPCode_Backend_4XX', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('HTTPCode_Backend_5XX', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('BackendConnectionErrors', 'Sum', 'COUNTER', 0, True),
-        MetricInfo('SurgeQueueLength', 'Maximum', 'GAUGE', 0, True),
-        MetricInfo('SpilloverCount', 'Sum', 'COUNTER', 0, True)
+        MetricInfo("HealthyHostCount", "Average", "GAUGE", 0, False),
+        MetricInfo("UnHealthyHostCount", "Average", "GAUGE", 0, False),
+        MetricInfo("RequestCount", "Sum", "GAUGE", 0, True),
+        MetricInfo("Latency", "Average", "GAUGE", 4, False),
+        MetricInfo("HTTPCode_ELB_4XX", "Sum", "GAUGE", 0, True),
+        MetricInfo("HTTPCode_ELB_5XX", "Sum", "GAUGE", 0, True),
+        MetricInfo("HTTPCode_Backend_2XX", "Sum", "GAUGE", 0, True),
+        MetricInfo("HTTPCode_Backend_3XX", "Sum", "GAUGE", 0, True),
+        MetricInfo("HTTPCode_Backend_4XX", "Sum", "GAUGE", 0, True),
+        MetricInfo("HTTPCode_Backend_5XX", "Sum", "GAUGE", 0, True),
+        MetricInfo("BackendConnectionErrors", "Sum", "GAUGE", 0, True),
+        MetricInfo("SurgeQueueLength", "Maximum", "GAUGE", 0, True),
+        MetricInfo("SpilloverCount", "Sum", "GAUGE", 0, True),
     ]
 
     def process_config(self):
         super(ElbCollector, self).process_config()
-        if str_to_bool(self.config['enabled']):
-            self.interval = self.config.as_int('interval')
+
+        if diamond.collector.str_to_bool(self.config["enabled"]):
+            self.interval = self.config.as_int("interval")
+
             # Why is this?
             if self.interval % 60 != 0:
-                raise Exception('Interval must be a multiple of 60 seconds: %s'
-                                % self.interval)
+                raise Exception(
+                    "Interval must be a multiple of 60 seconds: %s" % self.interval
+                )
 
-        if (('access_key_id' in self.config and
-             'secret_access_key' in self.config)):
+        if "access_key_id" in self.config and "secret_access_key" in self.config:
             self.auth_kwargs = {
-                'aws_access_key_id': self.config['access_key_id'],
-                'aws_secret_access_key': self.config['secret_access_key']
+                "aws_access_key_id": self.config["access_key_id"],
+                "aws_secret_access_key": self.config["secret_access_key"],
             }
         else:
             # If creds not present, assume we're using IAM roles with
             # instance profiles. Boto will automatically take care of using
             # the creds from the instance metatdata.
             self.auth_kwargs = {}
 
@@ -168,140 +172,150 @@
         return True
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ElbCollector, self).get_default_config()
-        config.update({
-            'path': 'elb',
-            'regions': ['us-west-1'],
-            'interval': 60,
-            'format': '$zone.$elb_name.$metric_name',
-        })
+        config.update(
+            {
+                "path": "elb",
+                "regions": ["us-west-1"],
+                "interval": 60,
+                "format": "$zone.$elb_name.$metric_name",
+            }
+        )
         return config
 
-    def publish_delayed_metric(self, name, value, timestamp, raw_value=None,
-                               precision=0, metric_type='GAUGE',
-                               instance=None):
+    def publish_delayed_metric(
+        self,
+        name,
+        value,
+        timestamp,
+        raw_value=None,
+        precision=0,
+        metric_type="GAUGE",
+        instance=None,
+    ):
         """
         Metrics may not be immediately available when querying cloudwatch.
         Hence, allow the ability to publish a metric from some the past given
         its timestamp.
         """
         # Get metric Path
         path = self.get_metric_path(name, instance)
 
         # Get metric TTL
-        ttl = float(self.config['interval']) * float(
-            self.config['ttl_multiplier'])
+        ttl = float(self.config["interval"]) * float(self.config["ttl_multiplier"])
 
         # Create Metric
-        metric = Metric(path, value, raw_value=raw_value, timestamp=timestamp,
-                        precision=precision, host=self.get_hostname(),
-                        metric_type=metric_type, ttl=ttl)
+        metric = diamond.metric.Metric(
+            path,
+            value,
+            raw_value=raw_value,
+            timestamp=timestamp,
+            precision=precision,
+            host=self.get_hostname(),
+            metric_type=metric_type,
+            ttl=ttl,
+        )
 
         # Publish Metric
         self.publish_metric(metric)
 
     def get_elb_names(self, region, config):
         """
         :param region: name of a region
         :param config: Collector config dict
         :return: list of elb names to query in the given region
         """
         # This function is ripe to be memoized but when ELBs are added/removed
         # dynamically over time, diamond will have to be restarted to pick
         # up the changes.
-        region_dict = config.get('regions', {}).get(region, {})
-        if 'elb_names' not in region_dict:
-            elb_conn = boto.ec2.elb.connect_to_region(region,
-                                                      **self.auth_kwargs)
-            full_elb_names = \
-                [elb.name for elb in elb_conn.get_all_load_balancers()]
+        region_dict = config.get("regions", {}).get(region, {})
+        if "elb_names" not in region_dict:
+            elb_conn = boto.ec2.elb.connect_to_region(region, **self.auth_kwargs)
+            full_elb_names = [elb.name for elb in elb_conn.get_all_load_balancers()]
 
             # Regular expressions for ELBs we DO NOT want to get metrics on.
-            matchers = \
-                [re.compile(regex) for regex in config.get('elbs_ignored', [])]
+            matchers = [re.compile(regex) for regex in config.get("elbs_ignored", [])]
 
             # cycle through elbs get the list of elbs that don't match
             elb_names = []
             for elb_name in full_elb_names:
                 if matchers and any([m.match(elb_name) for m in matchers]):
                     continue
                 elb_names.append(elb_name)
         else:
-            elb_names = region_dict['elb_names']
+            elb_names = region_dict["elb_names"]
         return elb_names
 
     def process_stat(self, region, zone, elb_name, metric, stat, end_time):
         template_tokens = {
-            'region': region,
-            'zone': zone,
-            'elb_name': elb_name,
-            'metric_name': metric.name,
+            "region": region,
+            "zone": zone,
+            "elb_name": elb_name,
+            "metric_name": metric.name,
         }
-        name_template = Template(self.config['format'])
+        name_template = string.Template(self.config["format"])
         formatted_name = name_template.substitute(template_tokens)
         self.publish_delayed_metric(
             formatted_name,
             stat[metric.aws_type],
             metric_type=metric.diamond_type,
             precision=metric.precision,
-            timestamp=time.mktime(utc_to_local(end_time).timetuple()))
+            timestamp=time.mktime(utc_to_local(end_time).timetuple()),
+        )
 
-    def process_metric(self, region_cw_conn, zone, start_time, end_time,
-                       elb_name, metric):
+    def process_metric(
+        self, region_cw_conn, zone, start_time, end_time, elb_name, metric
+    ):
         stats = region_cw_conn.get_metric_statistics(
-            self.config['interval'],
+            self.config["interval"],
             start_time,
             end_time,
             metric.name,
-            namespace='AWS/ELB',
+            namespace="AWS/ELB",
             statistics=[metric.aws_type],
-            dimensions={
-                'LoadBalancerName': elb_name,
-                'AvailabilityZone': zone
-            })
+            dimensions={"LoadBalancerName": elb_name, "AvailabilityZone": zone},
+        )
 
         # create a fake stat if the current metric should default to zero when
         # a stat is not returned. Cloudwatch just skips the metric entirely
         # instead of wasting space to store/emit a zero.
         if len(stats) == 0 and metric.default_to_zero:
-            stats.append({
-                u'Timestamp': start_time,
-                metric.aws_type: 0.0,
-                u'Unit': u'Count'
-            })
+            stats.append(
+                {"Timestamp": start_time, metric.aws_type: 0.0, "Unit": "Count"}
+            )
 
         for stat in stats:
-            self.process_stat(region_cw_conn.region.name, zone, elb_name,
-                              metric, stat, end_time)
+            self.process_stat(
+                region_cw_conn.region.name, zone, elb_name, metric, stat, end_time
+            )
 
-    def process_elb(self, region_cw_conn, zone,
-                    start_time, end_time, elb_name):
+    def process_elb(self, region_cw_conn, zone, start_time, end_time, elb_name):
         for metric in self.metrics:
-            self.process_metric(region_cw_conn, zone, start_time, end_time,
-                                elb_name, metric)
+            self.process_metric(
+                region_cw_conn, zone, start_time, end_time, elb_name, metric
+            )
 
     def process_zone(self, region_cw_conn, zone, start_time, end_time):
-        for elb_name in self.get_elb_names(region_cw_conn.region.name,
-                                           self.config):
-            self.process_elb(region_cw_conn, zone, start_time, end_time,
-                             elb_name)
+        for elb_name in self.get_elb_names(region_cw_conn.region.name, self.config):
+            self.process_elb(region_cw_conn, zone, start_time, end_time, elb_name)
 
     def process_region(self, region_cw_conn, start_time, end_time):
         threads = []
         for zone in get_zones(region_cw_conn.region.name, self.auth_kwargs):
             # Create a new connection for each thread, Boto isn't threadsafe.
-            t_conn = cloudwatch.connect_to_region(region_cw_conn.region.name,
-                                                  **self.auth_kwargs)
-            zone_thread = threading.Thread(target=self.process_zone,
-                                           args=(t_conn, zone,
-                                                 start_time, end_time))
+            t_conn = cloudwatch.connect_to_region(
+                region_cw_conn.region.name, **self.auth_kwargs
+            )
+            zone_thread = threading.Thread(
+                target=self.process_zone, args=(t_conn, zone, start_time, end_time)
+            )
             zone_thread.start()
 
             threads.append(zone_thread)
 
         # Make sure all threads have completed. Also allows scheduler to work
         # more 'correctly', because without this, the collector will 'complete'
         # in about 7ms.
@@ -312,11 +326,10 @@
         if not self.check_boto():
             return
 
         now = datetime.datetime.utcnow()
         end_time = now.replace(second=0, microsecond=0)
         start_time = end_time - datetime.timedelta(seconds=self.interval)
 
-        for region in self.config['regions'].keys():
-            region_cw_conn = cloudwatch.connect_to_region(region,
-                                                          **self.auth_kwargs)
+        for region in self.config["regions"].keys():
+            region_cw_conn = cloudwatch.connect_to_region(region, **self.auth_kwargs)
             self.process_region(region_cw_conn, start_time, end_time)
```

### Comparing `diamond-next-4.0.515/src/collectors/nfs/nfs.py` & `diamond-next-5.0.0/src/collectors/nfsd/nfsd.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,225 +1,203 @@
 # coding=utf-8
 
 """
-The NfsCollector collects nfs utilization metrics using /proc/net/rpc/nfs.
+The NfsdCollector collects nfsd utilization metrics using /proc/net/rpc/nfsd.
 
 #### Dependencies
 
- * /proc/net/rpc/nfs
+ * /proc/net/rpc/nfsd
 
 """
 
-import diamond.collector
 import os
 
+import diamond.collector
 
-class NfsCollector(diamond.collector.Collector):
 
-    PROC = '/proc/net/rpc/nfs'
+class NfsdCollector(diamond.collector.Collector):
+    PROC = "/proc/net/rpc/nfsd"
 
     def get_default_config_help(self):
-        config_help = super(NfsCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(NfsdCollector, self).get_default_config_help()
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
-        config = super(NfsCollector, self).get_default_config()
-        config.update({
-            'path':     'nfs'
-        })
+        config = super(NfsdCollector, self).get_default_config()
+        config.update({"path": "nfsd"})
+
         return config
 
     def collect(self):
         """
         Collect stats
         """
         if os.access(self.PROC, os.R_OK):
-
             results = {}
+
             # Open file
             file = open(self.PROC)
 
             for line in file:
                 line = line.split()
 
-                if line[0] == 'net':
-                    results['net.packets'] = line[1]
-                    results['net.udpcnt'] = line[2]
-                    results['net.tcpcnt'] = line[3]
-                    results['net.tcpconn'] = line[4]
-                elif line[0] == 'rpc':
-                    results['rpc.calls'] = line[1]
-                    results['rpc.retrans'] = line[2]
-                    results['rpc.authrefrsh'] = line[3]
-                elif line[0] == 'proc2':
-                    line.pop(1)     # remove column-cnt field
-                    results['v2.null'] = line[1]
-                    results['v2.getattr'] = line[2]
-                    results['v2.setattr'] = line[3]
-                    results['v2.root'] = line[4]
-                    results['v2.lookup'] = line[5]
-                    results['v2.readlink'] = line[6]
-                    results['v2.read'] = line[7]
-                    results['v2.wrcache'] = line[8]
-                    results['v2.write'] = line[9]
-                    results['v2.create'] = line[10]
-                    results['v2.remove'] = line[11]
-                    results['v2.rename'] = line[12]
-                    results['v2.link'] = line[13]
-                    results['v2.symlink'] = line[14]
-                    results['v2.mkdir'] = line[15]
-                    results['v2.rmdir'] = line[16]
-                    results['v2.readdir'] = line[17]
-                    results['v2.fsstat'] = line[18]
-                elif line[0] == 'proc3':
-                    line.pop(1)     # remove column-cnt field
-                    results['v3.null'] = line[1]
-                    results['v3.getattr'] = line[2]
-                    results['v3.setattr'] = line[3]
-                    results['v3.lookup'] = line[4]
-                    results['v3.access'] = line[5]
-                    results['v3.readlink'] = line[6]
-                    results['v3.read'] = line[7]
-                    results['v3.write'] = line[8]
-                    results['v3.create'] = line[9]
-                    results['v3.mkdir'] = line[10]
-                    results['v3.symlink'] = line[11]
-                    results['v3.mknod'] = line[12]
-                    results['v3.remove'] = line[13]
-                    results['v3.rmdir'] = line[14]
-                    results['v3.rename'] = line[15]
-                    results['v3.link'] = line[16]
-                    results['v3.readdir'] = line[17]
-                    results['v3.readdirplus'] = line[18]
-                    results['v3.fsstat'] = line[19]
-                    results['v3.fsinfo'] = line[20]
-                    results['v3.pathconf'] = line[21]
-                    results['v3.commit'] = line[22]
-                elif line[0] == 'proc4':
-                    line.pop(1)     # remove column-cnt field
-                    results['v4.null'] = line[1]
-                    results['v4.read'] = line[2]
-                    results['v4.write'] = line[3]
-                    results['v4.commit'] = line[4]
-                    results['v4.open'] = line[5]
-                    results['v4.open_conf'] = line[6]
-                    results['v4.open_noat'] = line[7]
-                    results['v4.open_dgrd'] = line[8]
-                    results['v4.close'] = line[9]
-                    results['v4.setattr'] = line[10]
-                    results['v4.fsinfo'] = line[11]
-                    results['v4.renew'] = line[12]
-                    results['v4.setclntid'] = line[13]
-                    results['v4.confirm'] = line[14]
-                    results['v4.lock'] = line[15]
-                    results['v4.lockt'] = line[16]
-                    results['v4.locku'] = line[17]
-                    results['v4.access'] = line[18]
-                    results['v4.getattr'] = line[19]
-                    results['v4.lookup'] = line[20]
-                    results['v4.lookup_root'] = line[21]
-                    results['v4.remove'] = line[22]
-                    results['v4.rename'] = line[23]
-                    results['v4.link'] = line[24]
-                    results['v4.symlink'] = line[25]
-                    results['v4.create'] = line[26]
-                    results['v4.pathconf'] = line[27]
-                    results['v4.statfs'] = line[28]
-                    results['v4.readlink'] = line[29]
-                    results['v4.readdir'] = line[30]
-                    try:
-                        results['v4.server_caps'] = line[31]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.delegreturn'] = line[32]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.getacl'] = line[33]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.setacl'] = line[34]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.fs_locations'] = line[35]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.rel_lkowner'] = line[36]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.exchange_id'] = line[37]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.create_ses'] = line[38]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.destroy_ses'] = line[39]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.sequence'] = line[40]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.get_lease_t'] = line[41]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.reclaim_comp'] = line[42]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.layoutget'] = line[43]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.layoutcommit'] = line[44]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.layoutreturn'] = line[45]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.getdevlist'] = line[46]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.getdevinfo'] = line[47]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.ds_write'] = line[48]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.ds_commit'] = line[49]
-                    except IndexError:
-                        pass
-                    try:
-                        results['v4.getdevlist'] = line[50]
-                    except IndexError:
-                        pass
+                if line[0] == "rc":
+                    results["reply_cache.hits"] = line[1]
+                    results["reply_cache.misses"] = line[2]
+                    results["reply_cache.nocache"] = line[3]
+                elif line[0] == "fh":
+                    results["filehandle.stale"] = line[1]
+                    results["filehandle.total-lookups"] = line[2]
+                    results["filehandle.anonlookups"] = line[3]
+                    results["filehandle.dir-not-in-cache"] = line[4]
+                    results["filehandle.nodir-not-in-cache"] = line[5]
+                elif line[0] == "io":
+                    results["input_output.bytes-read"] = line[1]
+                    results["input_output.bytes-written"] = line[2]
+                elif line[0] == "th":
+                    results["threads.threads"] = line[1]
+                    results["threads.fullcnt"] = line[2]
+                    results["threads.10-20-pct"] = line[3]
+                    results["threads.20-30-pct"] = line[4]
+                    results["threads.30-40-pct"] = line[5]
+                    results["threads.40-50-pct"] = line[6]
+                    results["threads.50-60-pct"] = line[7]
+                    results["threads.60-70-pct"] = line[8]
+                    results["threads.70-80-pct"] = line[9]
+                    results["threads.80-90-pct"] = line[10]
+                    results["threads.90-100-pct"] = line[11]
+                    results["threads.100-pct"] = line[12]
+                elif line[0] == "ra":
+                    results["read-ahead.cache-size"] = line[1]
+                    results["read-ahead.10-pct"] = line[2]
+                    results["read-ahead.20-pct"] = line[3]
+                    results["read-ahead.30-pct"] = line[4]
+                    results["read-ahead.40-pct"] = line[5]
+                    results["read-ahead.50-pct"] = line[6]
+                    results["read-ahead.60-pct"] = line[7]
+                    results["read-ahead.70-pct"] = line[8]
+                    results["read-ahead.80-pct"] = line[9]
+                    results["read-ahead.90-pct"] = line[10]
+                    results["read-ahead.100-pct"] = line[11]
+                    results["read-ahead.not-found"] = line[12]
+                elif line[0] == "net":
+                    results["net.cnt"] = line[1]
+                    results["net.udpcnt"] = line[2]
+                    results["net.tcpcnt"] = line[3]
+                    results["net.tcpconn"] = line[4]
+                elif line[0] == "rpc":
+                    results["rpc.cnt"] = line[1]
+                    results["rpc.badfmt"] = line[2]
+                    results["rpc.badauth"] = line[3]
+                    results["rpc.badclnt"] = line[4]
+                elif line[0] == "proc2":
+                    # the first column is a column-cnt field, skip
+                    # results['v2.unknown'] = line[1]
+                    results["v2.null"] = line[2]
+                    results["v2.getattr"] = line[3]
+                    results["v2.setattr"] = line[4]
+                    results["v2.root"] = line[5]
+                    results["v2.lookup"] = line[6]
+                    results["v2.readlink"] = line[7]
+                    results["v2.read"] = line[8]
+                    results["v2.wrcache"] = line[9]
+                    results["v2.write"] = line[10]
+                    results["v2.create"] = line[11]
+                    results["v2.remove"] = line[12]
+                    results["v2.rename"] = line[13]
+                    results["v2.link"] = line[14]
+                    results["v2.symlink"] = line[15]
+                    results["v2.mkdir"] = line[16]
+                    results["v2.rmdir"] = line[17]
+                    results["v2.readdir"] = line[18]
+                    results["v2.fsstat"] = line[19]
+                elif line[0] == "proc3":
+                    # the first column is a column-cnt field, skip
+                    # results['v3.unknown'] = line[1]
+                    results["v3.null"] = line[2]
+                    results["v3.getattr"] = line[3]
+                    results["v3.setattr"] = line[4]
+                    results["v3.lookup"] = line[5]
+                    results["v3.access"] = line[6]
+                    results["v3.readlink"] = line[7]
+                    results["v3.read"] = line[8]
+                    results["v3.write"] = line[9]
+                    results["v3.create"] = line[10]
+                    results["v3.mkdir"] = line[11]
+                    results["v3.symlink"] = line[12]
+                    results["v3.mknod"] = line[13]
+                    results["v3.remove"] = line[14]
+                    results["v3.rmdir"] = line[15]
+                    results["v3.rename"] = line[16]
+                    results["v3.link"] = line[17]
+                    results["v3.readdir"] = line[18]
+                    results["v3.readdirplus"] = line[19]
+                    results["v3.fsstat"] = line[20]
+                    results["v3.fsinfo"] = line[21]
+                    results["v3.pathconf"] = line[22]
+                    results["v3.commit"] = line[23]
+                elif line[0] == "proc4":
+                    # the first column is a column-cnt field, skip
+                    # results['v4.unknown'] = line[1]
+                    results["v4.null"] = line[2]
+                    results["v4.compound"] = line[3]
+                elif line[0] == "proc4ops":
+                    # the first column is a column-cnt field, skip
+                    # results['v4.ops.unknown'] = line[1]
+                    results["v4.ops.op0-unused"] = line[2]
+                    results["v4.ops.op1-unused"] = line[3]
+                    results["v4.ops.op2-future"] = line[4]
+                    results["v4.ops.access"] = line[5]
+                    results["v4.ops.close"] = line[6]
+                    results["v4.ops.commit"] = line[7]
+                    results["v4.ops.create"] = line[8]
+                    results["v4.ops.delegpurge"] = line[9]
+                    results["v4.ops.delegreturn"] = line[10]
+                    results["v4.ops.getattr"] = line[11]
+                    results["v4.ops.getfh"] = line[12]
+                    results["v4.ops.link"] = line[13]
+                    results["v4.ops.lock"] = line[14]
+                    results["v4.ops.lockt"] = line[15]
+                    results["v4.ops.locku"] = line[16]
+                    results["v4.ops.lookup"] = line[17]
+                    results["v4.ops.lookup_root"] = line[18]
+                    results["v4.ops.nverify"] = line[19]
+                    results["v4.ops.open"] = line[20]
+                    results["v4.ops.openattr"] = line[21]
+                    results["v4.ops.open_conf"] = line[22]
+                    results["v4.ops.open_dgrd"] = line[23]
+                    results["v4.ops.putfh"] = line[24]
+                    results["v4.ops.putpubfh"] = line[25]
+                    results["v4.ops.putrootfh"] = line[26]
+                    results["v4.ops.read"] = line[27]
+                    results["v4.ops.readdir"] = line[28]
+                    results["v4.ops.readlink"] = line[29]
+                    results["v4.ops.remove"] = line[30]
+                    results["v4.ops.rename"] = line[31]
+                    results["v4.ops.renew"] = line[32]
+                    results["v4.ops.restorefh"] = line[33]
+                    results["v4.ops.savefh"] = line[34]
+                    results["v4.ops.secinfo"] = line[35]
+                    results["v4.ops.setattr"] = line[36]
+                    results["v4.ops.setcltid"] = line[37]
+                    results["v4.ops.setcltidconf"] = line[38]
+                    results["v4.ops.verify"] = line[39]
+                    results["v4.ops.write"] = line[40]
+                    results["v4.ops.rellockowner"] = line[41]
 
             # Close File
             file.close()
 
             for stat in results.keys():
                 metric_name = stat
-                metric_value = long(float(results[stat]))
+                metric_value = int(float(results[stat]))
                 metric_value = self.derivative(metric_name, metric_value)
                 self.publish(metric_name, metric_value, precision=3)
+
             return True
 
         return False
```

### Comparing `diamond-next-4.0.515/src/collectors/openvpn/openvpn.py` & `diamond-next-5.0.0/src/collectors/openvpn/openvpn.py`

 * *Files 18% similar despite different names*

```diff
@@ -26,221 +26,245 @@
 
 You can also specify multiple and mixed instances::
 
     instances = file:///var/log/openvpn/openvpn.log, tcp://10.0.0.1:1195?admins
 
 #### Dependencies
 
- * urlparse
+ * urllib
 
 """
 
-import socket
-import diamond.collector
 import os.path
-import urlparse
+import socket
+import urllib.parse
+
 import time
 
+import diamond.collector
 
-class OpenVPNCollector(diamond.collector.Collector):
 
+class OpenVPNCollector(diamond.collector.Collector):
     def get_default_config_help(self):
         config_help = super(OpenVPNCollector, self).get_default_config_help()
-        config_help.update({
-            'instances': 'List of instances to collect stats from',
-            'timeout': 'network timeout'
-        })
+        config_help.update(
+            {
+                "instances": "List of instances to collect stats from",
+                "timeout": "network timeout",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(OpenVPNCollector, self).get_default_config()
-        config.update({
-            'path':      'openvpn',
-            'instances': 'file:///var/log/openvpn/status.log',
-            'timeout':   '10',
-        })
+        config.update(
+            {
+                "path": "openvpn",
+                "instances": "file:///var/log/openvpn/status.log",
+                "timeout": "10",
+            }
+        )
+
         return config
 
     def parse_url(self, uri):
         """
         Convert urlparse from a python 2.4 layout to a python 2.7 layout
         """
-        parsed = urlparse.urlparse(uri)
-        if 'scheme' not in parsed:
+        parsed = urllib.parse.urlparse(uri)
+
+        if "scheme" not in parsed:
+
             class Object(object):
                 pass
+
             newparsed = Object()
             newparsed.scheme = parsed[0]
             newparsed.netloc = parsed[1]
             newparsed.path = parsed[2]
             newparsed.params = parsed[3]
             newparsed.query = parsed[4]
             newparsed.fragment = parsed[5]
-            newparsed.username = ''
-            newparsed.password = ''
-            newparsed.hostname = ''
-            newparsed.port = ''
+            newparsed.username = ""
+            newparsed.password = ""
+            newparsed.hostname = ""
+            newparsed.port = ""
             parsed = newparsed
+
         return parsed
 
     def collect(self):
-        if isinstance(self.config['instances'], basestring):
-            instances = [self.config['instances']]
+        if isinstance(self.config["instances"], str):
+            instances = [self.config["instances"]]
         else:
-            instances = self.config['instances']
+            instances = self.config["instances"]
 
         for uri in instances:
             parsed = self.parse_url(uri)
-            collect = getattr(self, 'collect_%s' % (parsed.scheme,), None)
+            collect = getattr(self, "collect_%s" % (parsed.scheme,), None)
+
             if collect:
                 collect(uri)
             else:
-                self.log.error('OpenVPN no handler for %s', uri)
+                self.log.error("OpenVPN no handler for %s", uri)
 
     def collect_file(self, uri):
         parsed = self.parse_url(uri)
         filename = parsed.path
-        if '?' in filename:
-            filename, name = filename.split('?')
+
+        if "?" in filename:
+            filename, name = filename.split("?")
         else:
             name = os.path.splitext(os.path.basename(filename))[0]
 
         if not os.access(filename, os.R_OK):
-            self.log.error('OpenVPN collect failed: unable to read "%s"',
-                           filename)
+            self.log.error('OpenVPN collect failed: unable to read "%s"', filename)
             return
         else:
             self.log.info('OpenVPN parsing "%s" file: %s', name, filename)
 
-        fd = open(filename, 'r')
+        fd = open(filename, "r")
         lines = fd.readlines()
         fd.close()
 
         self.parse(name, lines)
 
     def collect_tcp(self, uri):
         parsed = self.parse_url(uri)
+
         try:
-            host, port = parsed.netloc.split(':')
+            host, port = parsed.netloc.split(":")
             port = int(port)
         except ValueError:
-            self.log.error('OpenVPN expected host:port in URI, got "%s"',
-                           parsed.netloc)
+            self.log.error('OpenVPN expected host:port in URI, got "%s"', parsed.netloc)
             return
 
-        if '?' in parsed.path:
+        if "?" in parsed.path:
             name = parsed.path[1:]
         else:
-            name = host.replace('.', '_')
+            name = host.replace(".", "_")
 
         self.log.info('OpenVPN parsing "%s" tcp: %s:%d', name, host, port)
 
         try:
             server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-            server.settimeout(int(self.config['timeout']))
+            server.settimeout(int(self.config["timeout"]))
             server.connect((host, port))
 
-            fd = server.makefile('rb')
+            fd = server.makefile("rb")
             line = fd.readline()
-            if not line.startswith('>INFO:OpenVPN'):
-                self.log.debug('OpenVPN received: %s', line.rstrip())
-                self.log.error('OpenVPN protocol error')
+
+            if not line.startswith(">INFO:OpenVPN"):
+                self.log.debug("OpenVPN received: %s", line.rstrip())
+                self.log.error("OpenVPN protocol error")
                 server.close()
                 return
 
-            server.send('status\r\n')
+            server.send("status\r\n")
 
             lines = []
+
             while True:
                 line = fd.readline()
                 lines.append(line)
-                if line.strip() == 'END':
+
+                if line.strip() == "END":
                     break
 
             # Hand over data to the parser
             self.parse(name, lines)
 
             # Bye
             server.close()
 
-        except socket.error, e:
-            self.log.error('OpenVPN management connection error: %s', e)
+        except socket.error as e:
+            self.log.error("OpenVPN management connection error: %s", e)
             return
 
     def parse(self, name, lines):
         for line in lines:
-            self.log.debug('OpenVPN: %s', line.rstrip())
+            self.log.debug("OpenVPN: %s", line.rstrip())
 
         time.sleep(0.5)
 
         number_connected_clients = 0
-        section = ''
+        section = ""
         heading = []
+
         for line in lines:
-            if line.strip() == 'END':
+            if line.strip() == "END":
                 break
-            elif line.lower().startswith('openvpn statistics'):
-                section = 'statistics'
-            elif line.lower().startswith('openvpn client list'):
-                section = 'clients'
-            elif line.lower().startswith('routing table'):
+            elif line.lower().startswith("openvpn statistics"):
+                section = "statistics"
+            elif line.lower().startswith("openvpn client list"):
+                section = "clients"
+            elif line.lower().startswith("routing table"):
                 # ignored
-                section = ''
-            elif line.lower().startswith('global stats'):
-                section = 'global'
-            elif ',' in line:
-                key, value = line.split(',', 1)
-                if key.lower() == 'updated':
+                section = ""
+            elif line.lower().startswith("global stats"):
+                section = "global"
+            elif "," in line:
+                key, value = line.split(",", 1)
+
+                if key.lower() == "updated":
                     continue
 
-                if section == 'statistics':
+                if section == "statistics":
                     # All values here are numeric
-                    self.publish_number('.'.join([
-                        name,
-                        'global',
-                        key, ]), value)
+                    self.publish_number(".".join([name, "global", key]), value)
 
-                elif section == 'clients':
+                elif section == "clients":
                     # Clients come with a heading
                     if not heading:
-                        heading = line.strip().split(',')
+                        heading = line.strip().split(",")
                     else:
                         info = {}
                         number_connected_clients += 1
-                        for k, v in zip(heading, line.strip().split(',')):
-                            info[k.lower()] = v
 
-                        self.publish_number('.'.join([
-                            name,
-                            section,
-                            info['common name'].replace('.', '_'),
-                            'bytes_rx']), info['bytes received'])
-                        self.publish_number('.'.join([
-                            name,
-                            section,
-                            info['common name'].replace('.', '_'),
-                            'bytes_tx']), info['bytes sent'])
+                        for k, v in zip(heading, line.strip().split(",")):
+                            info[k.lower()] = v
 
-                elif section == 'global':
+                        self.publish_number(
+                            ".".join(
+                                [
+                                    name,
+                                    section,
+                                    info["common name"].replace(".", "_"),
+                                    "bytes_rx",
+                                ]
+                            ),
+                            info["bytes received"],
+                        )
+                        self.publish_number(
+                            ".".join(
+                                [
+                                    name,
+                                    section,
+                                    info["common name"].replace(".", "_"),
+                                    "bytes_tx",
+                                ]
+                            ),
+                            info["bytes sent"],
+                        )
+                elif section == "global":
                     # All values here are numeric
-                    self.publish_number('.'.join([
-                        name,
-                        section,
-                        key, ]), value)
+                    self.publish_number(".".join([name, section, key]), value)
 
-            elif line.startswith('END'):
+            elif line.startswith("END"):
                 break
-        self.publish('%s.clients.connected' % name, number_connected_clients)
+
+        self.publish("%s.clients.connected" % name, number_connected_clients)
 
     def publish_number(self, key, value):
-        key = key.replace('/', '-').replace(' ', '_').lower()
+        key = key.replace("/", "-").replace(" ", "_").lower()
+
         try:
-            value = long(value)
+            value = int(value)
         except ValueError:
-            self.log.error('OpenVPN expected a number for "%s", got "%s"',
-                           key, value)
+            self.log.error('OpenVPN expected a number for "%s", got "%s"', key, value)
             return
         else:
             self.publish(key, value)
```

### Comparing `diamond-next-4.0.515/src/collectors/passenger_stats/passenger_stats.py` & `diamond-next-5.0.0/src/collectors/passenger_stats/passenger_stats.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,69 +1,87 @@
 # coding=utf-8
 
 """
 The PasengerCollector collects CPU and memory utilization of apache, nginx
 and passenger processes.
+It also collects requests in top-level and passenger applications group queues.
 
 Four key attributes to be published:
 
  * phusion_passenger_cpu
  * total_apache_memory
+ * total_apache_procs
  * total_passenger_memory
+ * total_passenger_procs
  * total_nginx_memory
+ * total_nginx_procs
+ * top_level_queue_size
+ * passenger_queue_size
 
 #### Dependencies
 
  * passenger-memory-stats
+ * passenger-status
 
 """
-import diamond.collector
+
 import os
 import re
 import subprocess
-from diamond.collector import str_to_bool
+
+import diamond.collector
 
 
 class PassengerCollector(diamond.collector.Collector):
     """
     Collect Memory and CPU Utilization for Passenger
     """
 
     def get_default_config_help(self):
         """
         Return help text
         """
         config_help = super(PassengerCollector, self).get_default_config_help()
-        config_help.update({
-            "bin":         "The path to the binary",
-            "use_sudo":    "Use sudo?",
-            "sudo_cmd":    "Path to sudo",
-        })
+        config_help.update(
+            {
+                "bin": "The path to the binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+                "passenger_status_bin": "The path to the binary passenger-status",
+                "passenger_memory_stats_bin": "The path to the binary passenger-memory-stats",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(PassengerCollector, self).get_default_config()
-        config.update({
-            "path":         "passenger_stats",
-            "bin":          "/usr/lib/ruby-flo/bin/passenger-memory-stats",
-            "use_sudo":     False,
-            "sudo_cmd":     "/usr/bin/sudo",
-        })
+        config.update(
+            {
+                "path": "passenger_stats",
+                "bin": "/usr/lib/ruby-flo/bin/passenger-memory-stats",
+                "use_sudo": False,
+                "sudo_cmd": "/usr/bin/sudo",
+                "passenger_status_bin": "/usr/bin/passenger-status",
+                "passenger_memory_stats_bin": "/usr/bin/passenger-memory-stats",
+            }
+        )
+
         return config
 
     def get_passenger_memory_stats(self):
         """
-        Execute passenger-memory-stats, parse its output, return dictionary with
-        stats.
+        Execute passenger-memory-stats, parse its output, return dictionary with stats.
         """
-        command = [self.config["bin"]]
-        if str_to_bool(self.config["use_sudo"]):
+        command = [self.config["passenger_memory_stats_bin"]]
+
+        if diamond.collector.str_to_bool(self.config["use_sudo"]):
             command.insert(0, self.config["sudo_cmd"])
 
         try:
             proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)
             (std_out, std_err) = proc1.communicate()
         except OSError:
             return {}
@@ -82,87 +100,156 @@
         #
         re_colour = re.compile("\x1B\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]")
         re_digit = re.compile("^\d")
         #
         apache_flag = 0
         nginx_flag = 0
         passenger_flag = 0
+
         for raw_line in std_out.splitlines():
             line = re_colour.sub("", raw_line)
+
             if "Apache processes" in line:
                 apache_flag = 1
             elif "Nginx processes" in line:
                 nginx_flag = 1
             elif "Passenger processes" in line:
                 passenger_flag = 1
             elif re_digit.match(line):
                 # If line starts with digit, then store PID and memory consumed
                 line_splitted = line.split()
+
                 if apache_flag == 1:
                     dict_stats["apache_procs"].append(line_splitted[0])
                     dict_stats["apache_mem_total"] += float(line_splitted[4])
                 elif nginx_flag == 1:
                     dict_stats["nginx_procs"].append(line_splitted[0])
                     dict_stats["nginx_mem_total"] += float(line_splitted[4])
                 elif passenger_flag == 1:
                     dict_stats["passenger_procs"].append(line_splitted[0])
-                    dict_stats[
-                        "passenger_mem_total"] += float(line_splitted[3])
+                    dict_stats["passenger_mem_total"] += float(line_splitted[3])
 
             elif "Processes:" in line:
                 passenger_flag = 0
                 apache_flag = 0
                 nginx_flag = 0
 
         return dict_stats
 
     def get_passenger_cpu_usage(self, dict_stats):
         """
         Execute % top; and return STDOUT.
         """
         try:
-            proc1 = subprocess.Popen(
-                ["top", "-b", "-n", "2"],
-                stdout=subprocess.PIPE)
+            proc1 = subprocess.Popen(["top", "-b", "-n", "2"], stdout=subprocess.PIPE)
             (std_out, std_err) = proc1.communicate()
         except OSError:
-            return (-1)
+            return -1
 
         re_lspaces = re.compile("^\s*")
         re_digit = re.compile("^\d")
         overall_cpu = 0
+
         for raw_line in std_out.splitlines():
             line = re_lspaces.sub("", raw_line)
+
             if not re_digit.match(line):
                 continue
 
             line_splitted = line.split()
+
             if line_splitted[0] in dict_stats["apache_procs"]:
                 overall_cpu += float(line_splitted[8])
             elif line_splitted[0] in dict_stats["nginx_procs"]:
                 overall_cpu += float(line_splitted[8])
             elif line_splitted[0] in dict_stats["passenger_procs"]:
                 overall_cpu += float(line_splitted[8])
 
         return overall_cpu
 
+    def get_passenger_queue_stats(self):
+        """
+        Execute passenger-stats, parse its output, returnand requests in queue
+        """
+        queue_stats = {
+            "top_level_queue_size": 0.0,
+            "passenger_queue_size": 0.0,
+        }
+
+        command = [self.config["passenger_status_bin"]]
+
+        if diamond.collector.str_to_bool(self.config["use_sudo"]):
+            command.insert(0, self.config["sudo_cmd"])
+
+        try:
+            proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)
+            (std_out, std_err) = proc1.communicate()
+        except OSError:
+            return {}
+
+        if std_out is None:
+            return {}
+
+        re_colour = re.compile("\x1B\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]")
+        re_requests = re.compile(r"Requests")
+        re_topqueue = re.compile(r"^top-level")
+
+        gen_info_flag = 0
+        app_groups_flag = 0
+
+        for raw_line in std_out.splitlines():
+            line = re_colour.sub("", raw_line)
+
+            if "General information" in line:
+                gen_info_flag = 1
+
+            if "Application groups" in line:
+                app_groups_flag = 1
+            elif re_requests.match(line) and re_topqueue.search(line):
+                # If line starts with Requests and line has top-level queue then
+                # store queue size
+                line_splitted = line.split()
+
+                if gen_info_flag == 1 and line_splitted:
+                    queue_stats["top_level_queue_size"] = float(line_splitted[5])
+            elif re_requests.search(line) and not re_topqueue.search(line):
+                # If line has Requests and nothing else special
+                line_splitted = line.split()
+
+                if app_groups_flag == 1 and line_splitted:
+                    queue_stats["passenger_queue_size"] = float(line_splitted[3])
+
+        return queue_stats
+
     def collect(self):
         """
         Collector Passenger stats
         """
         if not os.access(self.config["bin"], os.X_OK):
-            self.log.error("Path %s does not exist or is not executable",
-                           self.config["bin"])
+            self.log.error(
+                "Path %s does not exist or is not executable", self.config["bin"]
+            )
             return {}
 
         dict_stats = self.get_passenger_memory_stats()
+
         if len(dict_stats.keys()) == 0:
             return {}
 
+        queue_stats = self.get_passenger_queue_stats()
+
+        if len(queue_stats.keys()) == 0:
+            return {}
+
         overall_cpu = self.get_passenger_cpu_usage(dict_stats)
+
         if overall_cpu >= 0:
             self.publish("phusion_passenger_cpu", overall_cpu)
 
+        self.publish("total_passenger_procs", len(dict_stats["passenger_procs"]))
+        self.publish("total_nginx_procs", len(dict_stats["nginx_procs"]))
+        self.publish("total_apache_procs", len(dict_stats["apache_procs"]))
         self.publish("total_apache_memory", dict_stats["apache_mem_total"])
         self.publish("total_nginx_memory", dict_stats["nginx_mem_total"])
-        self.publish("total_passenger_memory",
-                     dict_stats["passenger_mem_total"])
+        self.publish("total_passenger_memory", dict_stats["passenger_mem_total"])
+        self.publish("top_level_queue_size", queue_stats["top_level_queue_size"])
+        self.publish("passenger_queue_size", queue_stats["passenger_queue_size"])
```

### Comparing `diamond-next-4.0.515/src/collectors/mesos_cgroup/mesos_cgroup.py` & `diamond-next-5.0.0/src/collectors/mesos_cgroup/mesos_cgroup.py`

 * *Files 13% similar despite different names*

```diff
@@ -18,117 +18,121 @@
 
 ```
     host = localhost
     port = 5051
 ```
 """
 
-import diamond.collector
 import json
-import urllib2
 import os
+import urllib.error
+import urllib.request
+
+import diamond.collector
 
 
 class MesosCGroupCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(MesosCGroupCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': 'Hostname',
-            'port': 'Port'
-        })
+        config_help = super(MesosCGroupCollector, self).get_default_config_help()
+        config_help.update({"host": "Hostname", "port": "Port"})
         return config_help
 
     def get_default_config(self):
         # https://github.com/python-diamond/Diamond/blob/master/src/diamond/collector.py#L312-L358
         config = super(MesosCGroupCollector, self).get_default_config()
-        config.update({
-            'mesos_state_path': 'state.json',
-            'cgroup_fs_path': '/sys/fs/cgroup',
-            'host': 'localhost',
-            'port': 5051,
-            'path_prefix': 'mesos',
-            'path': 'tasks',
-            'hostname': None
-        })
+        config.update(
+            {
+                "mesos_state_path": "state.json",
+                "cgroup_fs_path": "/sys/fs/cgroup",
+                "host": "localhost",
+                "port": 5051,
+                "path_prefix": "mesos",
+                "path": "tasks",
+                "hostname": None,
+            }
+        )
         return config
 
     def __init__(self, *args, **kwargs):
         super(MesosCGroupCollector, self).__init__(*args, **kwargs)
 
     def collect(self):
         containers = self.get_containers()
 
-        sysfs = containers['flags']['cgroups_hierarchy']
-        cgroup_root = containers['flags']['cgroups_root']
+        sysfs = containers["flags"]["cgroups_hierarchy"]
+        cgroup_root = containers["flags"]["cgroups_root"]
 
-        for aspect in ['cpuacct', 'cpu', 'memory']:
+        for aspect in ["cpuacct", "cpu", "memory"]:
             aspect_path = os.path.join(sysfs, aspect, cgroup_root)
 
             contents = os.listdir(aspect_path)
-            for task_id in [entry for entry in contents if
-                            os.path.isdir(os.path.join(aspect_path, entry))]:
 
+            for task_id in [
+                entry
+                for entry in contents
+                if os.path.isdir(os.path.join(aspect_path, entry))
+            ]:
                 if task_id not in containers:
                     continue
 
-                key_parts = [containers[task_id]['environment'],
-                             containers[task_id]['role'],
-                             containers[task_id]['task'],
-                             containers[task_id]['id'],
-                             aspect]
+                key_parts = [
+                    containers[task_id]["environment"],
+                    containers[task_id]["role"],
+                    containers[task_id]["task"],
+                    containers[task_id]["id"],
+                    aspect,
+                ]
 
                 # list task_id items
                 task_id = os.path.join(aspect_path, task_id)
 
                 if aspect == "cpuacct":
                     with open(os.path.join(task_id, "%s.usage" % aspect)) as f:
                         value = f.readline()
                         self.publish(
-                            self.clean_up(
-                                '.'.join(key_parts + ['usage'])), value)
+                            self.clean_up(".".join(key_parts + ["usage"])), value
+                        )
 
                 with open(os.path.join(task_id, "%s.stat" % aspect)) as f:
                     data = f.readlines()
 
                     for kv_pair in data:
                         key, value = kv_pair.split()
-                        self.publish(
-                            self.clean_up(
-                                '.'.join(key_parts + [key])), value)
+                        self.publish(self.clean_up(".".join(key_parts + [key])), value)
 
     def get_containers(self):
         state = self.get_mesos_state()
 
-        containers = {
-            'flags': state['flags']
-        }
-
-        if 'frameworks' in state:
-            for framework in state['frameworks']:
-                for executor in framework['executors']:
-                    container = executor['container']
-                    source = executor['source']
-                    role, environment, task, number = source.split('.')
-
-                    containers[container] = {'role': role,
-                                             'environment': environment,
-                                             'task': task,
-                                             'id': number
-                                             }
+        containers = {"flags": state["flags"]}
+
+        if "frameworks" in state:
+            for framework in state["frameworks"]:
+                for executor in framework["executors"]:
+                    container = executor["container"]
+                    source = executor["source"]
+                    role, environment, task, number = source.split(".")
+
+                    containers[container] = {
+                        "role": role,
+                        "environment": environment,
+                        "task": task,
+                        "id": number,
+                    }
 
         return containers
 
     def get_mesos_state(self):
         try:
-            url = "http://%s:%s/%s" % (self.config['host'],
-                                       self.config['port'],
-                                       self.config['mesos_state_path'])
-
-            return json.load(urllib2.urlopen(url))
-        except (urllib2.HTTPError, ValueError), err:
-            self.log.error('Unable to read JSON response: %s' % err)
+            url = "http://%s:%s/%s" % (
+                self.config["host"],
+                self.config["port"],
+                self.config["mesos_state_path"],
+            )
+
+            return json.load(urllib.request.urlopen(url))
+        except (urllib.error.HTTPError, ValueError) as err:
+            self.log.error("Unable to read JSON response: %s" % err)
+
             return {}
 
     def clean_up(self, text):
-        return text.replace('/', '.')
+        return text.replace("/", ".")
```

### Comparing `diamond-next-4.0.515/src/collectors/ntpd/ntpd.py` & `diamond-next-5.0.0/src/collectors/ntpd/ntpd.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,148 +8,151 @@
     * subprocess
 
 """
 
 import subprocess
 
 import diamond.collector
-from diamond.collector import str_to_bool
 
 
 class NtpdCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(NtpdCollector, self).get_default_config_help()
-        config_help.update({
-            'ntpq_bin':     'Path to ntpq binary',
-            'ntpdc_bin':    'Path to ntpdc binary',
-            'use_sudo':     'Use sudo?',
-            'sudo_cmd':     'Path to sudo',
-        })
+        config_help.update(
+            {
+                "ntpq_bin": "Path to ntpq binary",
+                "ntpdc_bin": "Path to ntpdc binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(NtpdCollector, self).get_default_config()
-        config.update({
-            'path':         'ntpd',
-            'ntpq_bin':     self.find_binary('/usr/bin/ntpq'),
-            'ntpdc_bin':    self.find_binary('/usr/bin/ntpdc'),
-            'use_sudo':     False,
-            'sudo_cmd':     self.find_binary('/usr/bin/sudo'),
-        })
+        config.update(
+            {
+                "path": "ntpd",
+                "ntpq_bin": self.find_binary("/usr/bin/ntpq"),
+                "ntpdc_bin": self.find_binary("/usr/bin/ntpdc"),
+                "use_sudo": False,
+                "sudo_cmd": self.find_binary("/usr/bin/sudo"),
+            }
+        )
+
         return config
 
     def run_command(self, command):
         try:
-            if str_to_bool(self.config['use_sudo']):
-                command.insert(0, self.config['sudo_cmd'])
+            if diamond.collector.str_to_bool(self.config["use_sudo"]):
+                command.insert(0, self.config["sudo_cmd"])
 
-            return subprocess.Popen(command,
-                                    stdout=subprocess.PIPE).communicate()[0]
+            return subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0]
         except OSError:
             self.log.exception("Unable to run %s", command)
             return ""
 
     def get_ntpq_output(self):
-        return self.run_command([self.config['ntpq_bin'], '-np'])
+        return self.run_command([self.config["ntpq_bin"], "-np"])
 
     def get_ntpq_stats(self):
         output = self.get_ntpq_output()
-
         data = {}
 
         for line in output.splitlines():
             # Only care about system peer
-            if not line.startswith('*'):
+            if not line.startswith("*"):
                 continue
 
             parts = line[1:].split()
 
-            data['stratum'] = {'val': parts[2], 'precision': 0}
-            data['when'] = {'val': parts[4], 'precision': 0}
-            if data['when']['val'] == '-':
+            data["stratum"] = {"val": parts[2], "precision": 0}
+            data["when"] = {"val": parts[4], "precision": 0}
+
+            if data["when"]["val"] == "-":
                 # sometimes, ntpq returns value '-' for 'when', continuos
                 # and try other system peer
                 continue
-            data['poll'] = {'val': parts[5], 'precision': 0}
-            data['reach'] = {'val': parts[6], 'precision': 0}
-            data['delay'] = {'val': parts[7], 'precision': 6}
-            data['jitter'] = {'val': parts[9], 'precision': 6}
+
+            data["poll"] = {"val": parts[5], "precision": 0}
+            data["reach"] = {"val": parts[6], "precision": 0}
+            data["delay"] = {"val": parts[7], "precision": 6}
+            data["jitter"] = {"val": parts[9], "precision": 6}
 
         def convert_to_second(when_ntpd_ouput):
             value = float(when_ntpd_ouput[:-1])
-            if when_ntpd_ouput.endswith('m'):
+            if when_ntpd_ouput.endswith("m"):
                 return value * 60
-            elif when_ntpd_ouput.endswith('h'):
+            elif when_ntpd_ouput.endswith("h"):
                 return value * 3600
-            elif when_ntpd_ouput.endswith('d'):
+            elif when_ntpd_ouput.endswith("d"):
                 return value * 86400
 
-        if 'when' in data:
-            if data['when']['val'] == '-':
+        if "when" in data:
+            if data["when"]["val"] == "-":
                 self.log.warning('ntpq returned bad value for "when"')
                 return []
 
-            if data['when']['val'].endswith(('m', 'h', 'd')):
-                data['when']['val'] = convert_to_second(data['when']['val'])
+            if data["when"]["val"].endswith(("m", "h", "d")):
+                data["when"]["val"] = convert_to_second(data["when"]["val"])
 
         return data.items()
 
     def get_ntpdc_kerninfo_output(self):
-        return self.run_command([self.config['ntpdc_bin'], '-c', 'kerninfo'])
+        return self.run_command([self.config["ntpdc_bin"], "-c", "kerninfo"])
 
     def get_ntpdc_kerninfo_stats(self):
         output = self.get_ntpdc_kerninfo_output()
-
         data = {}
 
         for line in output.splitlines():
-            key, val = line.split(':')
+            key, val = line.split(":")
             val = float(val.split()[0])
 
-            if key == 'pll offset':
-                data['offset'] = {'val': val, 'precision': 10}
-            elif key == 'pll frequency':
-                data['frequency'] = {'val': val, 'precision': 6}
-            elif key == 'maximum error':
-                data['max_error'] = {'val': val, 'precision': 6}
-            elif key == 'estimated error':
-                data['est_error'] = {'val': val, 'precision': 6}
-            elif key == 'status':
-                data['status'] = {'val': val, 'precision': 0}
+            if key == "pll offset":
+                data["offset"] = {"val": val, "precision": 10}
+            elif key == "pll frequency":
+                data["frequency"] = {"val": val, "precision": 6}
+            elif key == "maximum error":
+                data["max_error"] = {"val": val, "precision": 6}
+            elif key == "estimated error":
+                data["est_error"] = {"val": val, "precision": 6}
+            elif key == "status":
+                data["status"] = {"val": val, "precision": 0}
 
         return data.items()
 
     def get_ntpdc_sysinfo_output(self):
-        return self.run_command([self.config['ntpdc_bin'], '-c', 'sysinfo'])
+        return self.run_command([self.config["ntpdc_bin"], "-c", "sysinfo"])
 
     def get_ntpdc_sysinfo_stats(self):
         output = self.get_ntpdc_sysinfo_output()
-
         data = {}
 
         for line in output.splitlines():
-            key, val = line.split(':')[0:2]
+            key, val = line.split(":")[0:2]
+
             try:
                 val = float(val.split()[0])
 
-                if key == 'root distance':
-                    data['root_distance'] = {'val': val, 'precision': 6}
-                elif key == 'root dispersion':
-                    data['root_dispersion'] = {'val': val, 'precision': 6}
+                if key == "root distance":
+                    data["root_distance"] = {"val": val, "precision": 6}
+                elif key == "root dispersion":
+                    data["root_dispersion"] = {"val": val, "precision": 6}
             except Exception:
                 pass
 
         return data.items()
 
     def collect(self):
         for stat, v in self.get_ntpq_stats():
-            self.publish(stat, v['val'], precision=v['precision'])
+            self.publish(stat, v["val"], precision=v["precision"])
 
         for stat, v in self.get_ntpdc_kerninfo_stats():
-            self.publish(stat, v['val'], precision=v['precision'])
+            self.publish(stat, v["val"], precision=v["precision"])
 
         for stat, v in self.get_ntpdc_sysinfo_stats():
-            self.publish(stat, v['val'], precision=v['precision'])
+            self.publish(stat, v["val"], precision=v["precision"])
```

### Comparing `diamond-next-4.0.515/src/collectors/diskusage/diskusage.py` & `diamond-next-5.0.0/src/collectors/diskusage/diskusage.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,271 +10,281 @@
 
 #### Dependencies
 
  * /proc/diskstats
 
 """
 
-import diamond.collector
-import diamond.convertor
-import time
 import os
 import re
+import time
+
+import diamond.collector
+import diamond.convertor
 
 try:
     import psutil
 except ImportError:
     psutil = None
 
 
 class DiskUsageCollector(diamond.collector.Collector):
-
     MAX_VALUES = {
-        'reads':                    4294967295,
-        'reads_merged':             4294967295,
-        'reads_milliseconds':       4294967295,
-        'writes':                   4294967295,
-        'writes_merged':            4294967295,
-        'writes_milliseconds':      4294967295,
-        'io_milliseconds':          4294967295,
-        'io_milliseconds_weighted': 4294967295
+        "reads": 4294967295,
+        "reads_merged": 4294967295,
+        "reads_milliseconds": 4294967295,
+        "writes": 4294967295,
+        "writes_merged": 4294967295,
+        "writes_milliseconds": 4294967295,
+        "io_milliseconds": 4294967295,
+        "io_milliseconds_weighted": 4294967295,
     }
 
     LastCollectTime = None
 
     def get_default_config_help(self):
         config_help = super(DiskUsageCollector, self).get_default_config_help()
-        config_help.update({
-            'devices': "A regex of which devices to gather metrics for." +
-                       " Defaults to md, sd, xvd, disk, and dm devices",
-            'sector_size': 'The size to use to calculate sector usage',
-            'send_zero': 'Send io data even when there is no io',
-        })
+        config_help.update(
+            {
+                "devices": "A regex of which devices to gather metrics for. Defaults to md, sd, xvd, disk, and dm devices",
+                "sector_size": "The size to use to calculate sector usage",
+                "send_zero": "Send io data even when there is no io",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(DiskUsageCollector, self).get_default_config()
-        config.update({
-            'path':     'iostat',
-            'devices':  ('PhysicalDrive[0-9]+$' +
-                         '|md[0-9]+$' +
-                         '|sd[a-z]+[0-9]*$' +
-                         '|x?vd[a-z]+[0-9]*$' +
-                         '|disk[0-9]+$' +
-                         '|dm\-[0-9]+$'),
-            'sector_size': 512,
-            'send_zero': False,
-        })
+        config.update(
+            {
+                "path": "iostat",
+                "devices": "PhysicalDrive[0-9]+$|md[0-9]+$|sd[a-z]+[0-9]*$|x?vd[a-z]+[0-9]*$|disk[0-9]+$|dm\-[0-9]+$|cciss/c[0-9]+d[0-9]+$",
+                "sector_size": 512,
+                "send_zero": False,
+            }
+        )
+
         return config
 
     def get_disk_statistics(self):
         """
         Create a map of disks in the machine.
 
         http://www.kernel.org/doc/Documentation/iostats.txt
 
         Returns:
           (major, minor) -> DiskStatistics(device, ...)
         """
         result = {}
 
-        if os.access('/proc/diskstats', os.R_OK):
+        if os.access("/proc/diskstats", os.R_OK):
             self.proc_diskstats = True
-            fp = open('/proc/diskstats')
+            fp = open("/proc/diskstats")
 
             try:
                 for line in fp:
                     try:
                         columns = line.split()
+
                         # On early linux v2.6 versions, partitions have only 4
                         # output fields not 11. From linux 2.6.25 partitions
                         # have the full stats set.
                         if len(columns) < 14:
                             continue
+
                         major = int(columns[0])
                         minor = int(columns[1])
                         device = columns[2]
 
-                        if ((device.startswith('ram') or
-                             device.startswith('loop'))):
+                        if device.startswith("ram") or device.startswith("loop"):
                             continue
 
                         result[(major, minor)] = {
-                            'device': device,
-                            'reads': float(columns[3]),
-                            'reads_merged': float(columns[4]),
-                            'reads_sectors': float(columns[5]),
-                            'reads_milliseconds': float(columns[6]),
-                            'writes': float(columns[7]),
-                            'writes_merged': float(columns[8]),
-                            'writes_sectors': float(columns[9]),
-                            'writes_milliseconds': float(columns[10]),
-                            'io_in_progress': float(columns[11]),
-                            'io_milliseconds': float(columns[12]),
-                            'io_milliseconds_weighted': float(columns[13])
+                            "device": device,
+                            "reads": float(columns[3]),
+                            "reads_merged": float(columns[4]),
+                            "reads_sectors": float(columns[5]),
+                            "reads_milliseconds": float(columns[6]),
+                            "writes": float(columns[7]),
+                            "writes_merged": float(columns[8]),
+                            "writes_sectors": float(columns[9]),
+                            "writes_milliseconds": float(columns[10]),
+                            "io_in_progress": float(columns[11]),
+                            "io_milliseconds": float(columns[12]),
+                            "io_milliseconds_weighted": float(columns[13]),
                         }
                     except ValueError:
                         continue
             finally:
                 fp.close()
         else:
             self.proc_diskstats = False
+
             if not psutil:
-                self.log.error('Unable to import psutil')
+                self.log.error("Unable to import psutil")
+
                 return None
 
             disks = psutil.disk_io_counters(True)
-            sector_size = int(self.config['sector_size'])
+            sector_size = int(self.config["sector_size"])
+
             for disk in disks:
                 result[(0, len(result))] = {
-                    'device': disk,
-                    'reads': disks[disk].read_count,
-                    'reads_sectors': disks[disk].read_bytes / sector_size,
-                    'reads_milliseconds': disks[disk].read_time,
-                    'writes': disks[disk].write_count,
-                    'writes_sectors': disks[disk].write_bytes / sector_size,
-                    'writes_milliseconds': disks[disk].write_time,
-                    'io_milliseconds':
-                        disks[disk].read_time + disks[disk].write_time,
-                    'io_milliseconds_weighted':
-                        disks[disk].read_time + disks[disk].write_time
+                    "device": disk,
+                    "reads": disks[disk].read_count,
+                    "reads_sectors": disks[disk].read_bytes / sector_size,
+                    "reads_milliseconds": disks[disk].read_time,
+                    "writes": disks[disk].write_count,
+                    "writes_sectors": disks[disk].write_bytes / sector_size,
+                    "writes_milliseconds": disks[disk].write_time,
+                    "io_milliseconds": disks[disk].read_time + disks[disk].write_time,
+                    "io_milliseconds_weighted": disks[disk].read_time
+                    + disks[disk].write_time,
                 }
 
         return result
 
     def collect(self):
-
         # Handle collection time intervals correctly
-        CollectTime = time.time()
-        time_delta = float(self.config['interval'])
+        collect_time = time.time()
+        time_delta = float(self.config["interval"])
+
         if self.LastCollectTime:
-            time_delta = CollectTime - self.LastCollectTime
+            time_delta = collect_time - self.LastCollectTime
+
         if not time_delta:
-            time_delta = float(self.config['interval'])
-        self.LastCollectTime = CollectTime
+            time_delta = float(self.config["interval"])
 
-        exp = self.config['devices']
-        reg = re.compile(exp)
+        self.LastCollectTime = collect_time
 
+        exp = self.config["devices"]
+        reg = re.compile(exp)
         results = self.get_disk_statistics()
+
         if not results:
-            self.log.error('No diskspace metrics retrieved')
+            self.log.error("No diskspace metrics retrieved")
+
             return None
 
-        for key, info in results.iteritems():
+        for key, info in iter(results.items()):
             metrics = {}
-            name = info['device']
+            name = info["device"]
+
             if not reg.match(name):
                 continue
 
-            for key, value in info.iteritems():
-                if key == 'device':
+            for key, value in iter(info.items()):
+                if key == "device":
                     continue
+
                 oldkey = key
 
-                for unit in self.config['byte_unit']:
+                for unit in self.config["byte_unit"]:
                     key = oldkey
 
-                    if key.endswith('sectors'):
-                        key = key.replace('sectors', unit)
-                        value /= (1024 / int(self.config['sector_size']))
-                        value = diamond.convertor.binary.convert(value=value,
-                                                                 oldUnit='kB',
-                                                                 newUnit=unit)
+                    if key.endswith("sectors"):
+                        key = key.replace("sectors", unit)
+                        value /= 1024 / int(self.config["sector_size"])
+                        value = diamond.convertor.binary.convert(
+                            value=value, old_unit="kB", new_unit=unit
+                        )
                         self.MAX_VALUES[key] = diamond.convertor.binary.convert(
                             value=diamond.collector.MAX_COUNTER,
-                            oldUnit='byte',
-                            newUnit=unit)
+                            old_unit="byte",
+                            new_unit=unit,
+                        )
+
+                    metric_name = ".".join([info["device"], key])
 
-                    metric_name = '.'.join([info['device'], key])
                     # io_in_progress is a point in time counter, !derivative
-                    if key != 'io_in_progress':
+                    if key != "io_in_progress":
                         metric_value = self.derivative(
-                            metric_name,
-                            value,
-                            self.MAX_VALUES[key],
-                            time_delta=False)
+                            metric_name, value, self.MAX_VALUES[key], time_delta=False
+                        )
                     else:
                         metric_value = value
 
                     metrics[key] = metric_value
 
             if self.proc_diskstats:
-                metrics['read_requests_merged_per_second'] = (
-                    metrics['reads_merged'] / time_delta)
-                metrics['write_requests_merged_per_second'] = (
-                    metrics['writes_merged'] / time_delta)
-
-            metrics['reads_per_second'] = metrics['reads'] / time_delta
-            metrics['writes_per_second'] = metrics['writes'] / time_delta
-
-            for unit in self.config['byte_unit']:
-                metric_name = 'read_%s_per_second' % unit
-                key = 'reads_%s' % unit
+                metrics["read_requests_merged_per_second"] = (
+                    metrics["reads_merged"] / time_delta
+                )
+                metrics["write_requests_merged_per_second"] = (
+                    metrics["writes_merged"] / time_delta
+                )
+
+            metrics["reads_per_second"] = metrics["reads"] / time_delta
+            metrics["writes_per_second"] = metrics["writes"] / time_delta
+
+            for unit in self.config["byte_unit"]:
+                metric_name = "read_%s_per_second" % unit
+                key = "reads_%s" % unit
                 metrics[metric_name] = metrics[key] / time_delta
 
-                metric_name = 'write_%s_per_second' % unit
-                key = 'writes_%s' % unit
+                metric_name = "write_%s_per_second" % unit
+                key = "writes_%s" % unit
                 metrics[metric_name] = metrics[key] / time_delta
 
                 # Set to zero so the nodes are valid even if we have 0 io for
                 # the metric duration
-                metric_name = 'average_request_size_%s' % unit
+                metric_name = "average_request_size_%s" % unit
                 metrics[metric_name] = 0
 
-            metrics['io'] = metrics['reads'] + metrics['writes']
+            metrics["io"] = metrics["reads"] + metrics["writes"]
 
-            metrics['average_queue_length'] = (
-                metrics['io_milliseconds_weighted'] / time_delta / 1000.0)
+            metrics["average_queue_length"] = (
+                metrics["io_milliseconds_weighted"] / time_delta / 1000.0
+            )
 
-            metrics['util_percentage'] = (
-                metrics['io_milliseconds'] / time_delta / 10.0)
+            metrics["util_percentage"] = metrics["io_milliseconds"] / time_delta / 10.0
 
-            if metrics['reads'] > 0:
-                metrics['read_await'] = (
-                    metrics['reads_milliseconds'] / metrics['reads'])
+            if metrics["reads"] > 0:
+                metrics["read_await"] = metrics["reads_milliseconds"] / metrics["reads"]
             else:
-                metrics['read_await'] = 0
+                metrics["read_await"] = 0
 
-            if metrics['writes'] > 0:
-                metrics['write_await'] = (
-                    metrics['writes_milliseconds'] / metrics['writes'])
+            if metrics["writes"] > 0:
+                metrics["write_await"] = (
+                    metrics["writes_milliseconds"] / metrics["writes"]
+                )
             else:
-                metrics['write_await'] = 0
+                metrics["write_await"] = 0
 
-            for unit in self.config['byte_unit']:
-                rkey = 'reads_%s' % unit
-                wkey = 'writes_%s' % unit
-                metric_name = 'average_request_size_%s' % unit
-                if (metrics['io'] > 0):
-                    metrics[metric_name] = (
-                        metrics[rkey] + metrics[wkey]) / metrics['io']
+            for unit in self.config["byte_unit"]:
+                rkey = "reads_%s" % unit
+                wkey = "writes_%s" % unit
+                metric_name = "average_request_size_%s" % unit
+
+                if metrics["io"] > 0:
+                    metrics[metric_name] = (metrics[rkey] + metrics[wkey]) / metrics[
+                        "io"
+                    ]
                 else:
                     metrics[metric_name] = 0
 
-            metrics['iops'] = metrics['io'] / time_delta
+            metrics["iops"] = metrics["io"] / time_delta
 
-            if (metrics['io'] > 0):
-                metrics['service_time'] = (
-                    metrics['io_milliseconds'] / metrics['io'])
-                metrics['await'] = (
-                    metrics['reads_milliseconds'] +
-                    metrics['writes_milliseconds']) / metrics['io']
+            if metrics["io"] > 0:
+                metrics["service_time"] = metrics["io_milliseconds"] / metrics["io"]
+                metrics["await"] = (
+                    metrics["reads_milliseconds"] + metrics["writes_milliseconds"]
+                ) / metrics["io"]
             else:
-                metrics['service_time'] = 0
-                metrics['await'] = 0
+                metrics["service_time"] = 0
+                metrics["await"] = 0
 
             # http://www.scribd.com/doc/15013525
             # Page 28
-            metrics['concurrent_io'] = (
-                (metrics['reads_per_second'] + metrics['writes_per_second']) *
-                (metrics['service_time'] / 1000.0))
+            metrics["concurrent_io"] = (
+                metrics["reads_per_second"] + metrics["writes_per_second"]
+            ) * (metrics["service_time"] / 1000.0)
 
             # Only publish when we have io figures
-            if (metrics['io'] > 0 or self.config['send_zero']):
+            if metrics["io"] > 0 or self.config["send_zero"]:
                 for key in metrics:
-                    metric_name = '.'.join([info['device'], key]).replace(
-                        '/', '_')
+                    metric_name = ".".join([info["device"], key]).replace("/", "_")
                     self.publish(metric_name, metrics[key], precision=3)
```

### Comparing `diamond-next-4.0.515/src/collectors/unbound/unbound.py` & `diamond-next-5.0.0/src/collectors/unbound/unbound.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,86 +5,88 @@
 
 #### Dependencies
 
     * collections.defaultdict or kitchen
 
 """
 
+import diamond.collector
+from diamond.collector import str_to_bool
+
 try:
     from collections import defaultdict
 except ImportError:
     from kitchen.pycompat25.collections import defaultdict
 
-import diamond.collector
-from diamond.collector import str_to_bool
-
 
 class UnboundCollector(diamond.collector.ProcessCollector):
-
     def get_default_config_help(self):
         config_help = super(UnboundCollector, self).get_default_config_help()
-        config_help.update({
-            'bin':          'Path to unbound-control binary',
-            'histogram':    'Include histogram in collection',
-        })
+        config_help.update(
+            {
+                "bin": "Path to unbound-control binary",
+                "histogram": "Include histogram in collection",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(UnboundCollector, self).get_default_config()
-        config.update({
-            'path':         'unbound',
-            'bin':          self.find_binary('/usr/sbin/unbound-control'),
-            'histogram':    True,
-        })
+        config.update(
+            {
+                "path": "unbound",
+                "bin": self.find_binary("/usr/sbin/unbound-control"),
+                "histogram": True,
+            }
+        )
         return config
 
     def get_massaged_histogram(self, raw_histogram):
         histogram = defaultdict(int)
 
         for intv in sorted(raw_histogram.keys()):
             if intv <= 0.001024:
                 # Let's compress <1ms into 1 data point
-                histogram['1ms'] += raw_histogram[intv]
+                histogram["1ms"] += raw_histogram[intv]
             elif intv < 1.0:
-                # Convert to ms and since we're using the upper limit
-                # divide by 2 for lower limit
-                intv_name = ''.join([str(int(intv / 0.001024 / 2)), 'ms+'])
+                # Convert to ms and since we're using the upper limit divide by 2 for lower limit
+                intv_name = "".join([str(int(intv / 0.001024 / 2)), "ms+"])
                 histogram[intv_name] = raw_histogram[intv]
             elif intv == 1.0:
-                histogram['512ms+'] = raw_histogram[intv]
-            elif intv > 1.0 and intv <= 64.0:
+                histogram["512ms+"] = raw_histogram[intv]
+            elif 1.0 < intv <= 64.0:
                 # Convert upper limit into lower limit seconds
-                intv_name = ''.join([str(int(intv / 2)), 's+'])
+                intv_name = "".join([str(int(intv / 2)), "s+"])
                 histogram[intv_name] = raw_histogram[intv]
             else:
                 # Compress everything >64s into 1 data point
-                histogram['64s+'] += raw_histogram[intv]
+                histogram["64s+"] += raw_histogram[intv]
 
         return histogram
 
     def collect(self):
-        stats_output = self.run_command([' stats'])
+        stats_output = self.run_command([" stats"])
+
         if stats_output is None:
             return
 
         stats_output = stats_output[0]
-
         raw_histogram = {}
+        include_hist = str_to_bool(self.config["histogram"])
 
-        include_hist = str_to_bool(self.config['histogram'])
         for line in stats_output.splitlines():
-            stat_name, stat_value = line.split('=')
+            stat_name, stat_value = line.split("=")
 
-            if not stat_name.startswith('histogram'):
+            if not stat_name.startswith("histogram"):
                 self.publish(stat_name, stat_value)
             elif include_hist:
-                hist_intv = float(stat_name.split('.', 4)[4])
+                hist_intv = float(stat_name.split(".", 4)[4])
                 raw_histogram[hist_intv] = float(stat_value)
 
         if include_hist:
             histogram = self.get_massaged_histogram(raw_histogram)
 
-            for intv, value in histogram.iteritems():
-                self.publish('histogram.' + intv, value)
+            for intv, value in iter(histogram.items()):
+                self.publish("histogram." + intv, value)
```

### Comparing `diamond-next-4.0.515/src/collectors/snmp/snmp.py` & `diamond-next-5.0.0/src/collectors/snmp/snmp.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,17 +6,18 @@
 #### Dependencies
 
  * pysnmp
 
 """
 
 import socket
-
 import warnings
 
+import diamond.collector
+
 # pysnmp packages on debian 6.0 use sha and md5 which are deprecated
 # packages. there is nothing to be done about it until pysnmp
 # updates to use new hashlib module -- ignoring warning for now
 old_showwarning = warnings.showwarning
 warnings.filterwarnings("ignore", category=DeprecationWarning)
 
 cmdgen = None
@@ -26,53 +27,54 @@
     import pysnmp.debug
 except ImportError:
     pysnmp = None
     cmdgen = None
 
 warnings.showwarning = old_showwarning
 
-import diamond.collector
-
 
 class SNMPCollector(diamond.collector.Collector):
-
     def __init__(self, *args, **kwargs):
         super(SNMPCollector, self).__init__(*args, **kwargs)
-        self.snmpCmdGen = cmdgen.CommandGenerator()
+
+        if cmdgen is not None:
+            self.snmpCmdGen = cmdgen.CommandGenerator()
 
     def get_default_config_help(self):
         config_help = super(SNMPCollector, self).get_default_config_help()
-        config_help.update({
-            'timeout': 'Seconds before timing out the snmp connection',
-            'retries': 'Number of times to retry before bailing',
-        })
+        config_help.update(
+            {
+                "timeout": "Seconds before timing out the snmp connection",
+                "retries": "Number of times to retry before bailing",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         # Initialize default config
         default_config = super(SNMPCollector, self).get_default_config()
-        default_config['path_suffix'] = ''
-        default_config['path_prefix'] = 'systems'
-        default_config['timeout'] = 5
-        default_config['retries'] = 3
+        default_config["path_suffix"] = ""
+        default_config["path_prefix"] = "systems"
+        default_config["timeout"] = 5
+        default_config["retries"] = 3
         # Return default config
         return default_config
 
     def _convert_to_oid(self, s):
         d = s.split(".")
         return tuple([int(x) for x in d])
 
     def _convert_from_oid(self, oid):
         return ".".join([str(x) for x in oid])
 
     def collect(self):
-        for device in self.config['devices']:
-            host = self.config['devices'][device]['host']
-            port = self.config['devices'][device]['port']
-            community = self.config['devices'][device]['community']
+        for device in self.config["devices"]:
+            host = self.config["devices"][device]["host"]
+            port = self.config["devices"][device]["port"]
+            community = self.config["devices"][device]["community"]
             self.collect_snmp(device, host, port, community)
 
     def get(self, oid, host, port, community):
         """
         Perform SNMP get for a given OID
         """
         # Initialize return value
@@ -82,31 +84,28 @@
         if not isinstance(oid, tuple):
             oid = self._convert_to_oid(oid)
 
         # Convert Host to IP if necessary
         host = socket.gethostbyname(host)
 
         # Assemble SNMP Auth Data
-        snmpAuthData = cmdgen.CommunityData(
-            'agent-{0}'.format(community),
-            community)
+        snmp_auth_data = cmdgen.CommunityData("agent-{}".format(community), community)
 
         # Assemble SNMP Transport Data
-        snmpTransportData = cmdgen.UdpTransportTarget(
-            (host, port),
-            int(self.config['timeout']),
-            int(self.config['retries']))
+        snmp_transport_data = cmdgen.UdpTransportTarget(
+            (host, port), int(self.config["timeout"]), int(self.config["retries"])
+        )
 
         # Assemble SNMP Next Command
-        result = self.snmpCmdGen.getCmd(snmpAuthData, snmpTransportData, oid)
-        varBind = result[3]
+        result = self.snmpCmdGen.getCmd(snmp_auth_data, snmp_transport_data, oid)
+        var_bind = result[3]
 
         # TODO: Error check
 
-        for o, v in varBind:
+        for o, v in var_bind:
             ret[str(o)] = v.prettyPrint()
 
         return ret
 
     def walk(self, oid, host, port, community):
         """
         Perform an SNMP walk on a given OID
@@ -118,30 +117,25 @@
         if not isinstance(oid, tuple):
             oid = self._convert_to_oid(oid)
 
         # Convert Host to IP if necessary
         host = socket.gethostbyname(host)
 
         # Assemble SNMP Auth Data
-        snmpAuthData = cmdgen.CommunityData(
-            'agent-{0}'.format(community),
-            community)
+        snmp_auth_data = cmdgen.CommunityData("agent-{}".format(community), community)
 
         # Assemble SNMP Transport Data
-        snmpTransportData = cmdgen.UdpTransportTarget(
-            (host, port),
-            int(self.config['timeout']),
-            int(self.config['retries']))
+        snmp_transport_data = cmdgen.UdpTransportTarget(
+            (host, port), int(self.config["timeout"]), int(self.config["retries"])
+        )
 
         # Assemble SNMP Next Command
-        resultTable = self.snmpCmdGen.nextCmd(snmpAuthData,
-                                              snmpTransportData,
-                                              oid)
-        varBindTable = resultTable[3]
+        result_table = self.snmpCmdGen.nextCmd(snmp_auth_data, snmp_transport_data, oid)
+        var_bind_table = result_table[3]
 
         # TODO: Error Check
 
-        for varBindTableRow in varBindTable:
+        for varBindTableRow in var_bind_table:
             for o, v in varBindTableRow:
                 ret[str(o)] = v.prettyPrint()
 
         return ret
```

### Comparing `diamond-next-4.0.515/src/collectors/pgq/pgq.py` & `diamond-next-5.0.0/src/collectors/pgq/pgq.py`

 * *Files 12% similar despite different names*

```diff
@@ -16,96 +16,106 @@
 [[database1]]
 dsn = postgresql://user:secret@localhost
 
 [[database2]]
 dsn = host=localhost port=5432 dbname=mydb
 ```
 """
+
+import diamond.collector
+
 try:
     import psycopg2
     import psycopg2.extras
 except ImportError:
     psycopg2 = None
 
-import diamond.collector
-
 
 class PgQCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(PgQCollector, self).get_default_config_help()
-        config_help.update({
-            "instances": "The databases to be monitored. Each should have a "
-                         "`dsn` attribute, which must be a valid libpq "
-                         "connection string."
-        })
+        config_help.update(
+            {
+                "instances": "The databases to be monitored. Each should have a "
+                "`dsn` attribute, which must be a valid libpq "
+                "connection string."
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         config = super(PgQCollector, self).get_default_config()
-        config.update({
-            'instances': {},
-        })
+        config.update(
+            {
+                "instances": {},
+            }
+        )
+
         return config
 
     def collect(self):
         if psycopg2 is None:
-            self.log.error('Unable to import module psycopg2')
+            self.log.error("Unable to import module psycopg2")
+
             return None
 
-        for instance, configuration in self.config['instances'].iteritems():
-            connection = psycopg2.connect(configuration['dsn'])
+        for instance, configuration in iter(self.config["instances"].items()):
+            connection = psycopg2.connect(configuration["dsn"])
             connection.set_isolation_level(
                 psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT,
             )
             self._collect_for_instance(instance, connection)
 
     def _collect_for_instance(self, instance, connection):
-        "Collects metrics for a named connection."
+        """Collects metrics for a named connection."""
         with connection.cursor() as cursor:
             for queue, metrics in self.get_queue_info(instance, cursor):
                 for name, metric in metrics.items():
-                    self.publish('.'.join((instance, queue, name)), metric)
+                    self.publish(".".join((instance, queue, name)), metric)
 
         with connection.cursor() as cursor:
             consumers = self.get_consumer_info(instance, cursor)
+
             for queue, consumer, metrics in consumers:
                 for name, metric in metrics.items():
-                    key_parts = (instance, queue, 'consumers', consumer, name)
-                    self.publish('.'.join(key_parts), metric)
+                    key_parts = (instance, queue, "consumers", consumer, name)
+                    self.publish(".".join(key_parts), metric)
 
     QUEUE_INFO_STATEMENT = """
         SELECT
             queue_name,
             EXTRACT(epoch from ticker_lag),
             ev_per_sec
         FROM pgq.get_queue_info()
     """
 
     def get_queue_info(self, instance, cursor):
-        "Collects metrics for all queues on the connected database."
+        """Collects metrics for all queues on the connected database."""
         cursor.execute(self.QUEUE_INFO_STATEMENT)
+
         for queue_name, ticker_lag, ev_per_sec in cursor:
             yield queue_name, {
-                'ticker_lag': ticker_lag,
-                'ev_per_sec': ev_per_sec,
+                "ticker_lag": ticker_lag,
+                "ev_per_sec": ev_per_sec,
             }
 
     CONSUMER_INFO_STATEMENT = """
         SELECT
             queue_name,
             consumer_name,
             EXTRACT(epoch from lag),
             pending_events,
             EXTRACT(epoch from last_seen)
         FROM pgq.get_consumer_info()
     """
 
     def get_consumer_info(self, instance, cursor):
-        "Collects metrics for all consumers on the connected database."
+        """Collects metrics for all consumers on the connected database."""
         cursor.execute(self.CONSUMER_INFO_STATEMENT)
+
         for queue_name, consumer_name, lag, pending_events, last_seen in cursor:
             yield queue_name, consumer_name, {
-                'lag': lag,
-                'pending_events': pending_events,
-                'last_seen': last_seen,
+                "lag": lag,
+                "pending_events": pending_events,
+                "last_seen": last_seen,
             }
```

### Comparing `diamond-next-4.0.515/src/collectors/rabbitmq/rabbitmq.py` & `diamond-next-5.0.0/src/collectors/rabbitmq/rabbitmq.py`

 * *Files 17% similar despite different names*

```diff
@@ -17,248 +17,298 @@
   ** If a [vhosts] section exists but is empty, then no queues will be polled.
   ** To poll all vhosts and all queues, add the following.
   **   [vhosts]
   **   * = *
   **
 """
 
-import diamond.collector
+import base64
 import re
-from urlparse import urljoin
-from urllib import quote
-import urllib2
-from base64 import b64encode
+import urllib.parse
+import urllib.request
+
+import diamond.collector
 
 try:
     import json
 except ImportError:
     import simplejson as json
 
 
 class RabbitMQClient(object):
     """
     Tiny interface into the rabbit http api
     """
 
-    def __init__(self, host, user, password, timeout=5, scheme="http"):
-        self.base_url = '%s://%s/api/' % (scheme, host)
+    def __init__(self, log, host, user, password, timeout=5, scheme="http"):
+        self.log = log
+        self.base_url = "%s://%s/api/" % (scheme, host)
         self.timeout = timeout
-        self._authorization = 'Basic ' + b64encode('%s:%s' % (user, password))
+        base64string = base64.b64encode(bytes("%s:%s" % (user, password), "utf-8"))
+        self._authorization = "Basic %s" % base64string
 
     def do_call(self, path):
-        url = urljoin(self.base_url, path)
-        req = urllib2.Request(url)
-        req.add_header('Authorization', self._authorization)
-        return json.load(urllib2.urlopen(req, timeout=self.timeout))
+        url = urllib.parse.urljoin(self.base_url, path)
+        req = urllib.request.Request(url)
+        req.add_header("Authorization", self._authorization)
+
+        return json.load(urllib.request.urlopen(req, timeout=self.timeout))
 
     def get_all_vhosts(self):
-        return self.do_call('vhosts')
+        return self.do_call("vhosts")
 
     def get_vhost_names(self):
-        return [i['name'] for i in self.get_all_vhosts()]
+        return [i["name"] for i in self.get_all_vhosts()]
+
+    def get_queue(self, vhost, queue_name):
+        path = "queues"
 
-    def get_queues(self, vhost=None):
-        path = 'queues'
         if vhost:
-            vhost = quote(vhost, '')
-            path += '/%s' % vhost
+            vhost = urllib.parse.quote(vhost, "")
+            queue_name = urllib.parse.quote(queue_name, "")
+            path += "/%s/%s" % (vhost, queue_name)
 
-        queues = self.do_call(path)
-        return queues or []
+        try:
+            queue = self.do_call(path)
+            return queue or None
+        except Exception as e:
+            self.log.error("Error querying queue %s/%s: %s" % (vhost, queue_name, e))
+            return None
+
+    def get_queues(self, vhost):
+        path = "queues"
+        vhost = urllib.parse.quote(vhost, "")
+        path += "/%s" % vhost
+
+        try:
+            queues = self.do_call(path)
+            return queues or []
+        except Exception as e:
+            self.log.error("Error querying queues %s: %s" % (vhost, e))
+            return []
 
     def get_overview(self):
-        return self.do_call('overview')
+        return self.do_call("overview")
 
     def get_nodes(self):
-        return self.do_call('nodes')
+        return self.do_call("nodes")
 
     def get_node(self, node):
-        return self.do_call('nodes/%s' % node)
+        return self.do_call("nodes/%s" % node)
 
 
 class RabbitMQCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(RabbitMQCollector, self).get_default_config_help()
-        config_help.update({
-            'host': 'Hostname and port to collect from',
-            'user': 'Username',
-            'password': 'Password',
-            'replace_dot':
-            'A value to replace dot in queue names and vhosts names by',
-            'replace_slash':
-            'A value to replace a slash in queue names and vhosts names by',
-            'queues': 'Queues to publish. Leave empty to publish all.',
-            'vhosts':
-            'A list of vhosts and queues for which we want to collect',
-            'queues_ignored':
-            'A list of queues or regexes for queue names not to report on.',
-            'cluster':
-            'If this node is part of a cluster, will collect metrics on the'
-            ' cluster health'
-        })
+        config_help.update(
+            {
+                "host": "Hostname and port to collect from",
+                "user": "Username",
+                "password": "Password",
+                "replace_dot": "A value to replace dot in queue names and vhosts names by",
+                "replace_slash": "A value to replace a slash in queue names and vhosts names by",
+                "queues": "Queues to publish. Leave empty to publish all.",
+                "vhosts": "A list of vhosts and queues for which we want to collect",
+                "queues_ignored": "A list of queues or regexes for queue names not to report on.",
+                "cluster": "If this node is part of a cluster, will collect metrics on the cluster health",
+                "query_individual_queues": "If specific queues are set, query their metrics individually."
+                " When this is False, queue metrics will be queried in bulk and"
+                " filtered, which can time out for vhosts with many queues.",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(RabbitMQCollector, self).get_default_config()
-        config.update({
-            'path': 'rabbitmq',
-            'host': 'localhost:55672',
-            'user': 'guest',
-            'password': 'guest',
-            'replace_dot': False,
-            'replace_slash': False,
-            'queues_ignored': '',
-            'cluster': False,
-            'scheme': 'http',
-        })
+        config.update(
+            {
+                "path": "rabbitmq",
+                "host": "localhost:55672",
+                "user": "guest",
+                "password": "guest",
+                "replace_dot": False,
+                "replace_slash": False,
+                "queues_ignored": "",
+                "cluster": False,
+                "scheme": "http",
+                "query_individual_queues": False,
+            }
+        )
+
         return config
 
     def collect_health(self):
         health_metrics = [
-            'fd_used',
-            'fd_total',
-            'mem_used',
-            'mem_limit',
-            'sockets_used',
-            'sockets_total',
-            'disk_free_limit',
-            'disk_free',
-            'proc_used',
-            'proc_total',
+            "fd_used",
+            "fd_total",
+            "mem_used",
+            "mem_limit",
+            "sockets_used",
+            "sockets_total",
+            "disk_free_limit",
+            "disk_free",
+            "proc_used",
+            "proc_total",
         ]
+
         try:
-            client = RabbitMQClient(self.config['host'],
-                                    self.config['user'],
-                                    self.config['password'],
-                                    scheme=self.config['scheme'])
-            node_name = client.get_overview()['node']
+            client = RabbitMQClient(
+                self.log,
+                self.config["host"],
+                self.config["user"],
+                self.config["password"],
+                scheme=self.config["scheme"],
+            )
+            node_name = client.get_overview()["node"]
             node_data = client.get_node(node_name)
+
             for metric in health_metrics:
-                self.publish('health.{0}'.format(metric), node_data[metric])
-            if self.config['cluster']:
-                self.publish('cluster.partitions',
-                             len(node_data['partitions']))
+                self.publish("health.{}".format(metric), node_data[metric])
+
+            if self.config["cluster"]:
+                self.publish("cluster.partitions", len(node_data["partitions"]))
                 content = client.get_nodes()
-                self.publish('cluster.nodes', len(content))
-        except Exception, e:
-            self.log.error('Couldnt connect to rabbitmq %s', e)
+                self.publish("cluster.nodes", len(content))
+        except:
+            self.log.exception("Could not connect to rabbitmq")
             return {}
 
-    def collect(self):
-        self.collect_health()
+    def get_queue_metrics(self, client, vhost, queues):
+        # Allow the use of a asterix to glob the queues, but replace
+        # with a empty string to match how legacy config was.
+        if queues == "*":
+            queues = ""
+
+        allowed_queues = queues.split()
         matchers = []
-        if self.config['queues_ignored']:
-            for reg in self.config['queues_ignored'].split():
+
+        if self.config["queues_ignored"]:
+            for reg in self.config["queues_ignored"].split():
                 matchers.append(re.compile(reg))
-        try:
-            client = RabbitMQClient(self.config['host'],
-                                    self.config['user'],
-                                    self.config['password'],
-                                    scheme=self.config['scheme'])
-
-            legacy = False
-
-            if 'vhosts' not in self.config:
-                legacy = True
-
-                if 'queues' in self.config:
-                    vhost_conf = {"*": self.config['queues']}
-                else:
-                    vhost_conf = {"*": ""}
 
+        if len(allowed_queues) and self.config["query_individual_queues"]:
+            for queue_name in allowed_queues:
+                if matchers and any([m.match(queue_name) for m in matchers]):
+                    continue
+
+                queue = client.get_queue(vhost, queue_name)
+
+                if queue is not None:
+                    yield queue
+        else:
+            for queue in client.get_queues(vhost):
+                # If queues are defined and it doesn't match, then skip.
+                if queue["name"] not in allowed_queues and len(allowed_queues) > 0:
+                    continue
+
+                if matchers and any([m.match(queue["name"]) for m in matchers]):
+                    continue
+
+                yield queue
+
+    def get_vhost_conf(self, vhost_names):
+        legacy = False
+
+        if "vhosts" in self.config:
+            vhost_conf = self.config["vhosts"]
+        else:
             # Legacy configurations, those that don't include the [vhosts]
             # section require special care so that we do not break metric
-            # gathering for people that were using this collector before the
-            # update to support vhosts.
+            # gathering for people that were using this collector before
+            # the update to support vhosts.
+            legacy = True
 
-            if not legacy:
-                vhost_names = client.get_vhost_names()
-                if "*" in self.config['vhosts']:
-                    for vhost in vhost_names:
-                        # Copy the glob queue list to each vhost not
-                        # specifically defined in the configuration.
-                        if vhost not in self.config['vhosts']:
-                            self.config['vhosts'][vhost] = self.config[
-                                'vhosts']['*']
-
-                    del self.config['vhosts']["*"]
-                vhost_conf = self.config['vhosts']
-
-            # Iterate all vhosts in our vhosts configuration. For legacy this
-            # is "*" to force a single run.
-            for vhost in vhost_conf:
+            if "queues" in self.config:
+                vhost_conf = {"*": self.config["queues"]}
+            else:
+                vhost_conf = {"*": ""}
+
+        if "*" in vhost_conf:
+            for vhost in vhost_names:
+                # Copy the glob queue list to each vhost not
+                # specifically defined in the configuration.
+                if vhost not in vhost_conf:
+                    vhost_conf[vhost] = vhost_conf["*"]
+
+            del vhost_conf["*"]
+
+        return vhost_conf, legacy
+
+    def collect(self):
+        self.collect_health()
+
+        try:
+            client = RabbitMQClient(
+                self.log,
+                self.config["host"],
+                self.config["user"],
+                self.config["password"],
+                scheme=self.config["scheme"],
+            )
+
+            vhost_names = client.get_vhost_names()
+            vhost_conf, legacy = self.get_vhost_conf(vhost_names)
+
+            # Iterate all vhosts in our vhosts configurations
+            for vhost, queues in iter(vhost_conf.items()):
                 vhost_name = vhost
-                if self.config['replace_dot']:
-                    vhost_name = vhost_name.replace(
-                        '.', self.config['replace_dot'])
-
-                if self.config['replace_slash']:
-                    vhost_name = vhost_name.replace(
-                        '/', self.config['replace_slash'])
-
-                queues = vhost_conf[vhost]
-
-                # Allow the use of a asterix to glob the queues, but replace
-                # with a empty string to match how legacy config was.
-                if queues == "*":
-                    queues = ""
-                allowed_queues = queues.split()
-
-                # When we fetch queues, we do not want to define a vhost if
-                # legacy.
-                if legacy:
-                    vhost = None
-
-                for queue in client.get_queues(vhost):
-                    # If queues are defined and it doesn't match, then skip.
-                    if ((queue['name'] not in allowed_queues and
-                         len(allowed_queues) > 0)):
-                        continue
-                    if matchers and any(
-                            [m.match(queue['name']) for m in matchers]):
-                        continue
+
+                if self.config["replace_dot"]:
+                    vhost_name = vhost_name.replace(".", self.config["replace_dot"])
+
+                if self.config["replace_slash"]:
+                    vhost_name = vhost_name.replace("/", self.config["replace_slash"])
+
+                for queue in self.get_queue_metrics(client, vhost, queues):
                     for key in queue:
                         prefix = "queues"
+
                         if not legacy:
                             prefix = "vhosts.%s.%s" % (vhost_name, "queues")
 
-                        queue_name = queue['name']
-                        if self.config['replace_dot']:
+                        queue_name = queue["name"]
+
+                        if self.config["replace_dot"]:
                             queue_name = queue_name.replace(
-                                '.', self.config['replace_dot'])
+                                ".", self.config["replace_dot"]
+                            )
 
-                        if self.config['replace_slash']:
+                        if self.config["replace_slash"]:
                             queue_name = queue_name.replace(
-                                '/', self.config['replace_slash'])
+                                "/", self.config["replace_slash"]
+                            )
 
-                        name = '{0}.{1}'.format(prefix, queue_name)
+                        name = "{}.{}".format(prefix, queue_name)
 
                         self._publish_metrics(name, [], key, queue)
 
             overview = client.get_overview()
+
             for key in overview:
-                self._publish_metrics('', [], key, overview)
-        except Exception, e:
-            self.log.error('An error occurred collecting from RabbitMQ, %s', e)
+                self._publish_metrics("", [], key, overview)
+        except:
+            self.log.exception("An error occurred collecting from RabbitMQ")
             return {}
 
     def _publish_metrics(self, name, prev_keys, key, data):
         """Recursively publish keys"""
         value = data[key]
         keys = prev_keys + [key]
+
         if isinstance(value, dict):
             for new_key in value:
                 self._publish_metrics(name, keys, new_key, value)
-        elif isinstance(value, (float, int, long)):
-            joined_keys = '.'.join(keys)
+        elif isinstance(value, (float, int)):
+            joined_keys = ".".join(keys)
+
             if name:
-                publish_key = '{0}.{1}'.format(name, joined_keys)
+                publish_key = "{}.{}".format(name, joined_keys)
             else:
                 publish_key = joined_keys
             if isinstance(value, bool):
                 value = int(value)
 
             self.publish(publish_key, value)
```

### Comparing `diamond-next-4.0.515/src/collectors/httpd/httpd.py` & `diamond-next-5.0.0/src/collectors/httpd/httpd.py`

 * *Files 25% similar despite different names*

```diff
@@ -2,145 +2,211 @@
 
 """
 Collect stats from Apache HTTPD server using mod_status
 
 #### Dependencies
 
  * mod_status
- * httplib
- * urlparse
 
 """
 
+import http.client
 import re
-import httplib
-import urlparse
+import urllib.parse
+
 import diamond.collector
 
 
 class HttpdCollector(diamond.collector.Collector):
-
     def process_config(self):
         super(HttpdCollector, self).process_config()
-        if 'url' in self.config:
-            self.config['urls'].append(self.config['url'])
+
+        if "url" in self.config:
+            self.config["urls"].append(self.config["url"])
 
         self.urls = {}
-        if isinstance(self.config['urls'], basestring):
-            self.config['urls'] = self.config['urls'].split(',')
 
-        for url in self.config['urls']:
+        if isinstance(self.config["urls"], str):
+            self.config["urls"] = self.config["urls"].split(",")
+
+        for url in self.config["urls"]:
             # Handle the case where there is a trailing comman on the urls list
             if len(url) == 0:
                 continue
-            if ' ' in url:
-                parts = url.split(' ')
+
+            if " " in url:
+                parts = url.split(" ")
                 self.urls[parts[0]] = parts[1]
             else:
-                self.urls[''] = url
+                self.urls[""] = url
 
     def get_default_config_help(self):
         config_help = super(HttpdCollector, self).get_default_config_help()
-        config_help.update({
-            'urls': "Urls to server-status in auto format, comma seperated," +
-                    " Format 'nickname http://host:port/server-status?auto, " +
-                    ", nickname http://host:port/server-status?auto, etc'",
-        })
+        config_help.update(
+            {
+                "urls": "Urls to server-status in auto format, comma seperated,"
+                + " Format 'nickname http://host:port/server-status?auto, "
+                + ", nickname http://host:port/server-status?auto, etc'",
+                "max_redirects": "The maximum number of redirect requests to follow.",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(HttpdCollector, self).get_default_config()
-        config.update({
-            'path':     'httpd',
-            'urls':     ['localhost http://localhost:8080/server-status?auto']
-        })
+        config.update(
+            {
+                "path": "httpd",
+                "urls": ["localhost http://localhost:8080/server-status?auto"],
+                "max_redirects": 5,
+            }
+        )
+
         return config
 
     def collect(self):
         for nickname in self.urls.keys():
             url = self.urls[nickname]
 
             try:
-                while True:
+                redirects = 0
 
+                while True:
                     # Parse Url
-                    parts = urlparse.urlparse(url)
+                    parts = urllib.parse.urlparse(url)
 
-                    # Parse host and port
-                    endpoint = parts[1].split(':')
-                    if len(endpoint) > 1:
-                        service_host = endpoint[0]
-                        service_port = int(endpoint[1])
+                    # Set httplib class
+                    if parts.scheme == "http":
+                        connection = http.client.HTTPConnection(parts.netloc)
+                    elif parts.scheme == "https":
+                        connection = http.client.HTTPSConnection(parts.netloc)
                     else:
-                        service_host = endpoint[0]
-                        service_port = 80
+                        raise Exception("Invalid scheme: %s" % parts.scheme)
 
                     # Setup Connection
-                    connection = httplib.HTTPConnection(service_host,
-                                                        service_port)
-
-                    url = "%s?%s" % (parts[2], parts[4])
-
+                    url = "%s?%s" % (parts.path, parts.query)
                     connection.request("GET", url)
                     response = connection.getresponse()
                     data = response.read()
                     headers = dict(response.getheaders())
-                    if (('location' not in headers or
-                         headers['location'] == url)):
+
+                    if "location" not in headers or headers["location"] == url:
                         connection.close()
                         break
-                    url = headers['location']
+
+                    url = headers["location"]
                     connection.close()
-            except Exception, e:
-                self.log.error(
-                    "Error retrieving HTTPD stats for host %s:%s, url '%s': %s",
-                    service_host, str(service_port), url, e)
+
+                    redirects += 1
+
+                    if redirects > self.config["max_redirects"]:
+                        raise Exception("Too many redirects!")
+
+            except Exception as e:
+                self.log.error("Error retrieving HTTPD stats for '%s': %s", url, e)
                 continue
 
-            exp = re.compile('^([A-Za-z ]+):\s+(.+)$')
-            for line in data.split('\n'):
+            exp = re.compile("^([A-Za-z ]+):\s+(.+)$")
+
+            for line in data.split("\n"):
                 if line:
                     m = exp.match(line)
+
                     if m:
                         k = m.group(1)
                         v = m.group(2)
 
                         # IdleWorkers gets determined from the scoreboard
-                        if k == 'IdleWorkers':
+                        if k == "IdleWorkers":
                             continue
 
-                        if k == 'Scoreboard':
-                            for sb_kv in self._parseScoreboard(v):
+                        if k == "Scoreboard":
+                            for sb_kv in self._parse_scoreboard(v):
                                 self._publish(nickname, sb_kv[0], sb_kv[1])
                         else:
                             self._publish(nickname, k, v)
 
     def _publish(self, nickname, key, value):
 
-        metrics = ['ReqPerSec', 'BytesPerSec', 'BytesPerReq', 'BusyWorkers',
-                   'Total Accesses', 'IdleWorkers', 'StartingWorkers',
-                   'ReadingWorkers', 'WritingWorkers', 'KeepaliveWorkers',
-                   'DnsWorkers', 'ClosingWorkers', 'LoggingWorkers',
-                   'FinishingWorkers', 'CleanupWorkers']
-
-        metrics_precision = ['ReqPerSec', 'BytesPerSec', 'BytesPerReq']
+        metrics = [
+            "ReqPerSec",
+            "BytesPerSec",
+            "BytesPerReq",
+            "BusyWorkers",
+            "Total Accesses",
+            "IdleWorkers",
+            "StartingWorkers",
+            "ReadingWorkers",
+            "WritingWorkers",
+            "KeepaliveWorkers",
+            "DnsWorkers",
+            "ClosingWorkers",
+            "LoggingWorkers",
+            "FinishingWorkers",
+            "CleanupWorkers",
+            "ConnsAsyncClosing",
+            "CPUUser",
+            "CacheSubcaches",
+            "CacheCurrentEntries",
+            "CPULoad",
+            "Total kBytes",
+            "CacheIndexesPerSubcaches",
+            "CPUChildrenSystem",
+            "ConnsAsyncWriting",
+            "CacheSharedMemory",
+            "ServerUptimeSeconds",
+            "CacheStoreCount",
+            "CacheExpireCount",
+            "CacheReplaceCount",
+            "CPUChildrenUser",
+            "ConnsTotal",
+            "CacheRetrieveMissCount",
+            "CacheRetrieveHitCount",
+            "CacheTimeLeftOldestMax",
+            "CacheDiscardCount",
+            "CacheRemoveHitCount",
+            "CacheTimeLeftOldestMin",
+            "CPUSystem",
+            "ConnsAsyncKeepAlive",
+            "CacheTimeLeftOldestAvg",
+            "CacheRemoveMissCount",
+            "CacheIndexUsage",
+            "CacheUsage",
+        ]
+
+        metrics_precision = [
+            "ReqPerSec",
+            "BytesPerSec",
+            "BytesPerReq",
+            "CPULoad",
+            "CPUUser",
+            "CPUSystem",
+        ]
 
         if key in metrics:
             # Get Metric Name
             presicion_metric = False
-            metric_name = "%s" % re.sub('\s+', '', key)
+            metric_name = "%s" % re.sub("\s+", "", key)
+
             if metric_name in metrics_precision:
                 presicion_metric = 1
 
             # Prefix with the nickname?
             if len(nickname) > 0:
-                metric_name = nickname + '.' + metric_name
+                metric_name = nickname + "." + metric_name
+
+            # Strip percent mark from Cache*Usage
+            try:
+                value = value.replace("%", "")
+            except AttributeError:
+                pass
 
             # Use precision for ReqPerSec BytesPerSec BytesPerReq
             if presicion_metric:
                 # Get Metric Value
                 metric_value = "%f" % float(value)
 
                 # Publish Metric
@@ -148,22 +214,21 @@
             else:
                 # Get Metric Value
                 metric_value = "%d" % float(value)
 
                 # Publish Metric
                 self.publish(metric_name, metric_value)
 
-    def _parseScoreboard(self, sb):
-
-        ret = []
-
-        ret.append(('IdleWorkers', sb.count('_')))
-        ret.append(('ReadingWorkers', sb.count('R')))
-        ret.append(('WritingWorkers', sb.count('W')))
-        ret.append(('KeepaliveWorkers', sb.count('K')))
-        ret.append(('DnsWorkers', sb.count('D')))
-        ret.append(('ClosingWorkers', sb.count('C')))
-        ret.append(('LoggingWorkers', sb.count('L')))
-        ret.append(('FinishingWorkers', sb.count('G')))
-        ret.append(('CleanupWorkers', sb.count('I')))
+    def _parse_scoreboard(self, sb):
+        ret = [
+            ("IdleWorkers", sb.count("_")),
+            ("ReadingWorkers", sb.count("R")),
+            ("WritingWorkers", sb.count("W")),
+            ("KeepaliveWorkers", sb.count("K")),
+            ("DnsWorkers", sb.count("D")),
+            ("ClosingWorkers", sb.count("C")),
+            ("LoggingWorkers", sb.count("L")),
+            ("FinishingWorkers", sb.count("G")),
+            ("CleanupWorkers", sb.count("I")),
+        ]
 
         return ret
```

### Comparing `diamond-next-4.0.515/src/collectors/flume/flume.py` & `diamond-next-5.0.0/src/collectors/flume/flume.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,114 +1,121 @@
 # coding=utf-8
 
 """
 Collect statistics from Flume
 
 #### Dependencies
 
- * urllib2
+ * urllib
  * json or simplejson
 
 """
 
-import urllib2
+import urllib.error
+import urllib.request
+
 import diamond.collector
 
 try:
     import simplejson as json
 except ImportError:
     import json
 
 
 class FlumeCollector(diamond.collector.Collector):
-
     # items to collect
     _metrics_collect = {
-        'CHANNEL': [
-            'ChannelFillPercentage',
-            'EventPutAttemptCount',
-            'EventPutSuccessCount',
-            'EventTakeAttemptCount',
-            'EventTakeSuccessCount'
+        "CHANNEL": [
+            "ChannelFillPercentage",
+            "EventPutAttemptCount",
+            "EventPutSuccessCount",
+            "EventTakeAttemptCount",
+            "EventTakeSuccessCount",
+        ],
+        "SINK": [
+            "BatchCompleteCount",
+            "BatchEmptyCount",
+            "BatchUnderflowCount",
+            "ConnectionClosedCount",
+            "ConnectionCreatedCount",
+            "ConnectionFailedCount",
+            "EventDrainAttemptCount",
+            "EventDrainSuccessCount",
         ],
-        'SINK': [
-            'BatchCompleteCount',
-            'BatchEmptyCount',
-            'BatchUnderflowCount',
-            'ConnectionClosedCount',
-            'ConnectionCreatedCount',
-            'ConnectionFailedCount',
-            'EventDrainAttemptCount',
-            'EventDrainSuccessCount'
+        "SOURCE": [
+            "AppendAcceptedCount",
+            "AppendBatchAcceptedCount",
+            "AppendBatchReceivedCount",
+            "AppendReceivedCount",
+            "EventAcceptedCount",
+            "EventReceivedCount",
+            "OpenConnectionCount",
         ],
-        'SOURCE': [
-            'AppendAcceptedCount',
-            'AppendBatchAcceptedCount',
-            'AppendBatchReceivedCount',
-            'AppendReceivedCount',
-            'EventAcceptedCount',
-            'EventReceivedCount',
-            'OpenConnectionCount'
-        ]
     }
 
     def get_default_config_help(self):
         config_help = super(FlumeCollector, self).get_default_config_help()
-        config_help.update({
-            'req_host': 'Hostname',
-            'req_port': 'Port',
-            'req_path': 'Path',
-        })
+        config_help.update(
+            {
+                "req_host": "Hostname",
+                "req_port": "Port",
+                "req_path": "Path",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         default_config = super(FlumeCollector, self).get_default_config()
-        default_config['path'] = 'flume'
-        default_config['req_host'] = 'localhost'
-        default_config['req_port'] = 41414
-        default_config['req_path'] = '/metrics'
+        default_config["path"] = "flume"
+        default_config["req_host"] = "localhost"
+        default_config["req_port"] = 41414
+        default_config["req_path"] = "/metrics"
+
         return default_config
 
     def collect(self):
-        url = 'http://{0}:{1}{2}'.format(
-            self.config['req_host'],
-            self.config['req_port'],
-            self.config['req_path']
+        url = "http://{}:{}{}".format(
+            self.config["req_host"], self.config["req_port"], self.config["req_path"]
         )
 
         try:
-            resp = urllib2.urlopen(url)
+            resp = urllib.request.urlopen(url)
+
             try:
                 j = json.loads(resp.read())
                 resp.close()
-            except Exception, e:
+            except Exception as e:
                 resp.close()
-                self.log.error('Cannot load json data: %s', e)
+                self.log.error("Cannot load json data: %s", e)
+
                 return None
-        except urllib2.URLError, e:
-            self.log.error('Failed to open url: %s', e)
+        except urllib.error.URLError as e:
+            self.log.error("Failed to open url: %s", e)
+
             return None
-        except Exception, e:
-            self.log.error('Unknown error opening url: %s', e)
+        except Exception as e:
+            self.log.error("Unknown error opening url: %s", e)
+
             return None
 
-        for comp in j.iteritems():
+        for comp in iter(j.items()):
             comp_name = comp[0]
             comp_items = comp[1]
-            comp_type = comp_items['Type']
+            comp_type = comp_items["Type"]
 
             for item in self._metrics_collect[comp_type]:
-                if item.endswith('Count'):
-                    metric_name = '{0}.{1}'.format(comp_name, item[:-5])
+                if item.endswith("Count"):
+                    metric_name = "{}.{}".format(comp_name, item[:-5])
                     metric_value = int(comp_items[item])
                     self.publish_counter(metric_name, metric_value)
-                elif item.endswith('Percentage'):
-                    metric_name = '{0}.{1}'.format(comp_name, item)
+                elif item.endswith("Percentage"):
+                    metric_name = "{}.{}".format(comp_name, item)
                     metric_value = float(comp_items[item])
                     self.publish_gauge(metric_name, metric_value)
                 else:
                     metric_name = item
                     metric_value = int(comp_items[item])
                     self.publish_gauge(metric_name, metric_value)
```

### Comparing `diamond-next-4.0.515/src/collectors/xen_collector/xen_collector.py` & `diamond-next-5.0.0/src/collectors/xen_collector/xen_collector.py`

 * *Files 17% similar despite different names*

```diff
@@ -4,79 +4,86 @@
 The XENCollector grabs usage/allocation metrics using libvirt
 
 #### Dependencies
  * python-libvirt
 
 """
 
-from diamond.collector import Collector
-
 import os
+
+import diamond.collector
+
 try:
     import libvirt
 except ImportError:
     libvirt = None
 
 
-class XENCollector(Collector):
-
+class XENCollector(diamond.collector.Collector):
     def get_default_config_help(self):
         config_help = super(XENCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(XENCollector, self).get_default_config()
-        config.update({
-            'path':     'xen'
-        })
+        config.update({"path": "xen"})
         return config
 
     def collect(self):
         """
         Collect libvirt data
         """
         if libvirt is None:
-            self.log.error('Unable to import either libvirt')
+            self.log.error("Unable to import either libvirt")
             return {}
+
         # Open a restricted (non-root) connection to the hypervisor
         conn = libvirt.openReadOnly(None)
+
         # Get hardware info
         conninfo = conn.getInfo()
+
         # Initialize variables
         memallocated = 0
         coresallocated = 0
         totalcores = 0
-        results = {}
-        domIds = conn.listDomainsID()
-        if 0 in domIds:
+        dom_ids = conn.listDomainsID()
+
+        if 0 in dom_ids:
             # Total cores
-            domU = conn.lookupByID(0)
-            totalcores = domU.info()[3]
+            dom_u = conn.lookupByID(0)
+            totalcores = dom_u.info()[3]
+
         # Free Space
-        s = os.statvfs('/')
-        freeSpace = (s.f_bavail * s.f_frsize) / 1024
+        s = os.statvfs("/")
+        free_space = (s.f_bavail * s.f_frsize) / 1024
+
         # Calculate allocated memory and cores
-        for i in domIds:
+        for i in dom_ids:
             # Ignore 0
             if i == 0:
                 continue
-            domU = conn.lookupByID(i)
-            dominfo = domU.info()
+
+            dom_u = conn.lookupByID(i)
+            dominfo = dom_u.info()
             memallocated += dominfo[2]
+
             if i > 0:
                 coresallocated += dominfo[3]
+
         results = {
-            'InstalledMem': conninfo[1],
-            'MemAllocated': memallocated / 1024,
-            'MemFree': conninfo[1] - (memallocated / 1024),
-            'AllocatedCores': coresallocated,
-            'DiskFree': freeSpace,
-            'TotalCores': totalcores,
-            'FreeCores': (totalcores - coresallocated)
+            "InstalledMem": conninfo[1],
+            "MemAllocated": memallocated / 1024,
+            "MemFree": conninfo[1] - (memallocated / 1024),
+            "AllocatedCores": coresallocated,
+            "DiskFree": free_space,
+            "TotalCores": totalcores,
+            "FreeCores": (totalcores - coresallocated),
         }
+
         for k in results.keys():
             self.publish(k, results[k], 0)
```

### Comparing `diamond-next-4.0.515/src/collectors/netapp/netapp_inode.py` & `diamond-next-5.0.0/src/collectors/netapp/netapp_inode.py`

 * *Files 18% similar despite different names*

```diff
@@ -12,116 +12,109 @@
             ip = 192.168.1.45
             user = netapp_monitor
             password = b0bl0rd!
 
 """
 
 import diamond.collector
-from diamond.metric import Metric
+import diamond.metric
 
 try:
     import xml.etree.ElementTree as ET
 except ImportError:
     import cElementTree as ET
 
 try:
     from netappsdk.NaServer import *
     from netappsdk.NaElement import *
 except ImportError:
     netappsdk = None
 
-
-__author__ = 'peter@phyn3t.com'
+__author__ = "peter@phyn3t.com"
 
 
 class netapp_inodeCol(object):
-    """ Our netapp_inode Collector
-    """
+    """Our netapp_inode Collector"""
 
     def __init__(self, device, ip, user, password, prefix, pm):
-        """Instantiate _our_ stuff
-        """
+        """Instantiate _our_ stuff"""
 
         self.device = device
         self.ip = ip
         self.netapp_user = user
         self.netapp_password = password
         self.path_prefix = prefix
         self.publish_metric = pm
         self._netapp_login()
 
         filers_xml = self.get_netapp_data()
 
         for volume in filers_xml:
-            max_inodes = volume.find('files-total').text
-            used_inodes = volume.find('files-used').text
-            volume = volume.find('name').text
-            self.push('max_inodes', max_inodes, volume)
-            self.push('used_inodes', used_inodes, volume)
+            max_inodes = volume.find("files-total").text
+            used_inodes = volume.find("files-used").text
+            volume = volume.find("name").text
+            self.push("max_inodes", max_inodes, volume)
+            self.push("used_inodes", used_inodes, volume)
 
     def push(self, metric_name=None, metric_value=None, volume=None):
-        """ Ship that shit off to graphite broski
-        """
+        """Ship that shit off to graphite broski"""
 
         graphite_path = self.path_prefix
-        graphite_path += '.' + self.device + '.' + 'volume'
-        graphite_path += '.' + volume + '.' + metric_name
+        graphite_path += "." + self.device + "." + "volume"
+        graphite_path += "." + volume + "." + metric_name
 
-        metric = Metric(graphite_path, metric_value, precision=4,
-                        host=self.device)
+        metric = diamond.metric.Metric(
+            graphite_path, metric_value, precision=4, host=self.device
+        )
 
         self.publish_metric(metric)
 
     def get_netapp_data(self):
-        """ Retrieve netapp volume information
+        """Retrieve netapp volume information
 
-            returns ElementTree of netapp volume information
+        returns ElementTree of netapp volume information
 
         """
 
-        netapp_data = self.server.invoke('volume-list-info')
+        netapp_data = self.server.invoke("volume-list-info")
 
-        if netapp_data.results_status() == 'failed':
+        if netapp_data.results_status() == "failed":
             self.log.error(
-                'While using netapp API failed to retrieve '
-                'volume-list-info for netapp filer %s' % self.device)
+                "While using netapp API failed to retrieve volume-list-info for netapp filer %s"
+                % self.device
+            )
             return
 
-        netapp_xml = ET.fromstring(netapp_data.sprintf()).find('volumes')
+        netapp_xml = ET.fromstring(netapp_data.sprintf()).find("volumes")
 
         return netapp_xml
 
     def _netapp_login(self):
-        """ Login to our netapp filer
-        """
+        """Login to our netapp filer"""
 
         self.server = NaServer(self.ip, 1, 3)
-        self.server.set_transport_type('HTTPS')
-        self.server.set_style('LOGIN')
+        self.server.set_transport_type("HTTPS")
+        self.server.set_style("LOGIN")
         self.server.set_admin_user(self.netapp_user, self.netapp_password)
 
 
 class netapp_inode(diamond.collector.Collector):
-    """ Netapp inode diamond collector
-    """
+    """Netapp inode diamond collector"""
 
     running = set()
 
     def collect(self, device, ip, user, password):
-        """ Collects metrics for our netapp filer --START HERE--
-
-        """
+        """Collects metrics for our netapp filer --START HERE--"""
 
         if netappsdk is None:
-            self.log.error(
-                'Failed to import netappsdk.NaServer or netappsdk.NaElement')
+            self.log.error("Failed to import netappsdk.NaServer or netappsdk.NaElement")
             return
 
         if device in self.running:
             return
 
         self.running.add(device)
-        prefix = self.config['path_prefix']
+        prefix = self.config["path_prefix"]
         pm = self.publish_metric
 
         netapp_inodeCol(device, ip, user, password, prefix, pm)
         self.running.remove(device)
```

### Comparing `diamond-next-4.0.515/src/collectors/netapp/netappDisk.py` & `diamond-next-5.0.0/src/collectors/netapp/netappDisk.py`

 * *Files 18% similar despite different names*

```diff
@@ -14,318 +14,315 @@
             [[filer-corp-201]] <--- You can have as many filers as you want!
             ip = 192.168.1.45
             user = netapp_monitor
             password = b0bl0rd!
 
 """
 
-import diamond.collector
+from __future__ import print_function
+
 import time
-from diamond.metric import Metric
+
+import diamond.collector
+import diamond.metric
 
 try:
     import xml.etree.ElementTree as ET
 except ImportError:
     import cElementTree as ET
 
 try:
     from netappsdk.NaServer import *
     from netappsdk.NaElement import *
+
     netappsdk = 0
 except ImportError:
     netappsdk = 1
 
-__author__ = 'peter@phyn3t.com'
+__author__ = "peter@phyn3t.com"
 
 
 class netappDiskCol(object):
-    """ Our netappDisk Collector
-    """
+    """Our netappDisk Collector"""
 
     def __init__(self, device, ip, user, password, parent):
-        """ Collectors our metrics for our netapp filer
-        """
+        """Collectors our metrics for our netapp filer"""
 
         self.device = device
         self.ip = ip
         self.netapp_user = user
         self.netapp_password = password
         self.path_prefix = parent[0]
         self.publish_metric = parent[1]
         self.log = parent[2]
         self._netapp_login()
 
         # Grab our netapp XML
-        disk_xml = self.get_netapp_elem(
-            NaElement('disk-list-info'), 'disk-details')
+        disk_xml = self.get_netapp_elem(NaElement("disk-list-info"), "disk-details")
         storage_disk_xml = self.get_netapp_elem(
-            NaElement('storage-disk-get-iter'), 'attributes-list')
+            NaElement("storage-disk-get-iter"), "attributes-list"
+        )
 
         # Our metric collection and publishing goes here
         self.zero_disk(disk_xml)
         self.spare_disk(disk_xml)
         self.maintenance_center(storage_disk_xml)
         self.consistency_point()
         self.agr_busy()
 
     def agr_busy(self):
-        """ Collector for average disk busyness per aggregate
+        """Collector for average disk busyness per aggregate
 
-            As of Nov 22nd 2013 there is no API call for agr busyness.
-            You have to collect all disk busyness and then compute agr
-            busyness. #fml
+        As of Nov 22nd 2013 there is no API call for agr busyness.
+        You have to collect all disk busyness and then compute agr
+        busyness. #fml
 
         """
 
         c1 = {}  # Counters from time a
         c2 = {}  # Counters from time b
         disk_results = {}  # Disk busyness results %
         agr_results = {}  # Aggregate busyness results $
-        names = ['disk_busy', 'base_for_disk_busy', 'raid_name',
-                 'base_for_disk_busy', 'instance_uuid']
-        netapp_api = NaElement('perf-object-get-instances')
-        netapp_api.child_add_string('objectname', 'disk')
-        disk_1 = self.get_netapp_elem(netapp_api, 'instances')
+        names = [
+            "disk_busy",
+            "base_for_disk_busy",
+            "raid_name",
+            "base_for_disk_busy",
+            "instance_uuid",
+        ]
+        netapp_api = NaElement("perf-object-get-instances")
+        netapp_api.child_add_string("objectname", "disk")
+        disk_1 = self.get_netapp_elem(netapp_api, "instances")
         time.sleep(1)
-        disk_2 = self.get_netapp_elem(netapp_api, 'instances')
+        disk_2 = self.get_netapp_elem(netapp_api, "instances")
 
         for instance_data in disk_1:
             temp = {}
             for element in instance_data.findall(".//counters/counter-data"):
-                if element.find('name').text in names:
-                    temp[element.find('name').text] = element.find(
-                        'value').text
-
-            agr_name = temp['raid_name']
-            agr_name = agr_name[agr_name.find('/', 0):agr_name.find('/', 1)]
-            temp['raid_name'] = agr_name.lstrip('/')
-            c1[temp.pop('instance_uuid')] = temp
+                if element.find("name").text in names:
+                    temp[element.find("name").text] = element.find("value").text
+
+            agr_name = temp["raid_name"]
+            agr_name = agr_name[agr_name.find("/", 0) : agr_name.find("/", 1)]
+            temp["raid_name"] = agr_name.lstrip("/")
+            c1[temp.pop("instance_uuid")] = temp
 
         for instance_data in disk_2:
             temp = {}
             for element in instance_data.findall(".//counters/counter-data"):
-                if element.find('name').text in names:
-                    temp[element.find('name').text] = element.find(
-                        'value').text
-
-            agr_name = temp['raid_name']
-            agr_name = agr_name[agr_name.find('/', 0):agr_name.find('/', 1)]
-            temp['raid_name'] = agr_name.lstrip('/')
-            c2[temp.pop('instance_uuid')] = temp
+                if element.find("name").text in names:
+                    temp[element.find("name").text] = element.find("value").text
+
+            agr_name = temp["raid_name"]
+            agr_name = agr_name[agr_name.find("/", 0) : agr_name.find("/", 1)]
+            temp["raid_name"] = agr_name.lstrip("/")
+            c2[temp.pop("instance_uuid")] = temp
 
         for item in c1:
-            t_c1 = int(c1[item]['disk_busy'])  # time_counter_1
-            t_b1 = int(c1[item]['base_for_disk_busy'])  # time_base_1
-            t_c2 = int(c2[item]['disk_busy'])
-            t_b2 = int(c2[item]['base_for_disk_busy'])
+            t_c1 = int(c1[item]["disk_busy"])  # time_counter_1
+            t_b1 = int(c1[item]["base_for_disk_busy"])  # time_base_1
+            t_c2 = int(c2[item]["disk_busy"])
+            t_b2 = int(c2[item]["base_for_disk_busy"])
 
             disk_busy = 100 * (t_c2 - t_c1) / (t_b2 - t_b1)
 
-            if c1[item]['raid_name'] in disk_results:
-                disk_results[c1[item]['raid_name']].append(disk_busy)
+            if c1[item]["raid_name"] in disk_results:
+                disk_results[c1[item]["raid_name"]].append(disk_busy)
             else:
-                disk_results[c1[item]['raid_name']] = [disk_busy]
+                disk_results[c1[item]["raid_name"]] = [disk_busy]
 
         for aggregate in disk_results:
-            agr_results[aggregate] = \
-                sum(disk_results[aggregate]) / len(disk_results[aggregate])
+            agr_results[aggregate] = sum(disk_results[aggregate]) / len(
+                disk_results[aggregate]
+            )
 
         for aggregate in agr_results:
-            self.push('avg_busy', 'aggregate.' + aggregate,
-                      agr_results[aggregate])
+            self.push("avg_busy", "aggregate." + aggregate, agr_results[aggregate])
 
     def consistency_point(self):
-        """ Collector for getting count of consistancy points
-        """
+        """Collector for getting count of consistancy points"""
 
         cp_delta = {}
-        xml_path = 'instances/instance-data/counters'
-        netapp_api = NaElement('perf-object-get-instances')
-        netapp_api.child_add_string('objectname', 'wafl')
-        instance = NaElement('instances')
-        instance.child_add_string('instance', 'wafl')
-        counter = NaElement('counters')
-        counter.child_add_string('counter', 'cp_count')
+        xml_path = "instances/instance-data/counters"
+        netapp_api = NaElement("perf-object-get-instances")
+        netapp_api.child_add_string("objectname", "wafl")
+        instance = NaElement("instances")
+        instance.child_add_string("instance", "wafl")
+        counter = NaElement("counters")
+        counter.child_add_string("counter", "cp_count")
         netapp_api.child_add(counter)
         netapp_api.child_add(instance)
 
         cp_1 = self.get_netapp_elem(netapp_api, xml_path)
         time.sleep(3)
         cp_2 = self.get_netapp_elem(netapp_api, xml_path)
 
         for element in cp_1:
-            if element.find('name').text == 'cp_count':
-                cp_1 = element.find('value').text.rsplit(',')
+            if element.find("name").text == "cp_count":
+                cp_1 = element.find("value").text.rsplit(",")
                 break
         for element in cp_2:
-            if element.find('name').text == 'cp_count':
-                cp_2 = element.find('value').text.rsplit(',')
+            if element.find("name").text == "cp_count":
+                cp_2 = element.find("value").text.rsplit(",")
                 break
 
         if not type(cp_2) is list or not type(cp_1) is list:
-            log.error("consistency point data not available for filer: %s"
-                      % self.device)
+            self.log.error(
+                "consistency point data not available for filer: %s" % self.device
+            )
             return
 
         cp_1 = {
-            'wafl_timer': cp_1[0],
-            'snapshot': cp_1[1],
-            'wafl_avail_bufs': cp_1[2],
-            'dirty_blk_cnt': cp_1[3],
-            'full_nv_log': cp_1[4],
-            'b2b': cp_1[5],
-            'flush_gen': cp_1[6],
-            'sync_gen': cp_1[7],
-            'def_b2b': cp_1[8],
-            'con_ind_pin': cp_1[9],
-            'low_mbuf_gen': cp_1[10],
-            'low_datavec_gen': cp_1[11]
+            "wafl_timer": cp_1[0],
+            "snapshot": cp_1[1],
+            "wafl_avail_bufs": cp_1[2],
+            "dirty_blk_cnt": cp_1[3],
+            "full_nv_log": cp_1[4],
+            "b2b": cp_1[5],
+            "flush_gen": cp_1[6],
+            "sync_gen": cp_1[7],
+            "def_b2b": cp_1[8],
+            "con_ind_pin": cp_1[9],
+            "low_mbuf_gen": cp_1[10],
+            "low_datavec_gen": cp_1[11],
         }
 
         cp_2 = {
-            'wafl_timer': cp_2[0],
-            'snapshot': cp_2[1],
-            'wafl_avail_bufs': cp_2[2],
-            'dirty_blk_cnt': cp_2[3],
-            'full_nv_log': cp_2[4],
-            'b2b': cp_2[5],
-            'flush_gen': cp_2[6],
-            'sync_gen': cp_2[7],
-            'def_b2b': cp_2[8],
-            'con_ind_pin': cp_2[9],
-            'low_mbuf_gen': cp_2[10],
-            'low_datavec_gen': cp_2[11]
+            "wafl_timer": cp_2[0],
+            "snapshot": cp_2[1],
+            "wafl_avail_bufs": cp_2[2],
+            "dirty_blk_cnt": cp_2[3],
+            "full_nv_log": cp_2[4],
+            "b2b": cp_2[5],
+            "flush_gen": cp_2[6],
+            "sync_gen": cp_2[7],
+            "def_b2b": cp_2[8],
+            "con_ind_pin": cp_2[9],
+            "low_mbuf_gen": cp_2[10],
+            "low_datavec_gen": cp_2[11],
         }
 
         for item in cp_1:
             c1 = int(cp_1[item])
             c2 = int(cp_2[item])
             cp_delta[item] = c2 - c1
 
         for item in cp_delta:
-            self.push(item + '_CP', 'system.system', cp_delta[item])
+            self.push(item + "_CP", "system.system", cp_delta[item])
 
     def maintenance_center(self, storage_disk_xml=None):
-        """ Collector for how many disk(s) are in NetApp maintenance center
+        """Collector for how many disk(s) are in NetApp maintenance center
 
-            For more information on maintenance center please see:
-              bit.ly/19G4ptr
+        For more information on maintenance center please see:
+          bit.ly/19G4ptr
 
         """
 
         disk_in_maintenance = 0
 
         for filer_disk in storage_disk_xml:
-            disk_status = filer_disk.find('disk-raid-info/container-type')
-            if disk_status.text == 'maintenance':
+            disk_status = filer_disk.find("disk-raid-info/container-type")
+            if disk_status.text == "maintenance":
                 disk_in_maintenance += 1
 
-        self.push('maintenance_disk', 'disk', disk_in_maintenance)
+        self.push("maintenance_disk", "disk", disk_in_maintenance)
 
     def zero_disk(self, disk_xml=None):
-        """ Collector and publish not zeroed disk metrics
-        """
+        """Collector and publish not zeroed disk metrics"""
 
         troubled_disks = 0
         for filer_disk in disk_xml:
-            raid_state = filer_disk.find('raid-state').text
-            if not raid_state == 'spare':
+            raid_state = filer_disk.find("raid-state").text
+            if not raid_state == "spare":
                 continue
-            is_zeroed = filer_disk.find('is-zeroed').text
+            is_zeroed = filer_disk.find("is-zeroed").text
 
-            if is_zeroed == 'false':
+            if is_zeroed == "false":
                 troubled_disks += 1
-        self.push('not_zeroed', 'disk', troubled_disks)
+        self.push("not_zeroed", "disk", troubled_disks)
 
     def spare_disk(self, disk_xml=None):
-        """ Number of spare disk per type.
+        """Number of spare disk per type.
 
-            For example: storage.ontap.filer201.disk.SATA
+        For example: storage.ontap.filer201.disk.SATA
 
         """
 
         spare_disk = {}
         disk_types = set()
 
         for filer_disk in disk_xml:
-            disk_types.add(filer_disk.find('effective-disk-type').text)
-            if not filer_disk.find('raid-state').text == 'spare':
+            disk_types.add(filer_disk.find("effective-disk-type").text)
+            if not filer_disk.find("raid-state").text == "spare":
                 continue
 
-            disk_type = filer_disk.find('effective-disk-type').text
+            disk_type = filer_disk.find("effective-disk-type").text
             if disk_type in spare_disk:
                 spare_disk[disk_type] += 1
             else:
                 spare_disk[disk_type] = 1
 
         for disk_type in disk_types:
             if disk_type in spare_disk:
-                self.push('spare_' + disk_type, 'disk', spare_disk[disk_type])
+                self.push("spare_" + disk_type, "disk", spare_disk[disk_type])
             else:
-                self.push('spare_' + disk_type, 'disk', 0)
+                self.push("spare_" + disk_type, "disk", 0)
 
     def get_netapp_elem(self, netapp_api=None, sub_element=None):
-        """ Retrieve netapp elem
-        """
+        """Retrieve netapp elem"""
 
         netapp_data = self.server.invoke_elem(netapp_api)
 
-        if netapp_data.results_status() == 'failed':
+        if netapp_data.results_status() == "failed":
             self.log.error(
-                'While using netapp API failed to retrieve '
-                'disk-list-info for netapp filer %s' % self.device)
-            print netapp_data.sprintf()
+                "While using netapp API failed to retrieve disk-list-info for netapp filer %s"
+                % self.device
+            )
+            print(netapp_data.sprintf())
             return
-        netapp_xml = \
-            ET.fromstring(netapp_data.sprintf()).find(sub_element)
+
+        netapp_xml = ET.fromstring(netapp_data.sprintf()).find(sub_element)
 
         return netapp_xml
 
     def _netapp_login(self):
-        """ Login to our netapp filer
-        """
+        """Login to our netapp filer"""
 
         self.server = NaServer(self.ip, 1, 3)
-        self.server.set_transport_type('HTTPS')
-        self.server.set_style('LOGIN')
+        self.server.set_transport_type("HTTPS")
+        self.server.set_style("LOGIN")
         self.server.set_admin_user(self.netapp_user, self.netapp_password)
 
     def push(self, metric_name=None, type=None, metric_value=None):
-        """ Ship that shit off to graphite broski
-        """
+        """Ship that shit off to graphite broski"""
 
         graphite_path = self.path_prefix
-        graphite_path += '.' + self.device + '.' + type
-        graphite_path += '.' + metric_name
+        graphite_path += "." + self.device + "." + type
+        graphite_path += "." + metric_name
 
-        metric = Metric(
-            graphite_path,
-            metric_value,
-            precision=4,
-            host=self.device)
+        metric = diamond.metric.Metric(
+            graphite_path, metric_value, precision=4, host=self.device
+        )
 
         self.publish_metric(metric)
 
 
 class netappDisk(diamond.collector.Collector):
-    """ Netapp disk diamond scheduler
-    """
+    """Netapp disk diamond scheduler"""
 
     running = set()
 
     def collect(self, device, ip, user, password):
-        """ Collectors our metrics for our netapp filer --START HERE--
-        """
+        """Collectors our metrics for our netapp filer --START HERE--"""
 
         if netappsdk:
-            self.log.error(
-                'Failed to import netappsdk.NaServer or netappsdk.NaElement')
+            self.log.error("Failed to import netappsdk.NaServer or netappsdk.NaElement")
             return
 
         if device in self.running:
             return
 
         self.running.add(device)
-        parent = (self.config['path_prefix'], self.publish_metric, self.log)
+        parent = (self.config["path_prefix"], self.publish_metric, self.log)
 
         netappDiskCol(device, ip, user, password, parent)
         self.running.remove(device)
```

### Comparing `diamond-next-4.0.515/src/collectors/netapp/netapp.py` & `diamond-next-5.0.0/src/collectors/netapp/netapp.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,73 +21,76 @@
     [devices]
 
     [[na_filer]]
     ip = 123.123.123.123
     user = root
     password = strongpassword
 
-````
+```
 
 The primary source for documentation about the API has been
 "NetApp unified storage performance management using open interfaces"
 https://communities.netapp.com/docs/DOC-1044
 
 """
 
+from __future__ import print_function
+
+import re
+
 import sys
 import time
-import re
 import unicodedata
 
-from diamond.metric import Metric
+import diamond.collector
 import diamond.convertor
+import diamond.metric
 
 
 class NetAppCollector(diamond.collector.Collector):
-
     # This is the list of metrics to collect.
     # This is a dict of lists with tuples, which is parsed as such:
     # The dict name is the object name in the NetApp API.
     # For each object we have a list of metrics to retrieve.
     # Each tuple is built like this;
     # ("metric name in netapp api", "output name of metric", multiplier)
     # The purpose of the output name is to enable replacement of reported
     # metric names, since some the names in the API can be confusing.
     # The purpose of the multiplier is to scale all metrics to a common
     # scale, which is latencies in milliseconds, and data in bytes per sec.
     # This is needed since the API will return a mixture of percentages,
     # nanoseconds, milliseconds, bytes and kilobytes.
     METRICS = {
-        'aggregate': [
+        "aggregate": [
             ("user_reads", "user_read_iops", 1),
-            ("user_writes", "user_write_iops", 1)
+            ("user_writes", "user_write_iops", 1),
         ],
-        'disk': [
+        "disk": [
             ("disk_busy", "disk_busy_pct", 100),
             ("base_for_disk_busy", "base_for_disk_busy", 1),
             ("user_read_blocks", "user_read_blocks_per_sec", 1),
             ("user_write_blocks", "user_write_blocks_per_sec", 1),
             ("user_read_latency", "user_read_latency", 0.001),
-            ("user_write_latency", "user_write_latency", 0.001)
+            ("user_write_latency", "user_write_latency", 0.001),
         ],
-        'ifnet': [
+        "ifnet": [
             ("send_data", "tx_bytes_per_sec", 1),
-            ("recv_data", "rx_bytes_per_sec", 1)
+            ("recv_data", "rx_bytes_per_sec", 1),
         ],
-        'lun': [
+        "lun": [
             ("total_ops", "total_iops", 1),
             ("read_ops", "read_iops", 1),
             ("write_ops", "write_iops", 1),
-            ("avg_latency", "avg_latency", 1)
+            ("avg_latency", "avg_latency", 1),
         ],
-        'processor': [
+        "processor": [
             ("processor_busy", "processor_busy_pct", 100),
-            ("processor_elapsed_time", "processor_elapsed_time", 1)
+            ("processor_elapsed_time", "processor_elapsed_time", 1),
         ],
-        'system': [
+        "system": [
             ("nfs_ops", "nfs_iops", 1),
             ("cifs_ops", "cifs_iops", 1),
             ("http_ops", "http_iops", 1),
             ("fcp_ops", "fcp_iops", 1),
             ("http_ops", "http_iops", 1),
             ("iscsi_ops", "iscsi_iops", 1),
             ("read_ops", "read_iops", 1),
@@ -98,27 +101,27 @@
             ("avg_processor_busy", "avg_processor_busy_pct", 100),
             ("net_data_recv", "total_rx_bytes_per_sec", 1000),
             ("net_data_sent", "total_tx_bytes_per_sec", 1000),
             ("disk_data_read", "total_read_bytes_per_sec", 1000),
             ("disk_data_written", "total_write_bytes_per_sec", 1000),
             ("sys_read_latency", "sys_read_latency", 1),
             ("sys_write_latency", "sys_write_latency", 1),
-            ("sys_avg_latency", "sys_avg_latency", 1)
+            ("sys_avg_latency", "sys_avg_latency", 1),
         ],
-        'vfiler': [
+        "vfiler": [
             ("vfiler_cpu_busy", "cpu_busy_pct", 100),
             ("vfiler_cpu_busy_base", "cpu_busy_base", 1),
             ("vfiler_net_data_recv", "rx_bytes_per_sec", 1000),
             ("vfiler_net_data_sent", "tx_bytes_per_sec", 1000),
             ("vfiler_read_ops", "read_iops", 1),
             ("vfiler_write_ops", "write_iops", 1),
             ("vfiler_read_bytes", "read_bytes_per_sec", 1000),
             ("vfiler_write_bytes", "write_bytes_per_sec", 1000),
         ],
-        'volume': [
+        "volume": [
             ("total_ops", "total_iops", 1),
             ("avg_latency", "avg_latency", 0.001),
             ("read_ops", "read_iops", 1),
             ("write_ops", "write_iops", 1),
             ("read_latency", "read_latency", 0.001),
             ("write_latency", "write_latency", 0.001),
             ("read_data", "read_bytes_per_sec", 1),
@@ -142,15 +145,15 @@
             ("iscsi_read_ops", "iscsi_read_iops", 1),
             ("iscsi_write_ops", "iscsi_write_iops", 1),
             ("nfs_read_data", "nfs_read_bytes_per_sec", 1),
             ("nfs_write_data", "nfs_write_bytes_per_sec", 1),
             ("nfs_read_latency", "nfs_read_latency", 0.001),
             ("nfs_write_latency", "nfs_write_latency", 0.001),
             ("nfs_read_ops", "nfs_read_iops", 1),
-            ("nfs_write_ops", "nfs_write_iops", 1)
+            ("nfs_write_ops", "nfs_write_iops", 1),
         ],
     }
 
     # For some metrics we need to divide one value from the API with another.
     # This is a key-value list of the connected values.
     DIVIDERS = {
         "avg_latency": "total_ops",
@@ -192,195 +195,219 @@
 
     def get_default_config_help(self):
         config_help = super(NetAppCollector, self).get_default_config_help()
         return config_help
 
     def get_default_config(self):
         default_config = super(NetAppCollector, self).get_default_config()
-        default_config['path_prefix'] = "netapp"
-        default_config['netappsdkpath'] = "/opt/netapp/lib/python/NetApp"
+        default_config["path_prefix"] = "netapp"
+        default_config["netappsdkpath"] = "/opt/netapp/lib/python/NetApp"
         return default_config
 
     def _replace_and_publish(self, path, prettyname, value, device):
         """
         Inputs a complete path for a metric and a value.
         Replace the metric name and publish.
         """
         if value is None:
             return
+
         newpath = path
+
         # Change metric name before publish if needed.
         newpath = ".".join([".".join(path.split(".")[:-1]), prettyname])
-        metric = Metric(newpath, value, precision=4, host=device)
+        metric = diamond.metric.Metric(newpath, value, precision=4, host=device)
         self.publish_metric(metric)
 
-    def _gen_delta_depend(self, path, derivative, multiplier, prettyname,
-                          device):
+    def _gen_delta_depend(self, path, derivative, multiplier, prettyname, device):
         """
         For some metrics we need to divide the delta for one metric
         with the delta of another.
         Publishes a metric if the convertion goes well.
         """
         primary_delta = derivative[path]
         shortpath = ".".join(path.split(".")[:-1])
         basename = path.split(".")[-1]
         secondary_delta = None
+
         if basename in self.DIVIDERS.keys():
-            mateKey = ".".join([shortpath, self.DIVIDERS[basename]])
+            mate_key = ".".join([shortpath, self.DIVIDERS[basename]])
         else:
             return
-        if mateKey in derivative.keys():
-            secondary_delta = derivative[mateKey]
+
+        if mate_key in derivative.keys():
+            secondary_delta = derivative[mate_key]
         else:
             return
 
         # If we find a corresponding secondary_delta, publish a metric
         if primary_delta > 0 and secondary_delta > 0:
             value = (float(primary_delta) / secondary_delta) * multiplier
             self._replace_and_publish(path, prettyname, value, device)
 
-    def _gen_delta_per_sec(self, path, value_delta, time_delta, multiplier,
-                           prettyname, device):
+    def _gen_delta_per_sec(
+        self, path, value_delta, time_delta, multiplier, prettyname, device
+    ):
         """
         Calulates the difference between to point, and scales is to per second.
         """
         if time_delta < 0:
             return
+
         value = (value_delta / time_delta) * multiplier
+
         # Only publish if there is any data.
         # This helps keep unused metrics out of Graphite
         if value > 0.0:
             self._replace_and_publish(path, prettyname, value, device)
 
     def collect(self, device, ip, user, password):
         """
         This function collects the metrics for one filer.
         """
-        sys.path.append(self.config['netappsdkpath'])
+        sys.path.append(self.config["netappsdkpath"])
+
         try:
             import NaServer
         except ImportError:
-            self.log.error("Unable to load NetApp SDK from %s" % (
-                self.config['netappsdkpath']))
+            self.log.error(
+                "Unable to load NetApp SDK from %s" % (self.config["netappsdkpath"])
+            )
             return
 
         # Set up the parameters
         server = NaServer.NaServer(ip, 1, 3)
-        server.set_transport_type('HTTPS')
-        server.set_style('LOGIN')
+        server.set_transport_type("HTTPS")
+        server.set_style("LOGIN")
         server.set_admin_user(user, password)
 
-        # We're only able to query a single object at a time,
-        # so we'll loop over the objects.
+        # We're only able to query a single object at a time, so we'll loop over the objects.
         for na_object in self.METRICS.keys():
 
             # For easy reference later, generate a new dict for this object
-            LOCALMETRICS = {}
+            local_metrics = {}
+
             for metric in self.METRICS[na_object]:
                 metricname, prettyname, multiplier = metric
-                LOCALMETRICS[metricname] = {}
-                LOCALMETRICS[metricname]["prettyname"] = prettyname
-                LOCALMETRICS[metricname]["multiplier"] = multiplier
+                local_metrics[metricname] = {}
+                local_metrics[metricname]["prettyname"] = prettyname
+                local_metrics[metricname]["multiplier"] = multiplier
 
             # Keep track of how long has passed since we checked last
-            CollectTime = time.time()
+            collect_time = time.time()
             time_delta = None
+
             if na_object in self.LastCollectTime.keys():
-                time_delta = CollectTime - self.LastCollectTime[na_object]
-            self.LastCollectTime[na_object] = CollectTime
+                time_delta = collect_time - self.LastCollectTime[na_object]
+
+            self.LastCollectTime[na_object] = collect_time
 
             self.log.debug("Collecting metric of object %s" % na_object)
             query = NaServer.NaElement("perf-object-get-instances-iter-start")
             query.child_add_string("objectname", na_object)
             counters = NaServer.NaElement("counters")
-            for metric in LOCALMETRICS.keys():
+
+            for metric in local_metrics.keys():
                 counters.child_add_string("counter", metric)
+
             query.child_add(counters)
 
             res = server.invoke_elem(query)
-            if(res.results_status() == "failed"):
-                self.log.error("Connection to filer %s failed; %s" % (
-                    device, res.results_reason()))
+
+            if res.results_status() == "failed":
+                self.log.error(
+                    "Connection to filer %s failed; %s" % (device, res.results_reason())
+                )
                 return
 
             iter_tag = res.child_get_string("tag")
             num_records = 1
             max_records = 100
 
             # For some metrics there are dependencies between metrics for
             # a single object, so we'll need to collect all, so we can do
             # calculations later.
             raw = {}
 
-            while(num_records != 0):
-                query = NaServer.NaElement(
-                    "perf-object-get-instances-iter-next")
+            while num_records != 0:
+                query = NaServer.NaElement("perf-object-get-instances-iter-next")
                 query.child_add_string("tag", iter_tag)
                 query.child_add_string("maximum", max_records)
                 res = server.invoke_elem(query)
 
-                if(res.results_status() == "failed"):
-                    print "Connection to filer %s failed; %s" % (
-                        device, res.results_reason())
+                if res.results_status() == "failed":
+                    print(
+                        "Connection to filer %s failed; %s"
+                        % (device, res.results_reason())
+                    )
                     return
 
                 num_records = res.child_get_int("records")
 
-                if(num_records > 0):
+                if num_records > 0:
                     instances_list = res.child_get("instances")
                     instances = instances_list.children_get()
 
                     for instance in instances:
                         raw_name = unicodedata.normalize(
-                            'NFKD',
-                            instance.child_get_string("name")).encode(
-                            'ascii', 'ignore')
-                        # Shorten the name for disks as they are very long and
-                        # padded with zeroes, eg:
-                        # 5000C500:3A236B0B:00000000:00000000:00000000:...
-                        if na_object is "disk":
+                            "NFKD", instance.child_get_string("name")
+                        ).encode("ascii", "ignore")
+
+                        # Shorten the name for disks as they are very long and padded with zeroes, eg: 5000C500:3A236B0B:00000000:00000000:00000000:...
+                        if na_object == "disk":
                             non_zero_blocks = [
-                                block for block in raw_name.split(":")
+                                block
+                                for block in raw_name.split(":")
                                 if block != "00000000"
                             ]
                             raw_name = "".join(non_zero_blocks)
-                        instance_name = re.sub(r'\W', '_', raw_name)
+
+                        instance_name = re.sub(r"\W", "_", raw_name)
                         counters_list = instance.child_get("counters")
                         counters = counters_list.children_get()
 
                         for counter in counters:
                             metricname = unicodedata.normalize(
-                                'NFKD',
-                                counter.child_get_string("name")).encode(
-                                'ascii', 'ignore')
+                                "NFKD", counter.child_get_string("name")
+                            ).encode("ascii", "ignore")
                             metricvalue = counter.child_get_string("value")
-                            # We'll need a long complete pathname to not
-                            # confuse self.derivative
-                            pathname = ".".join([self.config["path_prefix"],
-                                                 device, na_object,
-                                                 instance_name, metricname])
+
+                            # We'll need a long complete pathname to not confuse self.derivative
+                            pathname = ".".join(
+                                [
+                                    self.config["path_prefix"],
+                                    device,
+                                    na_object,
+                                    instance_name,
+                                    metricname,
+                                ]
+                            )
                             raw[pathname] = int(metricvalue)
 
             # Do the math
-            self.log.debug("Processing %i metrics for object %s" % (len(raw),
-                                                                    na_object))
+            self.log.debug(
+                "Processing %i metrics for object %s" % (len(raw), na_object)
+            )
 
             # Since the derivative function both returns the derivative
             # and saves a new point, we'll need to store all derivatives
             # for local reference.
             derivative = {}
+
             for key in raw.keys():
                 derivative[key] = self.derivative(key, raw[key])
 
             for key in raw.keys():
                 metricname = key.split(".")[-1]
-                prettyname = LOCALMETRICS[metricname]["prettyname"]
-                multiplier = LOCALMETRICS[metricname]["multiplier"]
+                prettyname = local_metrics[metricname]["prettyname"]
+                multiplier = local_metrics[metricname]["multiplier"]
 
                 if metricname in self.DROPMETRICS:
                     continue
                 elif metricname in self.DIVIDERS.keys():
-                    self._gen_delta_depend(key, derivative, multiplier,
-                                           prettyname, device)
+                    self._gen_delta_depend(
+                        key, derivative, multiplier, prettyname, device
+                    )
                 else:
-                    self._gen_delta_per_sec(key, derivative[key], time_delta,
-                                            multiplier, prettyname, device)
+                    self._gen_delta_per_sec(
+                        key, derivative[key], time_delta, multiplier, prettyname, device
+                    )
```

### Comparing `diamond-next-4.0.515/src/collectors/servertechpdu/servertechpdu.py` & `diamond-next-5.0.0/src/collectors/servertechpdu/servertechpdu.py`

 * *Files 15% similar despite different names*

```diff
@@ -4,126 +4,133 @@
 SNMPCollector for Server Tech PDUs
 
 Server Tech is a manufacturer of PDUs
 http://www.servertech.com/
 
 """
 
-import time
-import re
 import os
+import re
+
 import sys
+import time
+
+import diamond.metric
 
 # Fix Path for locating the SNMPCollector
-sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
-                                             '../',
-                                             'snmp',
-                                             )))
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../", "snmp")))
 
-from diamond.metric import Metric
 from snmp import SNMPCollector as parent_SNMPCollector
 
 
 class ServerTechPDUCollector(parent_SNMPCollector):
     """
     SNMPCollector for ServerTech PDUs
     """
 
-    PDU_SYSTEM_GAUGES = {
-        "systemTotalWatts": "1.3.6.1.4.1.1718.3.1.6"
-    }
+    PDU_SYSTEM_GAUGES = {"systemTotalWatts": "1.3.6.1.4.1.1718.3.1.6"}
 
     PDU_INFEED_NAMES = "1.3.6.1.4.1.1718.3.2.2.1.3"
 
     PDU_INFEED_GAUGES = {
         "infeedCapacityAmps": "1.3.6.1.4.1.1718.3.2.2.1.10",
         "infeedVolts": "1.3.6.1.4.1.1718.3.2.2.1.11",
         "infeedAmps": "1.3.6.1.4.1.1718.3.2.2.1.7",
-        "infeedWatts": "1.3.6.1.4.1.1718.3.2.2.1.12"
+        "infeedWatts": "1.3.6.1.4.1.1718.3.2.2.1.12",
     }
 
     def get_default_config_help(self):
-        config_help = super(ServerTechPDUCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': 'PDU dns address',
-            'port': 'PDU port to collect snmp data',
-            'community': 'SNMP community'
-        })
+        config_help = super(ServerTechPDUCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "PDU dns address",
+                "port": "PDU port to collect snmp data",
+                "community": "SNMP community",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(ServerTechPDUCollector, self).get_default_config()
-        config.update({
-            'path':     'pdu',
-            'timeout': 15,
-            'retries': 3,
-        })
+        config.update(
+            {
+                "path": "pdu",
+                "timeout": 15,
+                "retries": 3,
+            }
+        )
         return config
 
     def collect_snmp(self, device, host, port, community):
         """
         Collect stats from device
         """
         # Log
         self.log.info("Collecting ServerTech PDU statistics from: %s" % device)
 
         # Set timestamp
         timestamp = time.time()
 
-        inputFeeds = {}
+        input_feeds = {}
 
         # Collect PDU input gauge values
         for gaugeName, gaugeOid in self.PDU_SYSTEM_GAUGES.items():
-            systemGauges = self.walk(gaugeOid, host, port, community)
-            for o, gaugeValue in systemGauges.items():
+            system_gauges = self.walk(gaugeOid, host, port, community)
+
+            for o, gaugeValue in system_gauges.items():
                 # Get Metric Name
-                metricName = gaugeName
+                metric_name = gaugeName
+
                 # Get Metric Value
-                metricValue = float(gaugeValue)
+                metric_value = float(gaugeValue)
+
                 # Get Metric Path
-                metricPath = '.'.join(
-                    ['devices', device, 'system', metricName])
+                metric_path = ".".join(["devices", device, "system", metric_name])
+
                 # Create Metric
-                metric = Metric(metricPath, metricValue, timestamp, 2)
+                metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 2)
+
                 # Publish Metric
                 self.publish_metric(metric)
 
         # Collect PDU input feed names
-        inputFeedNames = self.walk(
-            self.PDU_INFEED_NAMES, host, port, community)
-        for o, inputFeedName in inputFeedNames.items():
+        input_feed_names = self.walk(self.PDU_INFEED_NAMES, host, port, community)
+
+        for o, inputFeedName in input_feed_names.items():
             # Extract input feed name
-            inputFeed = ".".join(o.split(".")[-2:])
-            inputFeeds[inputFeed] = inputFeedName
+            input_feed = ".".join(o.split(".")[-2:])
+            input_feeds[input_feed] = inputFeedName
 
         # Collect PDU input gauge values
         for gaugeName, gaugeOid in self.PDU_INFEED_GAUGES.items():
-            inputFeedGauges = self.walk(gaugeOid, host, port, community)
-            for o, gaugeValue in inputFeedGauges.items():
+            input_feed_gauges = self.walk(gaugeOid, host, port, community)
+
+            for o, gaugeValue in input_feed_gauges.items():
                 # Extract input feed name
-                inputFeed = ".".join(o.split(".")[-2:])
+                input_feed = ".".join(o.split(".")[-2:])
 
                 # Get Metric Name
-                metricName = '.'.join([re.sub(r'\.|\\', '_',
-                                              inputFeeds[inputFeed]),
-                                       gaugeName])
+                metric_name = ".".join(
+                    [re.sub(r"\.|\\", "_", input_feeds[input_feed]), gaugeName]
+                )
 
                 # Get Metric Value
                 if gaugeName == "infeedVolts":
                     # Note: Voltage is in "tenth volts", so divide by 10
-                    metricValue = float(gaugeValue) / 10.0
+                    metric_value = float(gaugeValue) / 10.0
                 elif gaugeName == "infeedAmps":
                     # Note: Amps is in "hundredth amps", so divide by 100
-                    metricValue = float(gaugeValue) / 100.0
+                    metric_value = float(gaugeValue) / 100.0
                 else:
-                    metricValue = float(gaugeValue)
+                    metric_value = float(gaugeValue)
 
                 # Get Metric Path
-                metricPath = '.'.join(['devices', device, 'input', metricName])
+                metric_path = ".".join(["devices", device, "input", metric_name])
+
                 # Create Metric
-                metric = Metric(metricPath, metricValue, timestamp, 2)
+                metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 2)
+
                 # Publish Metric
                 self.publish_metric(metric)
```

### Comparing `diamond-next-4.0.515/src/collectors/sidekiqweb/sidekiqweb.py` & `diamond-next-5.0.0/src/collectors/sidekiqweb/sidekiqweb.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,70 +1,74 @@
 # coding=utf-8
 
 """
 Collects data from sidekiq web
 
 #### Dependencies
 
- * urllib2
+ * urllib
  * json (or simeplejson)
 
 """
 
+import urllib.request
+
+import diamond.collector
+import diamond.convertor
+
 try:
     import json
 except ImportError:
     import simplejson as json
 
-import urllib2
-import diamond.collector
-
 
 class SidekiqWebCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(SidekiqWebCollector,
-                            self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(SidekiqWebCollector, self).get_default_config_help()
+        config_help.update({})
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(SidekiqWebCollector, self).get_default_config()
-        config.update({
-            'host': 'localhost',
-            'port': 9999,
-            'byte_unit': ['byte'],
-        })
+        config.update(
+            {
+                "host": "localhost",
+                "port": 9999,
+                "byte_unit": ["byte"],
+            }
+        )
         return config
 
     def collect(self):
         try:
-            response = urllib2.urlopen("http://%s:%s/dashboard/stats" % (
-                self.config['host'], int(self.config['port'])))
-        except Exception, e:
-            self.log.error('Couldnt connect to sidekiq-web: %s', e)
+            response = urllib.request.urlopen(
+                "http://%s:%s/dashboard/stats"
+                % (self.config["host"], int(self.config["port"]))
+            )
+        except Exception as e:
+            self.log.error("Could not connect to sidekiq-web: %s", e)
+
             return {}
 
         try:
             j = json.loads(response.read())
-        except Exception, e:
-            self.log.error('Couldnt parse json: %s', e)
+        except Exception as e:
+            self.log.error("Could not parse json: %s", e)
+
             return {}
 
         for k in j:
             for item, value in j[k].items():
+                if isinstance(value, (bytes, str)) and "M" in value:
+                    value = float(value.replace("M", ""))
 
-                if isinstance(value, (str, unicode)) and 'M' in value:
-                    value = float(value.replace('M', ''))
-                    for unit in self.config['byte_unit']:
+                    for unit in self.config["byte_unit"]:
                         unit_value = diamond.convertor.binary.convert(
-                            value=value,
-                            oldUnit='megabyte',
-                            newUnit=unit)
+                            value=value, old_unit="megabyte", new_unit=unit
+                        )
 
                         self.publish("%s.%s_%s" % (k, item, unit), unit_value)
                 else:
                     self.publish("%s.%s" % (k, item), value)
```

### Comparing `diamond-next-4.0.515/src/collectors/resqueweb/resqueweb.py` & `diamond-next-5.0.0/src/collectors/chronyd/chronyd.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,63 +1,92 @@
 # coding=utf-8
 
 """
-Collects data for Resque Web
+Collect metrics from chrony - http://chrony.tuxfamily.org/
 
 #### Dependencies
 
- * urllib2
+ * subprocess
 
 """
 
-import urllib2
+import re
+import subprocess
+
 import diamond.collector
+import diamond.convertor
+
+LINE_PATTERN = re.compile("^(?P<source>\S+).*\s+(?P<offset>[+-]\d+)(?P<unit>\w+)\s+")
+IP_PATTERN = re.compile("^\d+\.\d+\.\d+\.\d+$")
+
+
+def cleanup_source(source):
+    if IP_PATTERN.search(source):
+        return source.replace(".", "_")
 
+    if "." in source:
+        hostname, _ = source.split(".", 1)
+        return hostname
 
-class ResqueWebCollector(diamond.collector.Collector):
+    return source
 
+
+class ChronydCollector(diamond.collector.Collector):
     def get_default_config_help(self):
-        config_help = super(ResqueWebCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(ChronydCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "bin": "The path to the chronyc binary",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
-        config = super(ResqueWebCollector, self).get_default_config()
-        config.update({
-            'host': 'localhost',
-            'port': 5678,
-            'path': 'resqueweb',
-        })
+        config = super(ChronydCollector, self).get_default_config()
+        config.update(
+            {
+                "path": "chrony",
+                "bin": "/usr/bin/chronyc",
+                "use_sudo": False,
+                "sudo_cmd": "/usr/bin/sudo",
+            }
+        )
+
         return config
 
-    def collect(self):
+    def get_output(self):
         try:
-            response = urllib2.urlopen("http://%s:%s/stats.txt" % (
-                self.config['host'], int(self.config['port'])))
-        except Exception, e:
-            self.log.error('Couldnt connect to resque-web: %s', e)
-            return {}
+            command = [self.config["bin"], "sourcestats"]
 
-        for data in response.read().split("\n"):
-            if data == "":
+            if diamond.collector.str_to_bool(self.config["use_sudo"]):
+                command.insert(0, self.config["sudo_cmd"])
+
+            return subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0]
+        except OSError:
+            return ""
+
+    def collect(self):
+        output = self.get_output()
+
+        for line in output.strip().split("\n"):
+            m = LINE_PATTERN.search(line)
+
+            if m is None:
                 continue
 
-            item, count = data.strip().split("=")
-            try:
-                count = int(count)
-                (item, queue) = item.split(".")
+            source = cleanup_source(m.group("source"))
+            offset = float(m.group("offset"))
+            unit = m.group("unit")
 
-                if item == "resque":
-                    if queue[-1] == "+":
-                        self.publish("%s.total" %
-                                     queue.replace("+", ""), count)
-                    else:
-                        self.publish("%s.current" % queue, count)
-                else:
-                    self.publish("queue.%s.current" % queue, count)
+            try:
+                value = diamond.convertor.time.convert(offset, unit, "ms")
+            except NotImplementedError as e:
+                self.log.error("Unable to convert %s%s: %s", offset, unit, e)
+                continue
 
-            except Exception, e:
-                self.log.error('Couldnt parse the queue: %s', e)
+            self.publish("%s.offset_ms" % source, value)
```

### Comparing `diamond-next-4.0.515/src/collectors/jcollectd/jcollectd.py` & `diamond-next-5.0.0/src/collectors/jcollectd/jcollectd.py`

 * *Files 12% similar despite different names*

```diff
@@ -18,196 +18,190 @@
 
 #### Dependencies
 
  * jcollectd sending metrics
 
 """
 
-
-import threading
+import queue
 import re
-import Queue
+import threading
 
 import diamond.collector
 import diamond.metric
 
 import collectd_network
 
-
 ALIVE = True
 
 
 class JCollectdCollector(diamond.collector.Collector):
-
     def __init__(self, *args, **kwargs):
         super(JCollectdCollector, self).__init__(*args, **kwargs)
         self.listener_thread = None
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(JCollectdCollector, self).get_default_config()
-        config.update({
-            'path':     'jvm',
-            'listener_host': '127.0.0.1',
-            'listener_port': 25826,
-        })
+        config.update(
+            {
+                "path": "jvm",
+                "listener_host": "127.0.0.1",
+                "listener_port": 25826,
+            }
+        )
         return config
 
     def collect(self):
         if not self.listener_thread:
             self.start_listener()
 
         q = self.listener_thread.queue
         while True:
             try:
                 dp = q.get(False)
                 metric = self.make_metric(dp)
-            except Queue.Empty:
+            except queue.Empty:
                 break
             self.publish_metric(metric)
 
     def start_listener(self):
-        self.listener_thread = ListenerThread(self.config['listener_host'],
-                                              self.config['listener_port'],
-                                              self.log)
+        self.listener_thread = ListenerThread(
+            self.config["listener_host"], self.config["listener_port"], self.log
+        )
         self.listener_thread.start()
 
     def stop_listener(self):
         global ALIVE
         ALIVE = False
         self.listener_thread.join()
-        self.log.error('Listener thread is shut down.')
+        self.log.error("Listener thread is shut down.")
 
     def make_metric(self, dp):
 
-        path = ".".join((dp.host, self.config['path'], dp.name))
+        path = ".".join((dp.host, self.config["path"], dp.name))
 
-        if 'path_prefix' in self.config:
-            prefix = self.config['path_prefix']
+        if "path_prefix" in self.config:
+            prefix = self.config["path_prefix"]
             if prefix:
                 path = ".".join((prefix, path))
 
-        if 'path_suffix' in self.config:
-            suffix = self.config['path_suffix']
+        if "path_suffix" in self.config:
+            suffix = self.config["path_suffix"]
             if suffix:
                 path = ".".join((path, suffix))
 
         if dp.is_counter:
             metric_type = "COUNTER"
         else:
             metric_type = "GAUGE"
-        metric = diamond.metric.Metric(path, dp.value, dp.time,
-                                       metric_type=metric_type)
+        metric = diamond.metric.Metric(path, dp.value, dp.time, metric_type=metric_type)
 
         return metric
 
     def __del__(self):
         if self.listener_thread:
             self.stop_listener()
 
 
 class ListenerThread(threading.Thread):
-
     def __init__(self, host, port, log, poll_interval=0.4):
         super(ListenerThread, self).__init__()
-        self.name = 'JCollectdListener'  # thread name
+        self.name = "JCollectdListener"  # thread name
 
         self.host = host
         self.port = port
         self.log = log
         self.poll_interval = poll_interval
 
-        self.queue = Queue.Queue()
+        self.queue = queue.Queue()
 
     def run(self):
-        self.log.info('ListenerThread started on {0}:{1}(udp)'.format(
-            self.host, self.port))
+        self.log.info(
+            "ListenerThread started on {}:{}(udp)".format(self.host, self.port)
+        )
 
         rdr = collectd_network.Reader(self.host, self.port)
 
         try:
             while ALIVE:
                 try:
                     items = rdr.interpret(poll_interval=self.poll_interval)
                     self.send_to_collector(items)
-                except ValueError, e:
-                    self.log.warn('Dropping bad packet: {0}'.format(e))
-        except Exception, e:
-            self.log.error('caught exception: type={0}, exc={1}'.format(type(e),
-                                                                        e))
+                except ValueError as e:
+                    self.log.warn("Dropping bad packet: {}".format(e))
+        except Exception as e:
+            self.log.error("caught exception: type={}, exc={}".format(type(e), e))
 
-        self.log.info('ListenerThread - stop')
+        self.log.info("ListenerThread - stop")
 
     def send_to_collector(self, items):
         if items is None:
             return
 
         for item in items:
             try:
                 metric = self.transform(item)
                 self.queue.put(metric)
-            except Queue.Full:
-                self.log.error('Queue to collector is FULL')
-            except Exception, e:
-                self.log.error('B00M! type={0}, exception={1}'.format(type(e),
-                                                                      e))
+            except queue.Full:
+                self.log.error("Queue to collector is FULL")
+            except Exception as e:
+                self.log.error("B00M! type={}, exception={}".format(type(e), e))
 
     def transform(self, item):
-
         parts = []
-
         path = item.plugininstance
+
         # extract jvm name from 'logstash-MemoryPool Eden Space'
-        if '-' in path:
-            (jvm, tail) = path.split('-', 1)
+        if "-" in path:
+            (jvm, tail) = path.split("-", 1)
             path = tail
         else:
-            jvm = 'unnamed'
+            jvm = "unnamed"
 
         # add JVM name
         parts.append(jvm)
 
         # add mbean name (e.g. 'java_lang')
         parts.append(item.plugin)
 
         # get typed mbean: 'MemoryPool Eden Space'
-        if ' ' in path:
-            (mb_type, mb_name) = path.split(' ', 1)
+        if " " in path:
+            (mb_type, mb_name) = path.split(" ", 1)
             parts.append(mb_type)
             parts.append(mb_name)
         else:
             parts.append(path)
 
         # add property name
         parts.append(item.typeinstance)
 
         # construct full path, from safe parts
-        name = '.'.join([sanitize_word(part) for part in parts])
+        name = ".".join([sanitize_word(part) for part in parts])
 
         if item[0][0] == 0:
             is_counter = True
         else:
             is_counter = False
         dp = Datapoint(item.host, item.time, name, item[0][1], is_counter)
 
         return dp
 
 
 def sanitize_word(s):
     """Remove non-alphanumerical characters from metric word.
     And trim excessive underscores.
     """
-    s = re.sub('[^\w-]+', '_', s)
-    s = re.sub('__+', '_', s)
-    return s.strip('_')
+    s = re.sub("[^\w-]+", "_", s)
+    s = re.sub("__+", "_", s)
+    return s.strip("_")
 
 
 class Datapoint(object):
-
     def __init__(self, host, time, name, value, is_counter):
         self.host = host
         self.time = time
         self.name = name
         self.value = value
         self.is_counter = is_counter
```

### Comparing `diamond-next-4.0.515/src/collectors/jcollectd/collectd_network.py` & `diamond-next-5.0.0/src/collectors/jcollectd/collectd_network.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,41 +19,32 @@
 # @see:
 # https://raw.github.com/collectd/collectd/master/contrib/collectd_network.py
 
 """
 Collectd network protocol implementation.
 """
 
+import copy
+import datetime
+import io
 import socket
 import struct
-import select
-import platform
-from datetime import datetime
-from copy import deepcopy
-
-if platform.python_version() < '2.8.0':
-    # Python 2.7 and below io.StringIO does not like unicode
-    from StringIO import StringIO
-else:
-    try:
-        from io import StringIO
-    except ImportError:
-        from cStringIO import StringIO
 
+import select
 
 DEFAULT_PORT = 25826
 """Default port"""
 
 DEFAULT_IPv4_GROUP = "239.192.74.66"
 """Default IPv4 multicast group"""
 
 DEFAULT_IPv6_GROUP = "ff18::efc0:4a42"
 """Default IPv6 multicast group"""
 
-HR_TIME_DIV = (2.0 ** 30)
+HR_TIME_DIV = 2.0**30
 
 # Message kinds
 TYPE_HOST = 0x0000
 TYPE_TIME = 0x0001
 TYPE_TIME_HR = 0x0008
 TYPE_PLUGIN = 0x0002
 TYPE_PLUGIN_INSTANCE = 0x0003
@@ -69,34 +60,33 @@
 
 # DS kinds
 DS_TYPE_COUNTER = 0
 DS_TYPE_GAUGE = 1
 DS_TYPE_DERIVE = 2
 DS_TYPE_ABSOLUTE = 3
 
-if hasattr(struct, 'Struct'):
+if hasattr(struct, "Struct"):
     header = struct.Struct("!2H")
     number = struct.Struct("!Q")
     short = struct.Struct("!H")
     double = struct.Struct("<d")
 
 
 def decode_network_values(ptype, plen, buf):
-    """Decodes a list of DS values in collectd network format
-    """
+    """Decodes a list of DS values in collectd network format"""
     nvalues = short.unpack_from(buf, header.size)[0]
     off = header.size + short.size + nvalues
     valskip = double.size
 
     # Check whether our expected packet size is the reported one
     assert ((valskip + 1) * nvalues + short.size + header.size) == plen
     assert double.size == number.size
 
     result = []
-    for dstype in [ord(x) for x in buf[header.size + short.size:off]]:
+    for dstype in [ord(x) for x in buf[header.size + short.size : off]]:
         if dstype == DS_TYPE_COUNTER:
             result.append((dstype, number.unpack_from(buf, off)[0]))
             off += valskip
         elif dstype == DS_TYPE_GAUGE:
             result.append((dstype, double.unpack_from(buf, off)[0]))
             off += valskip
         elif dstype == DS_TYPE_DERIVE:
@@ -108,45 +98,42 @@
         else:
             raise ValueError("DS type %i unsupported" % dstype)
 
     return result
 
 
 def decode_network_number(ptype, plen, buf):
-    """Decodes a number (64-bit unsigned) from collectd network format.
-    """
+    """Decodes a number (64-bit unsigned) from collectd network format."""
     return number.unpack_from(buf, header.size)[0]
 
 
 def decode_network_string(msgtype, plen, buf):
-    """Decodes a string from collectd network format.
-    """
-    return buf[header.size:plen - 1]
+    """Decodes a string from collectd network format."""
+    return buf[header.size : plen - 1]
 
 
 # Mapping of message types to decoding functions.
 _decoders = {
-    TYPE_VALUES:            decode_network_values,
-    TYPE_TIME:              decode_network_number,
-    TYPE_TIME_HR:           decode_network_number,
-    TYPE_INTERVAL:          decode_network_number,
-    TYPE_INTERVAL_HR:       decode_network_number,
-    TYPE_HOST:              decode_network_string,
-    TYPE_PLUGIN:            decode_network_string,
-    TYPE_PLUGIN_INSTANCE:   decode_network_string,
-    TYPE_TYPE:              decode_network_string,
-    TYPE_TYPE_INSTANCE:     decode_network_string,
-    TYPE_MESSAGE:           decode_network_string,
-    TYPE_SEVERITY:          decode_network_number,
+    TYPE_VALUES: decode_network_values,
+    TYPE_TIME: decode_network_number,
+    TYPE_TIME_HR: decode_network_number,
+    TYPE_INTERVAL: decode_network_number,
+    TYPE_INTERVAL_HR: decode_network_number,
+    TYPE_HOST: decode_network_string,
+    TYPE_PLUGIN: decode_network_string,
+    TYPE_PLUGIN_INSTANCE: decode_network_string,
+    TYPE_TYPE: decode_network_string,
+    TYPE_TYPE_INSTANCE: decode_network_string,
+    TYPE_MESSAGE: decode_network_string,
+    TYPE_SEVERITY: decode_network_number,
 }
 
 
 def decode_network_packet(buf):
-    """Decodes a network packet in collectd format.
-    """
+    """Decodes a network packet in collectd format."""
     off = 0
     blen = len(buf)
 
     while off < blen:
         ptype, plen = header.unpack_from(buf, off)
 
         if plen > blen - off:
@@ -168,19 +155,19 @@
     typeinstance = None
 
     def __init__(self, **kw):
         [setattr(self, k, v) for k, v in kw.items()]
 
     @property
     def datetime(self):
-        return datetime.fromtimestamp(self.time)
+        return datetime.datetime.fromtimestamp(self.time)
 
     @property
     def source(self):
-        buf = StringIO()
+        buf = io.StringIO()
         if self.host:
             buf.write(str(self.host))
         if self.plugin:
             buf.write("/")
             buf.write(str(self.plugin))
         if self.plugininstance:
             buf.write("/")
@@ -201,15 +188,15 @@
     FAILURE = 1
     WARNING = 2
     OKAY = 4
 
     SEVERITY = {
         FAILURE: "FAILURE",
         WARNING: "WARNING",
-        OKAY:    "OKAY",
+        OKAY: "OKAY",
     }
 
     __severity = 0
     message = ""
 
     def __set_severity(self, value):
         if value in (self.FAILURE, self.WARNING, self.OKAY):
@@ -219,21 +206,21 @@
 
     @property
     def severitystring(self):
         return self.SEVERITY.get(self.severity, "UNKNOWN")
 
     def __str__(self):
         return "%s [%s] %s" % (
-               super(Notification, self).__str__(),
-               self.severitystring,
-               self.message)
+            super(Notification, self).__str__(),
+            self.severitystring,
+            self.message,
+        )
 
 
 class Values(Data, list):
-
     def __str__(self):
         return "%s %s" % (Data.__str__(self), list.__str__(self))
 
 
 def interpret_opcodes(iterable):
     vl = Values()
     nt = Notification()
@@ -257,26 +244,27 @@
             vl.type = nt.type = data
         elif kind == TYPE_TYPE_INSTANCE:
             vl.typeinstance = nt.typeinstance = data
         elif kind == TYPE_SEVERITY:
             nt.severity = data
         elif kind == TYPE_MESSAGE:
             nt.message = data
-            yield deepcopy(nt)
+            yield copy.deepcopy(nt)
         elif kind == TYPE_VALUES:
             vl[:] = data
-            yield deepcopy(vl)
+            yield copy.deepcopy(vl)
 
 
 class Reader(object):
     """Network reader for collectd data.
 
     Listens on the network in a given address, which can be a multicast
     group address, and handles reading data when it arrives.
     """
+
     addr = None
     host = None
     port = DEFAULT_PORT
 
     BUFFER_SIZE = 16384
 
     def __init__(self, host=None, port=DEFAULT_PORT, multicast=False):
@@ -294,80 +282,69 @@
 
         if self.ipv6:
             sock_type = socket.AF_INET6
         else:
             sock_type = socket.AF_UNSPEC
 
         family, socktype, proto, canonname, sockaddr = socket.getaddrinfo(
-            hostname,
-            self.port,
-            sock_type,
-            socket.SOCK_DGRAM, 0, socket.AI_PASSIVE)[0]
+            hostname, self.port, sock_type, socket.SOCK_DGRAM, 0, socket.AI_PASSIVE
+        )[0]
 
         self._sock = socket.socket(family, socktype, proto)
         self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
         self._sock.bind(sockaddr)
 
         if multicast:
             if hasattr(socket, "SO_REUSEPORT"):
-                self._sock.setsockopt(
-                    socket.SOL_SOCKET,
-                    socket.SO_REUSEPORT, 1)
+                self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
 
             val = None
+
             if family == socket.AF_INET:
                 assert "." in self.host
-                val = struct.pack("4sl",
-                                  socket.inet_aton(self.host),
-                                  socket.INADDR_ANY)
+                val = struct.pack("4sl", socket.inet_aton(self.host), socket.INADDR_ANY)
             elif family == socket.AF_INET6:
                 raise NotImplementedError("IPv6 support not ready yet")
             else:
                 raise ValueError("Unsupported network address family")
 
             if self.ipv6:
                 sock_type = socket.IPPROTO_IPV6
             else:
                 sock_type = socket.IPPROTO_IP
 
-            self._sock.setsockopt(
-                sock_type,
-                socket.IP_ADD_MEMBERSHIP, val)
-            self._sock.setsockopt(
-                sock_type,
-                socket.IP_MULTICAST_LOOP, 0)
+            self._sock.setsockopt(sock_type, socket.IP_ADD_MEMBERSHIP, val)
+            self._sock.setsockopt(sock_type, socket.IP_MULTICAST_LOOP, 0)
 
         self._readlist = [self._sock]
 
     def receive(self, poll_interval):
-        """Receives a single raw collect network packet.
-        """
-        readable, writeable, errored = select.select(self._readlist, [], [],
-                                                     poll_interval)
+        """Receives a single raw collect network packet."""
+        readable, writeable, errored = select.select(
+            self._readlist, [], [], poll_interval
+        )
         for s in readable:
             data, addr = s.recvfrom(self.BUFFER_SIZE)
             if data:
                 return data
 
         return None
 
     def decode(self, poll_interval, buf=None):
-        """Decodes a given buffer or the next received packet.
-        """
+        """Decodes a given buffer or the next received packet."""
         if buf is None:
             buf = self.receive(poll_interval)
         if buf is None:
             return None
         return decode_network_packet(buf)
 
     def interpret(self, iterable=None, poll_interval=0.2):
-        """Interprets a sequence
-        """
+        """Interprets a sequence"""
         if iterable is None:
             iterable = self.decode(poll_interval)
             if iterable is None:
                 return None
 
-        if isinstance(iterable, basestring):
+        if isinstance(iterable, str):
             iterable = self.decode(poll_interval, iterable)
 
         return interpret_opcodes(iterable)
```

### Comparing `diamond-next-4.0.515/src/collectors/httpjson/httpjson.py` & `diamond-next-5.0.0/src/collectors/httpjson/httpjson.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,72 +1,75 @@
 # coding=utf-8
 
 """
 Simple collector which get JSON and parse it into flat metrics
 
 #### Dependencies
 
- * urllib2
+ * urllib
 
 """
 
-import urllib2
 import json
+import urllib.error
+import urllib.request
+
 import diamond.collector
 
 
 class HTTPJSONCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(HTTPJSONCollector, self).get_default_config_help()
-        config_help.update({
-            'url': 'Full URL',
-            'headers': 'Header variable if needed. '
-            'Will be added to every request',
-        })
+        config_help.update(
+            {
+                "url": "Full URL",
+                "headers": "Header variable if needed. "
+                "Will be added to every request",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         default_config = super(HTTPJSONCollector, self).get_default_config()
-        default_config.update({
-            'path': 'httpjson',
-            'url': 'http://localhost/stat',
-            'headers': {'User-Agent': 'Diamond HTTP collector'},
-        })
+        default_config.update(
+            {
+                "path": "httpjson",
+                "url": "http://localhost/stat",
+                "headers": {"User-Agent": "Diamond HTTP collector"},
+            }
+        )
         return default_config
 
     def _json_to_flat_metrics(self, prefix, data):
         for key, value in data.items():
             if isinstance(value, dict):
-                for k, v in self._json_to_flat_metrics(
-                        "%s.%s" % (prefix, key), value):
+                for k, v in self._json_to_flat_metrics("%s.%s" % (prefix, key), value):
                     yield k, v
             else:
                 try:
                     int(value)
                 except ValueError:
                     value = None
                 finally:
-                    yield ("%s.%s" % (prefix, key), value)
+                    yield "%s.%s" % (prefix, key), value
 
     def collect(self):
-        url = self.config['url']
+        url = self.config["url"]
 
-        req = urllib2.Request(url, headers=self.config['headers'])
-        req.add_header('Content-type', 'application/json')
+        req = urllib.request.Request(url, headers=self.config["headers"])
+        req.add_header("Content-type", "application/json")
 
         try:
-            resp = urllib2.urlopen(req)
-        except urllib2.URLError as e:
+            resp = urllib.request.urlopen(req)
+        except urllib.error.URLError as e:
             self.log.error("Can't open url %s. %s", url, e)
         else:
 
             content = resp.read()
 
             try:
                 data = json.loads(content)
             except ValueError as e:
                 self.log.error("Can't parse JSON object from %s. %s", url, e)
             else:
-                for metric_name, metric_value in self._json_to_flat_metrics(
-                        "", data):
+                for metric_name, metric_value in self._json_to_flat_metrics("", data):
                     self.publish(metric_name, metric_value)
```

### Comparing `diamond-next-4.0.515/src/collectors/ksm/ksm.py` & `diamond-next-5.0.0/src/collectors/ksm/ksm.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,44 +11,49 @@
 #### Dependencies
 
  * KSM built into your kernel. It does not have to be enabled, but the stats
  will be less than useful if it isn't:-)
 
 """
 
-import os
 import glob
+import os
+
 import diamond.collector
 
 
 class KSMCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(KSMCollector, self).get_default_config_help()
-        config_help.update({
-            'ksm_path': "location where KSM kernel data can be found",
-        })
+        config_help.update(
+            {
+                "ksm_path": "location where KSM kernel data can be found",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Return default config.
 
         path: Graphite path output
         ksm_path: location where KSM kernel data can be found
         """
         config = super(KSMCollector, self).get_default_config()
-        config.update({
-            'path': 'ksm',
-            'ksm_path': '/sys/kernel/mm/ksm'})
+        config.update({"path": "ksm", "ksm_path": "/sys/kernel/mm/ksm"})
+
         return config
 
     def collect(self):
-        for item in glob.glob(os.path.join(self.config['ksm_path'], "*")):
+        for item in glob.glob(os.path.join(self.config["ksm_path"], "*")):
             if os.access(item, os.R_OK):
                 filehandle = open(item)
+
                 try:
-                    self.publish(os.path.basename(item),
-                                 float(filehandle.readline().rstrip()))
+                    self.publish(
+                        os.path.basename(item), float(filehandle.readline().rstrip())
+                    )
                 except ValueError:
                     pass
+
                 filehandle.close()
```

### Comparing `diamond-next-4.0.515/src/collectors/netscalersnmp/netscalersnmp.py` & `diamond-next-5.0.0/src/collectors/netscalersnmp/netscalersnmp.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,27 +4,26 @@
 SNMPCollector for Netscaler Metrics
 
 NetScaler is a network appliance manufactured by Citrix providing level 4 load
 balancing, firewall, proxy and VPN functions.
 
 """
 
-import sys
 import os
-import time
-import struct
 import re
+import struct
+
+import sys
+import time
+
+import diamond.metric
 
 # Fix Path for locating the SNMPCollector
-sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),
-                                             '../',
-                                             'snmp',
-                                             )))
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../", "snmp")))
 
-from diamond.metric import Metric
 from snmp import SNMPCollector as parent_SNMPCollector
 
 
 class NetscalerSNMPCollector(parent_SNMPCollector):
     """
     SNMPCollector for Netscaler Metrics
     """
@@ -81,229 +80,264 @@
     """
 
     NETSCALER_SYSTEM_GUAGES = {
         "cpuUsage": "1.3.6.1.4.1.5951.4.1.1.41.1.0",
         "memUsage": "1.3.6.1.4.1.5951.4.1.1.41.2.0",
         "surgeQueue": "1.3.6.1.4.1.5951.4.1.1.46.15.0",
         "establishedServerConnections": "1.3.6.1.4.1.5951.4.1.1.46.10.0",
-        "establishedClientConnections": "1.3.6.1.4.1.5951.4.1.1.46.12.0"
+        "establishedClientConnections": "1.3.6.1.4.1.5951.4.1.1.46.12.0",
     }
 
-    NETSCALER_SYSTEM_COUNTERS = {
-        "httpTotRequests": "1.3.6.1.4.1.5951.4.1.1.48.67.0"
-    }
+    NETSCALER_SYSTEM_COUNTERS = {"httpTotRequests": "1.3.6.1.4.1.5951.4.1.1.48.67.0"}
 
     NETSCALER_VSERVER_NAMES = "1.3.6.1.4.1.5951.4.1.3.1.1.1"
 
     NETSCALER_VSERVER_TYPE = "1.3.6.1.4.1.5951.4.1.3.1.1.4"
 
     NETSCALER_VSERVER_STATE = "1.3.6.1.4.1.5951.4.1.3.1.1.5"
 
     NETSCALER_VSERVER_GUAGES = {
         "vsvrRequestRate": "1.3.6.1.4.1.5951.4.1.3.1.1.43",
         "vsvrRxBytesRate": "1.3.6.1.4.1.5951.4.1.3.1.1.44",
         "vsvrTxBytesRate": "1.3.6.1.4.1.5951.4.1.3.1.1.45",
         "vsvrCurServicesUp": "1.3.6.1.4.1.5951.4.1.3.1.1.41",
         "vsvrCurServicesDown": "1.3.6.1.4.1.5951.4.1.3.1.1.37",
         "vsvrCurServicesUnknown": "1.3.6.1.4.1.5951.4.1.3.1.1.38",
-        "vsvrCurServicesTransToOutOfSvc": "1.3.6.1.4.1.5951.4.1.3.1.1.40"
+        "vsvrCurServicesTransToOutOfSvc": "1.3.6.1.4.1.5951.4.1.3.1.1.40",
     }
 
     NETSCALER_SERVICE_NAMES = "1.3.6.1.4.1.5951.4.1.2.1.1.1"
 
     NETSCALER_SERVICE_TYPE = "1.3.6.1.4.1.5951.4.1.2.1.1.4"
 
     NETSCALER_SERVICE_STATE = "1.3.6.1.4.1.5951.4.1.2.1.1.5"
 
     NETSCALER_SERVICE_GUAGES = {
         "svcRequestRate": "1.3.6.1.4.1.5951.4.1.2.1.1.42",
         "svcSurgeCount": "1.3.6.1.4.1.5951.4.1.2.1.1.10",
         "svcEstablishedConn": "1.3.6.1.4.1.5951.4.1.2.1.1.8",
         "svcActiveConn": "1.3.6.1.4.1.5951.4.1.2.1.1.9",
-        "svcCurClntConnections": "1.3.6.1.4.1.5951.4.1.2.1.1.41"
+        "svcCurClntConnections": "1.3.6.1.4.1.5951.4.1.2.1.1.41",
     }
 
     MAX_VALUE = 18446744073709551615
 
     def get_default_config_help(self):
-        config_help = super(NetscalerSNMPCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'host': 'netscaler dns address',
-            'port': 'Netscaler port to collect snmp data',
-            'community': 'SNMP community',
-            'exclude_service_type': "list of service types to exclude" +
-                                    " (see MIB EntityProtocolType)",
-            'exclude_vserver_type': "list of vserver types to exclude" +
-                                    " (see MIB EntityProtocolType)"
-        })
+        config_help = super(NetscalerSNMPCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "host": "netscaler dns address",
+                "port": "Netscaler port to collect snmp data",
+                "community": "SNMP community",
+                "exclude_service_type": "list of service types to exclude  (see MIB EntityProtocolType)",
+                "exclude_vserver_type": "list of vserver types to exclude  (see MIB EntityProtocolType)",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(NetscalerSNMPCollector, self).get_default_config()
-        config.update({
-            'path':     'netscaler',
-            'timeout':  15,
-            'exclude_service_type': [],
-            'exclude_vserver_type': [],
-            'exclude_service_state': [],
-            'exclude_vserver_state': []
-        })
+        config.update(
+            {
+                "path": "netscaler",
+                "timeout": 15,
+                "exclude_service_type": [],
+                "exclude_vserver_type": [],
+                "exclude_service_state": [],
+                "exclude_vserver_state": [],
+            }
+        )
+
         return config
 
     def get_string_index_oid(self, s):
         """Turns a string into an oid format is length of name followed by
         name chars in ascii"""
-        return (len(self.get_bytes(s)), ) + self.get_bytes(s)
+        return (len(self.get_bytes(s)),) + self.get_bytes(s)
 
     def get_bytes(self, s):
         """Turns a string into a list of byte values"""
-        return struct.unpack('%sB' % len(s), s)
+        return struct.unpack("%sB" % len(s), s)
 
     def collect_snmp(self, device, host, port, community):
         """
         Collect Netscaler SNMP stats from device
         """
         # Log
         self.log.info("Collecting Netscaler statistics from: %s", device)
 
         # Set timestamp
         timestamp = time.time()
 
         # Collect Netscaler System OIDs
         for k, v in self.NETSCALER_SYSTEM_GUAGES.items():
             # Get Metric Name and Value
-            metricName = '.'.join([k])
-            metricValue = int(self.get(v, host, port, community)[v])
+            metric_name = ".".join([k])
+            metric_value = int(self.get(v, host, port, community)[v])
+
             # Get Metric Path
-            metricPath = '.'.join(['devices', device, 'system', metricName])
+            metric_path = ".".join(["devices", device, "system", metric_name])
+
             # Create Metric
-            metric = Metric(metricPath, metricValue, timestamp, 0)
+            metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 0)
+
             # Publish Metric
             self.publish_metric(metric)
 
         # Collect Netscaler System Counter OIDs
         for k, v in self.NETSCALER_SYSTEM_COUNTERS.items():
             # Get Metric Name and Value
-            metricName = '.'.join([k])
+            metric_name = ".".join([k])
+
             # Get Metric Path
-            metricPath = '.'.join(['devices', device, 'system', metricName])
+            metric_path = ".".join(["devices", device, "system", metric_name])
+
             # Get Metric Value
-            metricValue = self.derivative(metricPath, long(
-                self.get(v, host, port, community)[v]), self.MAX_VALUE)
+            metric_value = self.derivative(
+                metric_path, int(self.get(v, host, port, community)[v]), self.MAX_VALUE
+            )
+
             # Create Metric
-            metric = Metric(metricPath, metricValue, timestamp, 0)
+            metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 0)
+
             # Publish Metric
             self.publish_metric(metric)
 
         # Collect Netscaler Services
-        serviceNames = [v.strip("\'") for v in self.walk(
-            self.NETSCALER_SERVICE_NAMES, host, port, community).values()]
+        service_names = [
+            v.strip("'")
+            for v in self.walk(
+                self.NETSCALER_SERVICE_NAMES, host, port, community
+            ).values()
+        ]
 
-        for serviceName in serviceNames:
+        for serviceName in service_names:
             # Get Service Name in OID form
-            serviceNameOid = self.get_string_index_oid(serviceName)
+            service_name_oid = self.get_string_index_oid(serviceName)
 
             # Get Service Type
-            serviceTypeOid = ".".join([self.NETSCALER_SERVICE_TYPE,
-                                       self._convert_from_oid(serviceNameOid)])
-            serviceType = int(self.get(serviceTypeOid,
-                                       host,
-                                       port,
-                                       community)[serviceTypeOid].strip("\'"))
+            service_type_oid = ".".join(
+                [self.NETSCALER_SERVICE_TYPE, self._convert_from_oid(service_name_oid)]
+            )
+            service_type = int(
+                self.get(service_type_oid, host, port, community)[
+                    service_type_oid
+                ].strip("'")
+            )
 
             # Filter excluded service types
-            if serviceType in map(lambda v: int(v),
-                                  self.config.get('exclude_service_type')):
+            if service_type in map(
+                lambda v: int(v), self.config.get("exclude_service_type")
+            ):
                 continue
 
             # Get Service State
-            serviceStateOid = ".".join([self.NETSCALER_SERVICE_STATE,
-                                        self._convert_from_oid(serviceNameOid)])
-            serviceState = int(self.get(serviceStateOid,
-                                        host,
-                                        port,
-                                        community)[serviceStateOid].strip("\'"))
+            service_state_oid = ".".join(
+                [self.NETSCALER_SERVICE_STATE, self._convert_from_oid(service_name_oid)]
+            )
+            service_state = int(
+                self.get(service_state_oid, host, port, community)[
+                    service_state_oid
+                ].strip("'")
+            )
 
             # Filter excluded service states
-            if serviceState in map(lambda v: int(v),
-                                   self.config.get('exclude_service_state')):
+            if service_state in map(
+                lambda v: int(v), self.config.get("exclude_service_state")
+            ):
                 continue
 
             for k, v in self.NETSCALER_SERVICE_GUAGES.items():
-                serviceGuageOid = ".".join(
-                    [v, self._convert_from_oid(serviceNameOid)])
+                service_guage_oid = ".".join(
+                    [v, self._convert_from_oid(service_name_oid)]
+                )
+
                 # Get Metric Name
-                metricName = '.'.join([re.sub(r'\.|\\', '_', serviceName), k])
+                metric_name = ".".join([re.sub(r"\.|\\", "_", serviceName), k])
+
                 # Get Metric Value
-                metricValue = int(self.get(serviceGuageOid,
-                                           host,
-                                           port,
-                                           community
-                                           )[serviceGuageOid].strip("\'"))
+                metric_value = int(
+                    self.get(service_guage_oid, host, port, community)[
+                        service_guage_oid
+                    ].strip("'")
+                )
+
                 # Get Metric Path
-                metricPath = '.'.join(['devices',
-                                       device,
-                                       'service',
-                                       metricName])
+                metric_path = ".".join(["devices", device, "service", metric_name])
+
                 # Create Metric
-                metric = Metric(metricPath, metricValue, timestamp, 0)
+                metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 0)
+
                 # Publish Metric
                 self.publish_metric(metric)
 
         # Collect Netscaler Vservers
-        vserverNames = [v.strip("\'") for v in self.walk(
-            self.NETSCALER_VSERVER_NAMES, host, port, community).values()]
+        vserver_names = [
+            v.strip("'")
+            for v in self.walk(
+                self.NETSCALER_VSERVER_NAMES, host, port, community
+            ).values()
+        ]
 
-        for vserverName in vserverNames:
+        for vserverName in vserver_names:
             # Get Vserver Name in OID form
-            vserverNameOid = self.get_string_index_oid(vserverName)
+            vserver_name_oid = self.get_string_index_oid(vserverName)
 
             # Get Vserver Type
-            vserverTypeOid = ".".join([self.NETSCALER_VSERVER_TYPE,
-                                       self._convert_from_oid(vserverNameOid)])
-            vserverType = int(self.get(vserverTypeOid,
-                                       host,
-                                       port,
-                                       community)[vserverTypeOid].strip("\'"))
+            vserver_type_oid = ".".join(
+                [self.NETSCALER_VSERVER_TYPE, self._convert_from_oid(vserver_name_oid)]
+            )
+            vserver_type = int(
+                self.get(vserver_type_oid, host, port, community)[
+                    vserver_type_oid
+                ].strip("'")
+            )
 
             # filter excluded vserver types
-            if vserverType in map(lambda v: int(v),
-                                  self.config.get('exclude_vserver_type')):
+            if vserver_type in map(
+                lambda v: int(v), self.config.get("exclude_vserver_type")
+            ):
                 continue
 
             # Get Service State
-            vserverStateOid = ".".join([self.NETSCALER_VSERVER_STATE,
-                                        self._convert_from_oid(vserverNameOid)])
-            vserverState = int(self.get(vserverStateOid,
-                                        host,
-                                        port,
-                                        community)[vserverStateOid].strip("\'"))
+            vserver_state_oid = ".".join(
+                [self.NETSCALER_VSERVER_STATE, self._convert_from_oid(vserver_name_oid)]
+            )
+            vserver_state = int(
+                self.get(vserver_state_oid, host, port, community)[
+                    vserver_state_oid
+                ].strip("'")
+            )
 
             # Filter excluded vserver state
-            if vserverState in map(lambda v: int(v),
-                                   self.config.get('exclude_vserver_state')):
+            if vserver_state in map(
+                lambda v: int(v), self.config.get("exclude_vserver_state")
+            ):
                 continue
 
             for k, v in self.NETSCALER_VSERVER_GUAGES.items():
-                vserverGuageOid = ".".join(
-                    [v, self._convert_from_oid(vserverNameOid)])
+                vserver_guage_oid = ".".join(
+                    [v, self._convert_from_oid(vserver_name_oid)]
+                )
+
                 # Get Metric Name
-                metricName = '.'.join([re.sub(r'\.|\\', '_', vserverName), k])
+                metric_name = ".".join([re.sub(r"\.|\\", "_", vserverName), k])
+
                 # Get Metric Value
-                metricValue = int(self.get(vserverGuageOid,
-                                           host,
-                                           port,
-                                           community
-                                           )[vserverGuageOid].strip("\'"))
+                metric_value = int(
+                    self.get(vserver_guage_oid, host, port, community)[
+                        vserver_guage_oid
+                    ].strip("'")
+                )
+
                 # Get Metric Path
-                metricPath = '.'.join(['devices',
-                                       device,
-                                       'vserver',
-                                       metricName])
+                metric_path = ".".join(["devices", device, "vserver", metric_name])
+
                 # Create Metric
-                metric = Metric(metricPath, metricValue, timestamp, 0)
+                metric = diamond.metric.Metric(metric_path, metric_value, timestamp, 0)
+
                 # Publish Metric
                 self.publish_metric(metric)
```

### Comparing `diamond-next-4.0.515/src/collectors/jolokia/cassandra_jolokia.py` & `diamond-next-5.0.0/src/collectors/jolokia/cassandra_jolokia.py`

 * *Files 11% similar despite different names*

```diff
@@ -16,63 +16,65 @@
 
 ```
     percentiles '50,95,99'
     histogram_regex '.*HistogramMicros$'
 ```
 """
 
-from jolokia import JolokiaCollector
-import math
-import string
 import re
 
+import math
+
+import jolokia
 
-class CassandraJolokiaCollector(JolokiaCollector):
-    # override to allow setting which percentiles will be collected
 
+class CassandraJolokiaCollector(jolokia.JolokiaCollector):
+    # override to allow setting which percentiles will be collected
     def get_default_config_help(self):
-        config_help = super(CassandraJolokiaCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'percentiles':
-            'Comma separated list of percentiles to be collected '
-            '(e.g., "50,95,99").',
-            'histogram_regex':
-            'Filter to only process attributes that match this regex'
-        })
+        config_help = super(CassandraJolokiaCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "percentiles": "Comma separated list of percentiles to be collected "
+                '(e.g., "50,95,99").',
+                "histogram_regex": "Filter to only process attributes that match this regex",
+            }
+        )
+
         return config_help
 
     # override to allow setting which percentiles will be collected
     def get_default_config(self):
         config = super(CassandraJolokiaCollector, self).get_default_config()
-        config.update({
-            'percentiles': ['50', '95', '99'],
-            'histogram_regex': '.*HistogramMicros$'
-        })
+        config.update(
+            {"percentiles": ["50", "95", "99"], "histogram_regex": ".*HistogramMicros$"}
+        )
+
         return config
 
     def __init__(self, *args, **kwargs):
         super(CassandraJolokiaCollector, self).__init__(*args, **kwargs)
         self.offsets = self.create_offsets(91)
         self.update_config(self.config)
 
     def update_config(self, config):
-        if 'percentiles' in config:
-            self.percentiles = map(int, config['percentiles'])
-        if 'histogram_regex' in config:
-            self.histogram_regex = re.compile(config['histogram_regex'])
+        if "percentiles" in config:
+            self.percentiles = map(int, config["percentiles"])
+
+        if "histogram_regex" in config:
+            self.histogram_regex = re.compile(config["histogram_regex"])
 
     # override: Interpret beans that match the `histogram_regex` as histograms,
     # and collect percentiles from them.
     def interpret_bean_with_list(self, prefix, values):
         if not self.histogram_regex.match(prefix):
             return
 
         buckets = values
         offsets = self.offsets
+
         for percentile in self.percentiles:
             value = self.compute_percentile(offsets, buckets, percentile)
             cleaned_key = self.clean_up("%s.p%s" % (prefix, percentile))
             self.publish(cleaned_key, value)
 
     # Adapted from Cassandra docs:
     # https://bit.ly/13M5JPE
@@ -81,31 +83,37 @@
     # values greater than the previous offset and less than or equal to the
     # current offset. The offsets start at 1 and each subsequent offset is
     # calculated by multiplying the previous offset by 1.2, rounding up, and
     # removing duplicates. The offsets can range from 1 to approximately 25
     # million, with less precision as the offsets get larger.
     def compute_percentile(self, offsets, buckets, percentile_int):
         non_zero_points_sum = sum(buckets)
-        if non_zero_points_sum is 0:
+
+        if non_zero_points_sum == 0:
             return 0
-        middle_point_index = math.floor(
-            non_zero_points_sum * (percentile_int / float(100)))
 
+        middle_point_index = math.floor(
+            non_zero_points_sum * (percentile_int / float(100))
+        )
         points_seen = 0
+
         for index, bucket in enumerate(buckets):
             points_seen += bucket
+
             if points_seen >= middle_point_index:
                 return round((offsets[index] - offsets[index - 1]) / 2)
 
     # Returns a list of offsets for `n` buckets.
     def create_offsets(self, bucket_count):
         last_num = 1
         offsets = [last_num]
 
         for index in range(bucket_count):
             next_num = round(last_num * 1.2)
+
             if next_num == last_num:
                 next_num += 1
+
             offsets.append(next_num)
             last_num = next_num
 
         return offsets
```

### Comparing `diamond-next-4.0.515/src/collectors/drbd/drbd.py` & `diamond-next-5.0.0/src/collectors/drbd/drbd.py`

 * *Files 24% similar despite different names*

```diff
@@ -2,85 +2,91 @@
 
 """
 DRBD metric collector
 
   Read and publish metrics from all available resources in /proc/drbd
 """
 
-import diamond.collector
 import re
 
+import diamond.collector
+
 
 class DRBDCollector(diamond.collector.Collector):
     """
     DRBD Simple metric collector
     """
 
     def get_default_config_help(self):
         config_help = super(DRBDCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(DRBDCollector, self).get_default_config()
-        config.update({
-            'path': 'drbd'
-        })
+        config.update({"path": "drbd"})
+
         return config
 
     def collect(self):
         """
         Overrides the Collector.collect method
         """
         performance_indicators = {
-            'ns': 'network_send',
-            'nr': 'network_receive',
-            'dw': 'disk_write',
-            'dr': 'disk_read',
-            'al': 'activity_log',
-            'bm': 'bit_map',
-            'lo': 'local_count',
-            'pe': 'pending',
-            'ua': 'unacknowledged',
-            'ap': 'application_pending',
-            'ep': 'epochs',
-            'wo': 'write_order',
-            'oos': 'out_of_sync',
-            'cs': 'connection_state',
-            'ro': 'roles',
-            'ds': 'disk_states'
+            "ns": "network_send",
+            "nr": "network_receive",
+            "dw": "disk_write",
+            "dr": "disk_read",
+            "al": "activity_log",
+            "bm": "bit_map",
+            "lo": "local_count",
+            "pe": "pending",
+            "ua": "unacknowledged",
+            "ap": "application_pending",
+            "ep": "epochs",
+            "wo": "write_order",
+            "oos": "out_of_sync",
+            "cs": "connection_state",
+            "ro": "roles",
+            "ds": "disk_states",
         }
 
         results = dict()
+
         try:
-            statusfile = open('/proc/drbd', 'r')
-            current_resource = ''
+            statusfile = open("/proc/drbd", "r")
+            current_resource = ""
+
             for line in statusfile:
-                if re.search('version', line) is None:
-                    if re.search(r' \d: cs', line):
-                        matches = re.match(r' (\d): (cs:\w+) (ro:\w+/\w+) '
-                                           '(ds:\w+/\w+) (\w{1}) .*', line)
+                if re.search("version", line) is None:
+                    if re.search(r" \d: cs", line):
+                        matches = re.match(
+                            r" (\d): (cs:\w+) (ro:\w+/\w+) (ds:\w+/\w+) (\w{1}) .*",
+                            line,
+                        )
                         current_resource = matches.group(1)
                         results[current_resource] = dict()
-                    elif re.search(r'\sns:', line):
+                    elif re.search(r"\sns:", line):
                         metrics = line.strip().split(" ")
+
                         for metric in metrics:
                             item, value = metric.split(":")
                             results[current_resource][
-                                performance_indicators[item]] = value
-
+                                performance_indicators[item]
+                            ] = value
                 else:
                     continue
+
             statusfile.close()
-        except IOError, errormsg:
-            self.log.error("Can't read DRBD status file: {0}".format(errormsg))
+        except IOError as errormsg:
+            self.log.error("Can't read DRBD status file: {}".format(errormsg))
             return
 
         for resource in results.keys():
             for metric_name, metric_value in results[resource].items():
                 if metric_value.isdigit():
                     self.publish(resource + "." + metric_name, metric_value)
                 else:
```

### Comparing `diamond-next-4.0.515/src/collectors/nfacct/nfacct.py` & `diamond-next-5.0.0/src/collectors/nfacct/nfacct.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,71 +5,82 @@
 
 #### Dependencies
 
  * [nfacct](http://www.netfilter.org/projects/nfacct/)
 
 """
 
-import diamond.collector
-from subprocess import Popen, PIPE
 import re
+import subprocess
 
-from diamond.collector import str_to_bool
+import diamond.collector
 
 
 class NetfilterAccountingCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = (
-            super(NetfilterAccountingCollector, self).get_default_config_help())
-        config_help.update({
-            'bin': 'The path to the smartctl binary',
-            'reset': 'Reset counters after collecting',
-            'use_sudo': 'Use sudo?',
-            'sudo_cmd': 'Path to sudo',
-        })
+        config_help = super(
+            NetfilterAccountingCollector, self
+        ).get_default_config_help()
+        config_help.update(
+            {
+                "bin": "The path to the smartctl binary",
+                "reset": "Reset counters after collecting",
+                "use_sudo": "Use sudo?",
+                "sudo_cmd": "Path to sudo",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns default configuration options.
         """
         config = super(NetfilterAccountingCollector, self).get_default_config()
-        config.update({
-            'path': 'nfacct',
-            'bin': 'nfacct',
-            'use_sudo': False,
-            'reset': True,
-            'sudo_cmd': '/usr/bin/sudo',
-            'method': 'Threaded'
-        })
+        config.update(
+            {
+                "path": "nfacct",
+                "bin": "nfacct",
+                "use_sudo": False,
+                "reset": True,
+                "sudo_cmd": "/usr/bin/sudo",
+                "method": "Threaded",
+            }
+        )
+
         return config
 
     def collect(self):
         """
         Collect and publish netfilter counters
         """
-        cmd = [self.config['bin'], "list"]
+        cmd = [self.config["bin"], "list"]
 
-        if str_to_bool(self.config['reset']):
+        if diamond.collector.str_to_bool(self.config["reset"]):
             cmd.append("reset")
 
-        if str_to_bool(self.config['use_sudo']):
-            cmd.insert(0, self.config['sudo_cmd'])
+        if diamond.collector.str_to_bool(self.config["use_sudo"]):
+            cmd.insert(0, self.config["sudo_cmd"])
 
         # We avoid use of the XML format to mtaintain compatbility with older
         # versions of nfacct and also to avoid the bug where pkts and bytes were
         # flipped
 
         # Each line is of the format:
         # { pkts = 00000000000001121700, bytes = 00000000000587037355 } = ipv4;
         matcher = re.compile("{ pkts = (.*), bytes = (.*) } = (.*);")
-        lines = Popen(cmd, stdout=PIPE).communicate()[0].strip().splitlines()
+        lines = (
+            subprocess.Popen(cmd, stdout=subprocess.PIPE)
+            .communicate()[0]
+            .strip()
+            .splitlines()
+        )
 
         for line in lines:
             matches = re.match(matcher, line)
+
             if matches:
                 num_packets = int(matches.group(1))
                 num_bytes = int(matches.group(2))
                 name = matches.group(3)
                 self.publish(name + ".pkts", num_packets)
                 self.publish(name + ".bytes", num_bytes)
```

### Comparing `diamond-next-4.0.515/src/collectors/userscripts/userscripts.py` & `diamond-next-5.0.0/src/collectors/userscripts/userscripts.py`

 * *Files 10% similar despite different names*

```diff
@@ -16,86 +16,109 @@
 
 #### Dependencies
 
  * [subprocess](http://docs.python.org/library/subprocess.html)
 
 """
 
-import diamond.collector
-import diamond.convertor
 import os
 import subprocess
 
+import diamond.collector
+import diamond.convertor
+
 
 class UserScriptsCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(UserScriptsCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'scripts_path': "Path to find the scripts to run",
-        })
+        config_help = super(UserScriptsCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "scripts_path": "Path to find the scripts to run",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(UserScriptsCollector, self).get_default_config()
-        config.update({
-            'path':         '.',
-            'scripts_path': '/etc/diamond/user_scripts/',
-            'floatprecision': 4,
-        })
+        config.update(
+            {
+                "path": ".",
+                "scripts_path": "/etc/diamond/user_scripts/",
+                "floatprecision": 4,
+            }
+        )
+
         return config
 
     def collect(self):
-        scripts_path = self.config['scripts_path']
+        scripts_path = self.config["scripts_path"]
+
         if not os.access(scripts_path, os.R_OK):
             return None
+
         for script in os.listdir(scripts_path):
             absolutescriptpath = os.path.join(scripts_path, script)
             executable = os.access(absolutescriptpath, os.X_OK)
             is_file = os.path.isfile(absolutescriptpath)
+
             if is_file:
                 if not executable:
                     self.log.info("%s is not executable" % absolutescriptpath)
                     continue
             else:
-                # Don't bother logging skipped non-file files (typically
-                # directories)
+                # Don't bother logging skipped non-file files (typically directories)
                 continue
-            out = None
+
             self.log.debug("Executing %s" % absolutescriptpath)
+
             try:
-                proc = subprocess.Popen([absolutescriptpath],
-                                        stdout=subprocess.PIPE,
-                                        stderr=subprocess.PIPE)
+                proc = subprocess.Popen(
+                    [absolutescriptpath], stdout=subprocess.PIPE, stderr=subprocess.PIPE
+                )
                 (out, err) = proc.communicate()
-            except subprocess.CalledProcessError, e:
-                self.log.error("%s error launching: %s; skipping" %
-                               (absolutescriptpath, e))
+            except subprocess.CalledProcessError as e:
+                self.log.error(
+                    "%s error launching: %s; skipping" % (absolutescriptpath, e)
+                )
                 continue
+
             if proc.returncode:
-                self.log.error("%s return exit value %s; skipping" %
-                               (absolutescriptpath, proc.returncode))
+                self.log.error(
+                    "%s return exit value %s; skipping"
+                    % (absolutescriptpath, proc.returncode)
+                )
+
             if not out:
                 self.log.info("%s return no output" % absolutescriptpath)
                 continue
+
             if err:
-                self.log.error("%s returned error output (stderr): %s" %
-                               (absolutescriptpath, err))
+                self.log.error(
+                    "%s returned error output (stderr): %s" % (absolutescriptpath, err)
+                )
+
+            out = out.decode("utf-8")
+
             # Use filter to remove empty lines of output
-            for line in filter(None, out.split('\n')):
+            for line in filter(None, out.split("\n")):
                 # Ignore invalid lines
                 try:
                     name, value = line.split()
                     float(value)
                 except ValueError:
-                    self.log.error("%s returned invalid/unparsable output: %s" %
-                                   (absolutescriptpath, line))
+                    self.log.error(
+                        "%s returned invalid/unparsable output: %s"
+                        % (absolutescriptpath, line)
+                    )
                     continue
+
                 name, value = line.split()
                 floatprecision = 0
+
                 if "." in value:
-                    floatprecision = self.config['floatprecision']
+                    floatprecision = self.config["floatprecision"]
+
                 self.publish(name, value, precision=floatprecision)
```

### Comparing `diamond-next-4.0.515/src/collectors/redisstat/redisstat.py` & `diamond-next-5.0.0/src/collectors/redisstat/redisstat.py`

 * *Files 11% similar despite different names*

```diff
@@ -53,115 +53,123 @@
 If not specified the port will be used. In case of unix sockets, the base name
 without file extension (i.e. in the aforementioned examples ``redis``)
 is the default metric key.
 
 
 """
 
-import diamond.collector
-import time
 import os
+import time
+
+import diamond.collector
 
 try:
     import redis
 except ImportError:
     redis = None
 
 
-SOCKET_PREFIX = 'unix:'
+SOCKET_PREFIX = "unix:"
 SOCKET_PREFIX_LEN = len(SOCKET_PREFIX)
 
 
 class RedisCollector(diamond.collector.Collector):
-
     _DATABASE_COUNT = 16
     _DEFAULT_DB = 0
-    _DEFAULT_HOST = 'localhost'
+    _DEFAULT_HOST = "localhost"
     _DEFAULT_PORT = 6379
     _DEFAULT_SOCK_TIMEOUT = 5
-    _KEYS = {'clients.blocked': 'blocked_clients',
-             'clients.connected': 'connected_clients',
-             'clients.longest_output_list': 'client_longest_output_list',
-             'cpu.parent.sys': 'used_cpu_sys',
-             'cpu.children.sys': 'used_cpu_sys_children',
-             'cpu.parent.user': 'used_cpu_user',
-             'cpu.children.user': 'used_cpu_user_children',
-             'hash_max_zipmap.entries': 'hash_max_zipmap_entries',
-             'hash_max_zipmap.value': 'hash_max_zipmap_value',
-             'keys.evicted': 'evicted_keys',
-             'keys.expired': 'expired_keys',
-             'keyspace.hits': 'keyspace_hits',
-             'keyspace.misses': 'keyspace_misses',
-             'last_save.changes_since': 'changes_since_last_save',
-             'last_save.time': 'last_save_time',
-             'memory.internal_view': 'used_memory',
-             'memory.external_view': 'used_memory_rss',
-             'memory.fragmentation_ratio': 'mem_fragmentation_ratio',
-             'process.commands_processed': 'total_commands_processed',
-             'process.connections_received': 'total_connections_received',
-             'process.uptime': 'uptime_in_seconds',
-             'pubsub.channels': 'pubsub_channels',
-             'pubsub.patterns': 'pubsub_patterns',
-             'slaves.connected': 'connected_slaves',
-             'slaves.last_io': 'master_last_io_seconds_ago'}
-    _RENAMED_KEYS = {'last_save.changes_since': 'rdb_changes_since_last_save',
-                     'last_save.time': 'rdb_last_save_time'}
+    _KEYS = {
+        "clients.blocked": "blocked_clients",
+        "clients.connected": "connected_clients",
+        "clients.longest_output_list": "client_longest_output_list",
+        "cpu.parent.sys": "used_cpu_sys",
+        "cpu.children.sys": "used_cpu_sys_children",
+        "cpu.parent.user": "used_cpu_user",
+        "cpu.children.user": "used_cpu_user_children",
+        "hash_max_zipmap.entries": "hash_max_zipmap_entries",
+        "hash_max_zipmap.value": "hash_max_zipmap_value",
+        "keys.evicted": "evicted_keys",
+        "keys.expired": "expired_keys",
+        "keyspace.hits": "keyspace_hits",
+        "keyspace.misses": "keyspace_misses",
+        "last_save.changes_since": "changes_since_last_save",
+        "last_save.time": "last_save_time",
+        "memory.internal_view": "used_memory",
+        "memory.external_view": "used_memory_rss",
+        "memory.fragmentation_ratio": "mem_fragmentation_ratio",
+        "process.commands_processed": "total_commands_processed",
+        "process.connections_received": "total_connections_received",
+        "process.uptime": "uptime_in_seconds",
+        "pubsub.channels": "pubsub_channels",
+        "pubsub.patterns": "pubsub_patterns",
+        "replication.master_sync_in_progress": "master_sync_in_progress",
+        "slaves.connected": "connected_slaves",
+        "slaves.last_io": "master_last_io_seconds_ago",
+    }
+    _RENAMED_KEYS = {
+        "last_save.changes_since": "rdb_changes_since_last_save",
+        "last_save.time": "rdb_last_save_time",
+    }
 
     def process_config(self):
         super(RedisCollector, self).process_config()
-        instance_list = self.config['instances']
+        instance_list = self.config["instances"]
+
         # configobj make str of single-element list, let's convert
-        if isinstance(instance_list, basestring):
+        if isinstance(instance_list, str):
             instance_list = [instance_list]
 
         # process original single redis instance
         if len(instance_list) == 0:
-            host = self.config['host']
-            port = int(self.config['port'])
-            auth = self.config['auth']
+            host = self.config["host"]
+            port = int(self.config["port"])
+            auth = self.config["auth"]
+
             if auth is not None:
-                instance_list.append('%s:%d/%s' % (host, port, auth))
+                instance_list.append("%s:%d/%s" % (host, port, auth))
             else:
-                instance_list.append('%s:%d' % (host, port))
+                instance_list.append("%s:%d" % (host, port))
 
         self.instances = {}
-        for instance in instance_list:
 
-            if '@' in instance:
-                (nickname, hostport) = instance.split('@', 1)
+        for instance in instance_list:
+            if "@" in instance:
+                (nickname, hostport) = instance.split("@", 1)
             else:
                 nickname = None
                 hostport = instance
 
             if hostport.startswith(SOCKET_PREFIX):
-                unix_socket, __, port_auth = hostport[
-                    SOCKET_PREFIX_LEN:].partition(':')
-                auth = port_auth.partition('/')[2] or None
+                unix_socket, __, port_auth = hostport[SOCKET_PREFIX_LEN:].partition(":")
+                auth = port_auth.partition("/")[2] or None
 
                 if nickname is None:
-                    nickname = os.path.splitext(
-                        os.path.basename(unix_socket))[0]
-                self.instances[nickname] = (self._DEFAULT_HOST,
-                                            self._DEFAULT_PORT,
-                                            unix_socket,
-                                            auth)
+                    nickname = os.path.splitext(os.path.basename(unix_socket))[0]
+
+                self.instances[nickname] = (
+                    self._DEFAULT_HOST,
+                    self._DEFAULT_PORT,
+                    unix_socket,
+                    auth,
+                )
             else:
-                if '/' in hostport:
-                    parts = hostport.split('/')
+                if "/" in hostport:
+                    parts = hostport.split("/")
                     hostport = parts[0]
-                    auth = parts[1]
+                    auth = "/".join(parts[1:])
                 else:
                     auth = None
 
-                if ':' in hostport:
-                    if hostport[0] == ':':
+                if ":" in hostport:
+                    if hostport[0] == ":":
                         host = self._DEFAULT_HOST
                         port = int(hostport[1:])
                     else:
-                        parts = hostport.split(':')
+                        parts = hostport.split(":")
                         host = parts[0]
                         port = int(parts[1])
                 else:
                     host = hostport
                     port = self._DEFAULT_PORT
 
                 if nickname is None:
@@ -169,193 +177,221 @@
 
                 self.instances[nickname] = (host, port, None, auth)
 
         self.log.debug("Configured instances: %s" % self.instances.items())
 
     def get_default_config_help(self):
         config_help = super(RedisCollector, self).get_default_config_help()
-        config_help.update({
-            'host': 'Hostname to collect from',
-            'port': 'Port number to collect from',
-            'timeout': 'Socket timeout',
-            'db': '',
-            'auth': 'Password?',
-            'databases': 'how many database instances to collect',
-            'instances': "Redis addresses, comma separated, syntax:" +
-                         " nick1@host:port, nick2@:port or nick3@host"
-        })
+        config_help.update(
+            {
+                "host": "Hostname to collect from",
+                "port": "Port number to collect from",
+                "timeout": "Socket timeout",
+                "db": "",
+                "auth": "Password?",
+                "databases": "how many database instances to collect",
+                "instances": "Redis addresses, comma separated, syntax: nick1@host:port, nick2@:port or nick3@host",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
-        Return default config
+                Return default config
 
-:rtype: dict
+        :rtype: dict
 
         """
         config = super(RedisCollector, self).get_default_config()
-        config.update({
-            'host': self._DEFAULT_HOST,
-            'port': self._DEFAULT_PORT,
-            'timeout': self._DEFAULT_SOCK_TIMEOUT,
-            'db': self._DEFAULT_DB,
-            'auth': None,
-            'databases': self._DATABASE_COUNT,
-            'path': 'redis',
-            'instances': [],
-        })
+        config.update(
+            {
+                "host": self._DEFAULT_HOST,
+                "port": self._DEFAULT_PORT,
+                "timeout": self._DEFAULT_SOCK_TIMEOUT,
+                "db": self._DEFAULT_DB,
+                "auth": None,
+                "databases": self._DATABASE_COUNT,
+                "path": "redis",
+                "instances": [],
+            }
+        )
         return config
 
     def _client(self, host, port, unix_socket, auth):
         """Return a redis client for the configuration.
 
-:param str host: redis host
-:param int port: redis port
-:rtype: redis.Redis
+        :param str host: redis host
+        :param int port: redis port
+        :rtype: redis.Redis
 
         """
-        db = int(self.config['db'])
-        timeout = int(self.config['timeout'])
+        db = int(self.config["db"])
+        timeout = int(self.config["timeout"])
+
         try:
-            cli = redis.Redis(host=host, port=port,
-                              db=db, socket_timeout=timeout, password=auth,
-                              unix_socket_path=unix_socket)
+            cli = redis.Redis(
+                host=host,
+                port=port,
+                db=db,
+                socket_timeout=timeout,
+                password=auth,
+                unix_socket_path=unix_socket,
+            )
             cli.ping()
             return cli
-        except Exception, ex:
-            self.log.error("RedisCollector: failed to connect to %s:%i. %s.",
-                           unix_socket or host, port, ex)
+        except Exception as ex:
+            self.log.error(
+                "RedisCollector: failed to connect to %s:%i. %s.",
+                unix_socket or host,
+                port,
+                ex,
+            )
 
     def _precision(self, value):
         """Return the precision of the number
 
-:param str value: The value to find the precision of
-:rtype: int
+        :param str value: The value to find the precision of
+        :rtype: int
 
         """
         value = str(value)
-        decimal = value.rfind('.')
+        decimal = value.rfind(".")
+
         if decimal == -1:
             return 0
+
         return len(value) - decimal - 1
 
     def _publish_key(self, nick, key):
         """Return the full key for the partial key.
 
-:param str nick: Nickname for Redis instance
-:param str key: The key name
-:rtype: str
+        :param str nick: Nickname for Redis instance
+        :param str key: The key name
+        :rtype: str
 
         """
-        return '%s.%s' % (nick, key)
+        return "%s.%s" % (nick, key)
 
     def _get_info(self, host, port, unix_socket, auth):
         """Return info dict from specified Redis instance
 
-:param str host: redis host
-:param int port: redis port
-:rtype: dict
+        :param str host: redis host
+        :param int port: redis port
+        :rtype: dict
 
         """
 
         client = self._client(host, port, unix_socket, auth)
         if client is None:
             return None
 
         info = client.info()
         del client
         return info
 
     def _get_config(self, host, port, unix_socket, auth, config_key):
         """Return config string from specified Redis instance and config key
 
-:param str host: redis host
-:param int port: redis port
-:param str host: redis config_key
-:rtype: str
+        :param str host: redis host
+        :param int port: redis port
+        :param str host: redis config_key
+        :rtype: str
 
         """
 
         client = self._client(host, port, unix_socket, auth)
         if client is None:
             return None
 
         config_value = client.config_get(config_key)
         del client
         return config_value
 
     def collect_instance(self, nick, host, port, unix_socket, auth):
         """Collect metrics from a single Redis instance
 
-:param str nick: nickname of redis instance
-:param str host: redis host
-:param int port: redis port
-:param str unix_socket: unix socket, if applicable
-:param str auth: authentication password
+        :param str nick: nickname of redis instance
+        :param str host: redis host
+        :param int port: redis port
+        :param str unix_socket: unix socket, if applicable
+        :param str auth: authentication password
 
         """
 
         # Connect to redis and get the info
         info = self._get_info(host, port, unix_socket, auth)
         if info is None:
             return
 
         # The structure should include the port for multiple instances per
         # server
         data = dict()
 
+        # Role needs to be handled outside the the _KEYS dict
+        # since the value is a string, not a int / float
+        # Also, master_sync_in_progress is only available if the
+        # redis instance is a slave, so default it here so that
+        # the metric is cleared if the instance flips from slave
+        # to master
+        if "role" in info:
+            if info["role"] == "master":
+                data["replication.master"] = 1
+                data["replication.master_sync_in_progress"] = 0
+            else:
+                data["replication.master"] = 0
+
         # Connect to redis and get the maxmemory config value
         # Then calculate the % maxmemory of memory used
-        maxmemory_config = self._get_config(host, port, unix_socket, auth,
-                                            'maxmemory')
-        if maxmemory_config and 'maxmemory' in maxmemory_config.keys():
-            maxmemory = float(maxmemory_config['maxmemory'])
+        maxmemory_config = self._get_config(host, port, unix_socket, auth, "maxmemory")
+
+        if maxmemory_config and "maxmemory" in maxmemory_config.keys():
+            maxmemory = float(maxmemory_config["maxmemory"])
 
             # Only report % used if maxmemory is a non zero value
             if maxmemory == 0:
                 maxmemory_percent = 0.0
             else:
-                maxmemory_percent = info['used_memory'] / maxmemory * 100
+                maxmemory_percent = info["used_memory"] / maxmemory * 100
                 maxmemory_percent = round(maxmemory_percent, 2)
-            data['memory.used_percent'] = float("%.2f" % maxmemory_percent)
+
+            data["memory.used_percent"] = float("%.2f" % maxmemory_percent)
 
         # Iterate over the top level keys
         for key in self._KEYS:
             if self._KEYS[key] in info:
                 data[key] = info[self._KEYS[key]]
 
         # Iterate over renamed keys for 2.6 support
         for key in self._RENAMED_KEYS:
             if self._RENAMED_KEYS[key] in info:
                 data[key] = info[self._RENAMED_KEYS[key]]
 
         # Look for databaase speific stats
-        for dbnum in range(0, int(self.config.get('databases',
-                                                  self._DATABASE_COUNT))):
-            db = 'db%i' % dbnum
+        for dbnum in range(0, int(self.config.get("databases", self._DATABASE_COUNT))):
+            db = "db%i" % dbnum
             if db in info:
                 for key in info[db]:
-                    data['%s.%s' % (db, key)] = info[db][key]
+                    data["%s.%s" % (db, key)] = info[db][key]
 
         # Time since last save
-        for key in ['last_save_time', 'rdb_last_save_time']:
+        for key in ["last_save_time", "rdb_last_save_time"]:
             if key in info:
-                data['last_save.time_since'] = int(time.time()) - info[key]
+                data["last_save.time_since"] = int(time.time()) - info[key]
 
         # Publish the data to graphite
         for key in data:
-            self.publish(self._publish_key(nick, key),
-                         data[key],
-                         precision=self._precision(data[key]),
-                         metric_type='GAUGE')
+            self.publish(
+                self._publish_key(nick, key),
+                data[key],
+                precision=self._precision(data[key]),
+                metric_type="GAUGE",
+            )
 
     def collect(self):
-        """Collect the stats from the redis instance and publish them.
-
-        """
+        """Collect the stats from the redis instance and publish them."""
         if redis is None:
-            self.log.error('Unable to import module redis')
+            self.log.error("Unable to import module redis")
             return {}
 
         for nick in self.instances.keys():
             (host, port, unix_socket, auth) = self.instances[nick]
             self.collect_instance(nick, host, int(port), unix_socket, auth)
```

### Comparing `diamond-next-4.0.515/src/collectors/snmpinterface/snmpinterface.py` & `diamond-next-5.0.0/src/collectors/snmpinterface/snmpinterface.py`

 * *Files 20% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 remote SNMP-enabled devices such as routers and switches using SNMP IF_MIB
 
 #### Installation
 
 The snmpinterfacecollector.py module should be installed into your Diamond
 installation collectors directory. This directory is defined
 in diamond.cfg under the *collectors_path* directive. This defaults to
-*/usr/lib/diamond/collectors/* on Ubuntu.
+*/usr/share/diamond/collectors/* on Ubuntu.
 
 The SNMPInterfaceCollector.cfg file should be installed into your diamond
 installation config directory. This directory is defined
 in diamond.cfg under the *collectors_config_path* directive. This defaults to
 */etc/diamond/* on Ubuntu.
 
 Once the collector is installed and configured, you can wait for diamond to
@@ -58,158 +58,166 @@
 This implimentation is well suited for collecting a small number of metrics
 locally. If you want to collect a large number of remote metrics, consider
 https://github.com/GreggBzz/snmp-interface-poll as an alternative collector
 
 """
 
 import os
-import sys
 import re
 
-sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)),
-                                'snmp'))
-from snmp import SNMPCollector as parent_SNMPCollector
+import sys
+
 import diamond.convertor
 
+sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), "snmp"))
 
-class SNMPInterfaceCollector(parent_SNMPCollector):
+from snmp import SNMPCollector as parent_SNMPCollector
 
+
+class SNMPInterfaceCollector(parent_SNMPCollector):
     # IF-MIB OID
     IF_MIB_INDEX_OID = "1.3.6.1.2.1.2.2.1.1"
     IF_MIB_NAME_OID = "1.3.6.1.2.1.31.1.1.1.1"
     IF_MIB_TYPE_OID = "1.3.6.1.2.1.2.2.1.3"
 
     # A list of IF-MIB 32bit counters to walk
-    IF_MIB_GAUGE_OID_TABLE = {'ifInDiscards': "1.3.6.1.2.1.2.2.1.13",
-                              'ifInErrors': "1.3.6.1.2.1.2.2.1.14",
-                              'ifOutDiscards': "1.3.6.1.2.1.2.2.1.19",
-                              'ifOutErrors': "1.3.6.1.2.1.2.2.1.20"}
+    IF_MIB_GAUGE_OID_TABLE = {
+        "ifInDiscards": "1.3.6.1.2.1.2.2.1.13",
+        "ifInErrors": "1.3.6.1.2.1.2.2.1.14",
+        "ifOutDiscards": "1.3.6.1.2.1.2.2.1.19",
+        "ifOutErrors": "1.3.6.1.2.1.2.2.1.20",
+    }
 
     # A list of IF-MIB 64bit counters to talk
-    IF_MIB_COUNTER_OID_TABLE = {'ifHCInOctets': "1.3.6.1.2.1.31.1.1.1.6",
-                                'ifInUcastPkts': "1.3.6.1.2.1.31.1.1.1.7",
-                                'ifInMulticastPkts': "1.3.6.1.2.1.31.1.1.1.8",
-                                'ifInBroadcastPkts': "1.3.6.1.2.1.31.1.1.1.9",
-                                'ifHCOutOctets': "1.3.6.1.2.1.31.1.1.1.10",
-                                'ifOutUcastPkts': "1.3.6.1.2.1.31.1.1.1.11",
-                                'ifOutMulticastPkts': "1.3.6.1.2.1.31.1.1.1.12",
-                                'ifOutBroadcastPkts': "1.3.6.1.2.1.31.1.1.1.13"}
+    IF_MIB_COUNTER_OID_TABLE = {
+        "ifHCInOctets": "1.3.6.1.2.1.31.1.1.1.6",
+        "ifInUcastPkts": "1.3.6.1.2.1.31.1.1.1.7",
+        "ifInMulticastPkts": "1.3.6.1.2.1.31.1.1.1.8",
+        "ifInBroadcastPkts": "1.3.6.1.2.1.31.1.1.1.9",
+        "ifHCOutOctets": "1.3.6.1.2.1.31.1.1.1.10",
+        "ifOutUcastPkts": "1.3.6.1.2.1.31.1.1.1.11",
+        "ifOutMulticastPkts": "1.3.6.1.2.1.31.1.1.1.12",
+        "ifOutBroadcastPkts": "1.3.6.1.2.1.31.1.1.1.13",
+    }
 
     # A list of interface types we care about
     IF_TYPES = ["6"]
 
     def get_default_config_help(self):
-        config_help = super(SNMPInterfaceCollector,
-                            self).get_default_config_help()
-        config_help.update({
-        })
+        config_help = super(SNMPInterfaceCollector, self).get_default_config_help()
+        config_help.update({})
+
         return config_help
 
     def get_default_config(self):
         """
         Override SNMPCollector.get_default_config method to provide
         default_config for the SNMPInterfaceCollector
         """
-        default_config = super(SNMPInterfaceCollector,
-                               self).get_default_config()
-        default_config['path'] = 'interface'
-        default_config['byte_unit'] = ['bit', 'byte']
+        default_config = super(SNMPInterfaceCollector, self).get_default_config()
+        default_config["path"] = "interface"
+        default_config["byte_unit"] = ["bit", "byte"]
         return default_config
 
     def collect_snmp(self, device, host, port, community):
         """
         Collect SNMP interface data from device
         """
         # Log
         self.log.info("Collecting SNMP interface statistics from: %s", device)
 
-        # Define a list of interface indexes
-        ifIndexes = []
-
         # Get Interface Indexes
-        ifIndexOid = '.'.join([self.IF_MIB_INDEX_OID])
-        ifIndexData = self.walk(ifIndexOid, host, port, community)
-        ifIndexes = [v for v in ifIndexData.values()]
+        if_index_oid = ".".join([self.IF_MIB_INDEX_OID])
+        if_index_data = self.walk(if_index_oid, host, port, community)
 
-        for ifIndex in ifIndexes:
+        # Define a list of interface indexes
+        if_indexes = [v for v in if_index_data.values()]
+
+        for ifIndex in if_indexes:
             # Get Interface Type
-            ifTypeOid = '.'.join([self.IF_MIB_TYPE_OID, ifIndex])
-            ifTypeData = self.get(ifTypeOid, host, port, community)
-            if ifTypeData[ifTypeOid] not in self.IF_TYPES:
+            if_type_oid = ".".join([self.IF_MIB_TYPE_OID, ifIndex])
+            if_type_data = self.get(if_type_oid, host, port, community)
+
+            if if_type_data[if_type_oid] not in self.IF_TYPES:
                 # Skip Interface
                 continue
+
             # Get Interface Name
-            ifNameOid = '.'.join([self.IF_MIB_NAME_OID, ifIndex])
-            ifNameData = self.get(ifNameOid, host, port, community)
-            ifName = ifNameData[ifNameOid]
+            if_name_oid = ".".join([self.IF_MIB_NAME_OID, ifIndex])
+            if_name_data = self.get(if_name_oid, host, port, community)
+            if_name = if_name_data[if_name_oid]
+
             # Remove quotes from string
-            ifName = re.sub(r'(\"|\')', '', ifName)
+            if_name = re.sub(r"(\"|\')", "", if_name)
 
             # Get Gauges
             for gaugeName, gaugeOid in self.IF_MIB_GAUGE_OID_TABLE.items():
-                ifGaugeOid = '.'.join([self.IF_MIB_GAUGE_OID_TABLE[gaugeName],
-                                       ifIndex])
-                ifGaugeData = self.get(ifGaugeOid, host, port, community)
-                ifGaugeValue = ifGaugeData[ifGaugeOid]
-                if not ifGaugeValue:
+                if_gauge_oid = ".".join(
+                    [self.IF_MIB_GAUGE_OID_TABLE[gaugeName], ifIndex]
+                )
+                if_gauge_data = self.get(if_gauge_oid, host, port, community)
+                if_gauge_value = if_gauge_data[if_gauge_oid]
+
+                if not if_gauge_value:
                     continue
 
                 # Get Metric Name and Value
-                metricIfDescr = re.sub(r'\W', '_', ifName)
-                metricName = '.'.join([metricIfDescr, gaugeName])
-                metricValue = int(ifGaugeValue)
+                metric_if_descr = re.sub(r"\W", "_", if_name)
+                metric_name = ".".join([metric_if_descr, gaugeName])
+                metric_value = int(if_gauge_value)
+
                 # Get Metric Path
-                metricPath = '.'.join(['devices',
-                                       device,
-                                       self.config['path'],
-                                       metricName])
+                metric_path = ".".join(
+                    ["devices", device, self.config["path"], metric_name]
+                )
+
                 # Publish Metric
-                self.publish_gauge(metricPath, metricValue)
+                self.publish_gauge(metric_path, metric_value)
 
             # Get counters (64bit)
-            counterItems = self.IF_MIB_COUNTER_OID_TABLE.items()
-            for counterName, counterOid in counterItems:
-                ifCounterOid = '.'.join(
-                    [self.IF_MIB_COUNTER_OID_TABLE[counterName], ifIndex])
-                ifCounterData = self.get(ifCounterOid, host, port, community)
-                ifCounterValue = ifCounterData[ifCounterOid]
-                if not ifCounterValue:
+            counter_items = self.IF_MIB_COUNTER_OID_TABLE.items()
+
+            for counterName, counterOid in counter_items:
+                if_counter_oid = ".".join(
+                    [self.IF_MIB_COUNTER_OID_TABLE[counterName], ifIndex]
+                )
+                if_counter_data = self.get(if_counter_oid, host, port, community)
+                if_counter_value = if_counter_data[if_counter_oid]
+
+                if not if_counter_value:
                     continue
 
                 # Get Metric Name and Value
-                metricIfDescr = re.sub(r'\W', '_', ifName)
+                metric_if_descr = re.sub(r"\W", "_", if_name)
 
-                if counterName in ['ifHCInOctets', 'ifHCOutOctets']:
-                    for unit in self.config['byte_unit']:
+                if counterName in ["ifHCInOctets", "ifHCOutOctets"]:
+                    for unit in self.config["byte_unit"]:
                         # Convert Metric
-                        metricName = '.'.join([metricIfDescr,
-                                               counterName.replace('Octets',
-                                                                   unit)])
-                        metricValue = diamond.convertor.binary.convert(
-                            value=ifCounterValue,
-                            oldUnit='byte',
-                            newUnit=unit)
+                        metric_name = ".".join(
+                            [metric_if_descr, counterName.replace("Octets", unit)]
+                        )
+                        metric_value = diamond.convertor.binary.convert(
+                            value=if_counter_value, old_unit="byte", new_unit=unit
+                        )
 
                         # Get Metric Path
-                        metricPath = '.'.join(['devices',
-                                               device,
-                                               self.config['path'],
-                                               metricName])
+                        metric_path = ".".join(
+                            ["devices", device, self.config["path"], metric_name]
+                        )
+
                         # Publish Metric
-                        self.publish_counter(metricPath,
-                                             metricValue,
-                                             max_value=18446744073709600000,
-                                             )
+                        self.publish_counter(
+                            metric_path, metric_value, max_value=18446744073709600000
+                        )
                 else:
-                    metricName = '.'.join([metricIfDescr, counterName])
-                    metricValue = int(ifCounterValue)
+                    metric_name = ".".join([metric_if_descr, counterName])
+                    metric_value = int(if_counter_value)
 
                     # Get Metric Path
-                    metricPath = '.'.join(['devices',
-                                           device,
-                                           self.config['path'],
-                                           metricName])
+                    metric_path = ".".join(
+                        ["devices", device, self.config["path"], metric_name]
+                    )
+
                     # Publish Metric
-                    self.publish_counter(metricPath,
-                                         metricValue,
-                                         max_value=18446744073709600000,
-                                         )
+                    self.publish_counter(
+                        metric_path, metric_value, max_value=18446744073709600000
+                    )
```

### Comparing `diamond-next-4.0.515/src/collectors/twemproxy/twemproxy.py` & `diamond-next-5.0.0/src/collectors/twemproxy/twemproxy.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,136 +19,133 @@
 TO use a unix socket, set a host string like this
 
 ```
     hosts = /path/to/blah.sock, app-1@/path/to/bleh.sock,
 ```
 """
 
-import diamond.collector
-import socket
 import re
+import socket
+
+import diamond.collector
 
 try:
     import simplejson as json
 except ImportError:
     import json
 
 
 class TwemproxyCollector(diamond.collector.Collector):
     GAUGES = [
-        'uptime',
-        'curr_connections',
-        'client_connections',
-        'server_connections',
-        'server_ejected_at',
-        'in_queue',
-        'in_queue_bytes',
-        'out_queue',
-        'out_queue_bytes'
+        "uptime",
+        "curr_connections",
+        "client_connections",
+        "server_connections",
+        "server_ejected_at",
+        "in_queue",
+        "in_queue_bytes",
+        "out_queue",
+        "out_queue_bytes",
     ]
 
-    IGNORED = [
-        'service',
-        'source',
-        'timestamp',
-        'version'
-    ]
+    IGNORED = ["service", "source", "timestamp", "version"]
 
     def get_default_config_help(self):
         config_help = super(TwemproxyCollector, self).get_default_config_help()
-        config_help.update({
-            'hosts': "List of hosts, and ports to collect. Set an alias by " +
-            " prefixing the host:port with alias@",
-        })
+        config_help.update(
+            {
+                "hosts": "List of hosts, and ports to collect. Set an alias by  prefixing the host:port with alias@",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(TwemproxyCollector, self).get_default_config()
-        config.update({
-            'path':     'twemproxy',
-            'hosts': ['localhost:22222']
-        })
+        config.update({"path": "twemproxy", "hosts": ["localhost:22222"]})
         return config
 
     def get_raw_stats(self, host, port):
-        data = ''
+        stats_data = ""
+
         # connect
         try:
             if port is None:
                 sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
                 sock.connect(host)
             else:
                 sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                 sock.connect((host, int(port)))
 
-            stats_data = ''
             while True:
                 data = sock.recv(1024)
+
                 if not data:
                     break
+
                 stats_data += data
+
             sock.close()
 
         except socket.error:
-            self.log.exception('Failed to get stats from %s:%s',
-                               host, port)
+            self.log.exception("Failed to get stats from %s:%s", host, port)
 
         try:
             return json.loads(stats_data)
         except (TypeError, ValueError):
-            self.log.error("Unable to parse response from Twemproxy as a"
-                           " json object")
+            self.log.error("Unable to parse response from Twemproxy as a json object")
             return False
 
     def get_stats(self, host, port):
-        stats = {}
-        pools = {}
         data = self.get_raw_stats(host, port)
 
         if data is None:
-            self.log.error('Unable to import json')
+            self.log.error("Unable to import json")
             return {}
 
         stats = {}
         pools = {}
-        for stat, value in data.iteritems():
+
+        for stat, value in iter(data.items()):
             # Test if this is a pool
             if isinstance(value, dict):
-                pool_name = stat.replace('.', '_')
+                pool_name = stat.replace(".", "_")
                 pools[pool_name] = {}
-                for pool_stat, pool_value in value.iteritems():
+
+                for pool_stat, pool_value in iter(value.items()):
                     # Test if this is a pool server
                     if isinstance(pool_value, dict):
-                        server_name = pool_stat.replace('.', '_')
+                        server_name = pool_stat.replace(".", "_")
                         pools[pool_name][server_name] = {}
-                        for server_stat, server_value in pool_value.iteritems():
-                            pools[pool_name][server_name][server_stat] = \
-                                int(server_value)
+
+                        for server_stat, server_value in iter(pool_value.items()):
+                            pools[pool_name][server_name][server_stat] = int(
+                                server_value
+                            )
                     else:
                         pools[pool_name][pool_stat] = int(pool_value)
             else:
                 if stat in self.IGNORED:
                     continue
                 else:
                     stats[stat] = int(value)
 
         return stats, pools
 
     def collect(self):
-        hosts = self.config.get('hosts')
+        hosts = self.config.get("hosts")
 
         # Convert a string config value to be an array
-        if isinstance(hosts, basestring):
+        if isinstance(hosts, str):
             hosts = [hosts]
 
         for host in hosts:
-            matches = re.search('((.+)\@)?([^:]+)(:(\d+))?', host)
+            matches = re.search("((.+)\@)?([^:]+)(:(\d+))?", host)
             alias = matches.group(2)
             hostname = matches.group(3)
             port = matches.group(5)
 
             if alias is None:
                 alias = hostname
 
@@ -157,29 +154,43 @@
             for stat in stats:
                 if stat in self.GAUGES:
                     self.publish_gauge(alias + "." + stat, stats[stat])
                 else:
                     self.publish_counter(alias + "." + stat, stats[stat])
 
             # Pool stats
-            for pool, pool_stats in pools.iteritems():
-                for stat, stat_value in pool_stats.iteritems():
+            for pool, pool_stats in iter(pools.items()):
+                for stat, stat_value in iter(pool_stats.items()):
                     # Test if this is a pool server
                     if isinstance(stat_value, dict):
-                        for server_stat, server_value in stat_value.iteritems():
+                        for server_stat, server_value in iter(stat_value.items()):
                             if server_stat in self.GAUGES:
                                 self.publish_gauge(
-                                    alias + ".pools." + pool + ".servers." +
-                                    stat + "." + server_stat, server_value)
+                                    alias
+                                    + ".pools."
+                                    + pool
+                                    + ".servers."
+                                    + stat
+                                    + "."
+                                    + server_stat,
+                                    server_value,
+                                )
                             else:
                                 self.publish_counter(
-                                    alias + ".pools." + pool + ".servers." +
-                                    stat + "." + server_stat, server_value)
+                                    alias
+                                    + ".pools."
+                                    + pool
+                                    + ".servers."
+                                    + stat
+                                    + "."
+                                    + server_stat,
+                                    server_value,
+                                )
                     else:
                         if stat in self.GAUGES:
                             self.publish_gauge(
-                                alias + ".pools." + pool + "." + stat,
-                                stat_value)
+                                alias + ".pools." + pool + "." + stat, stat_value
+                            )
                         else:
                             self.publish_counter(
-                                alias + ".pools." + pool + "." + stat,
-                                stat_value)
+                                alias + ".pools." + pool + "." + stat, stat_value
+                            )
```

### Comparing `diamond-next-4.0.515/src/collectors/phpfpm/phpfpm.py` & `diamond-next-5.0.0/src/collectors/phpfpm/phpfpm.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,90 +1,109 @@
 # coding=utf-8
 
 """
-Collects data from php-fpm if the pm.status_path is enabled
+Collects data from php-fpm if the
+[pm.status_path](https://secure.php.net/manual/en/install.fpm.configuration.php#pm.status-path)
+is enabled in php-fpm configuration
 
 
 #### Usage
 
 A sample php-fpm config for this collector to work is
 
 ```
 pm.status_path = /fpm-status
 ```
 
+If the URL of the fpm-status page is http://127.0.0.1:8080/fpm-status, you
+need to set:
+
+```
+enabled = True
+host = 127.0.0.1
+port = 8080
+uri = fpm-status
+```
+
 #### Dependencies
 
- * urllib2
+ * urllib
  * json (or simeplejson)
 
 """
 
+import urllib.request
+
+import diamond.collector
+
 try:
     import json
 except ImportError:
     import simplejson as json
 
-import urllib2
-import diamond.collector
-
 
 class PhpFpmCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
         config_help = super(PhpFpmCollector, self).get_default_config_help()
-        config_help.update({
-        })
+        config_help.update(
+            {
+                "uri": "Path part of the URL, with or without the leading /",
+                "host": "Host part of the URL",
+                "port": "Port part of the URL",
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(PhpFpmCollector, self).get_default_config()
-        config.update({
-            'host': 'localhost',
-            'port': 80,
-            'uri': 'fpm-status',
-            'byte_unit': ['byte'],
-            'path': 'phpfpm',
-        })
+        config.update(
+            {
+                "host": "localhost",
+                "port": 80,
+                "uri": "fpm-status",
+                "byte_unit": ["byte"],
+                "path": "phpfpm",
+            }
+        )
         return config
 
     def collect(self):
-        #
         # if there is a / in front remove it
-        if self.config['uri'][0] == '/':
-            self.config['uri'] = self.config['uri'][1:]
+        if self.config["uri"][0] == "/":
+            self.config["uri"] = self.config["uri"][1:]
 
         try:
-            response = urllib2.urlopen("http://%s:%s/%s?json" % (
-                self.config['host'], int(self.config['port']),
-                self.config['uri']))
-        except Exception, e:
-            self.log.error('Couldnt connect to php-fpm status page: %s', e)
+            response = urllib.request.urlopen(
+                "http://%s:%s/%s?json"
+                % (self.config["host"], int(self.config["port"]), self.config["uri"])
+            )
+        except Exception as e:
+            self.log.error("Could not connect to php-fpm status page: %s", e)
             return {}
 
         try:
             j = json.loads(response.read())
-        except Exception, e:
-            self.log.error('Couldnt parse json: %s', e)
+        except Exception as e:
+            self.log.error("Could not parse json: %s", e)
             return {}
 
         valid_metrics = [
-            'accepted_conn',
-            'listen_queue',
-            'max_listen_queue',
-            'listen_queue_len',
-            'idle_processes',
-            'active_processes',
-            'total_processes',
-            'max_active_processes',
-            'max_children_reached',
-            'slow_requests'
+            "accepted_conn",
+            "listen_queue",
+            "max_listen_queue",
+            "listen_queue_len",
+            "idle_processes",
+            "active_processes",
+            "total_processes",
+            "max_active_processes",
+            "max_children_reached",
+            "slow_requests",
         ]
         for k, v in j.items():
             #
             # php-fpm has spaces in the keys so lets replace all spaces with _
             k = k.replace(" ", "_")
 
             if k in valid_metrics:
```

### Comparing `diamond-next-4.0.515/src/collectors/memory_docker/memory_docker.py` & `diamond-next-5.0.0/src/collectors/memory_docker/memory_docker.py`

 * *Files 22% similar despite different names*

```diff
@@ -6,39 +6,46 @@
 #### Dependencies
 
  * docker
 
 """
 
 import os
+
 import sys
 
 try:
     import docker
 except ImportError:
     docker = None
 
-sys.path.append(os.path.join(os.path.dirname(os.path.dirname(__file__)),
-                             'memory_cgroup'))
+sys.path.append(
+    os.path.join(os.path.dirname(os.path.dirname(__file__)), "memory_cgroup")
+)
+
 from memory_cgroup import MemoryCgroupCollector
 
 
 class MemoryDockerCollector(MemoryCgroupCollector):
-
     def collect(self):
         if docker is None:
-            self.log.error('Unable to import docker')
+            self.log.error("Unable to import docker")
+
             return
 
         self.containers = dict(
-            (c['Id'], c['Names'][0][1:])
+            (c["Id"], c["Names"][0][1:])
             for c in docker.Client().containers(all=True)
-            if c['Names'] is not None)
+            if c["Names"] is not None
+        )
+
         return super(MemoryDockerCollector, self).collect()
 
     def publish(self, metric_name, value, metric_type):
         for container_id, container_name in self.containers.items():
             metric_name = metric_name.replace(
-                'docker.' + container_id + '.',
-                'docker.' + container_name + '.')
+                "docker." + container_id + ".", "docker." + container_name + "."
+            )
+
         return super(MemoryDockerCollector, self).publish(
-            metric_name, value, metric_type)
+            metric_name, value, metric_type
+        )
```

### Comparing `diamond-next-4.0.515/src/collectors/processresources/processresources.py` & `diamond-next-5.0.0/src/collectors/processresources/processresources.py`

 * *Files 12% similar despite different names*

```diff
@@ -37,14 +37,15 @@
 import time
 
 import diamond.collector
 import diamond.convertor
 
 try:
     import psutil
+
     psutil
 except ImportError:
     psutil = None
 
 
 def match_process(pid, name, cmdline, exe, cfg):
     """
@@ -55,51 +56,55 @@
     :param name: process name
     :param cmdline: process cmdline
     :param cfg: the dictionary from processes that describes with the
         process group we're testing for
     :return: True if it matches
     :rtype: bool
     """
-    if cfg['selfmon'] and pid == os.getpid():
+    if cfg["selfmon"] and pid == os.getpid():
         return True
-    for exe_re in cfg['exe']:
+
+    for exe_re in cfg["exe"]:
         if exe_re.search(exe):
             return True
-    for name_re in cfg['name']:
+    for name_re in cfg["name"]:
         if name_re.search(name):
             return True
-    for cmdline_re in cfg['cmdline']:
-        if cmdline_re.search(' '.join(cmdline)):
+    for cmdline_re in cfg["cmdline"]:
+        if cmdline_re.search(" ".join(cmdline)):
             return True
+
     return False
 
 
 def process_info(process, info_keys):
     results = {}
     process_info = process.as_dict(info_keys)
     metrics = ((key, process_info.get(key, None)) for key in info_keys)
+
     for key, value in metrics:
         if type(value) in [float, int]:
             results.update({key: value})
-        elif hasattr(value, '_asdict'):
-            for subkey, subvalue in value._asdict().iteritems():
+        elif hasattr(value, "_asdict"):
+            for subkey, subvalue in iter(value._asdict().items()):
                 results.update({"%s.%s" % (key, subkey): subvalue})
+
     return results
 
 
 def get_value(process, name):
     result = getattr(process, name)
+
     try:
         return result()
     except TypeError:
         return result
 
 
 class ProcessResourcesCollector(diamond.collector.Collector):
-
     def process_config(self):
         super(ProcessResourcesCollector, self).process_config()
         """
         prepare self.processes, which is a descriptor dictionary in
         pg_name: {
             exe: [regex],
             name: [regex],
@@ -107,108 +112,130 @@
             selfmon: [boolean],
             procs: [psutil.Process],
             count_workers: [boolean]
         }
         """
         self.processes = {}
         self.processes_info = {}
-        for pg_name, cfg in self.config['process'].items():
+
+        for pg_name, cfg in self.config["process"].items():
             pg_cfg = {}
-            for key in ('exe', 'name', 'cmdline'):
+
+            for key in ("exe", "name", "cmdline"):
                 pg_cfg[key] = cfg.get(key, [])
+
                 if not isinstance(pg_cfg[key], list):
                     pg_cfg[key] = [pg_cfg[key]]
+
                 pg_cfg[key] = [re.compile(e) for e in pg_cfg[key]]
-            pg_cfg['selfmon'] = cfg.get('selfmon', '').lower() == 'true'
-            pg_cfg['count_workers'] = cfg.get(
-                'count_workers', '').lower() == 'true'
+
+            pg_cfg["selfmon"] = cfg.get("selfmon", "").lower() == "true"
+            pg_cfg["count_workers"] = cfg.get("count_workers", "").lower() == "true"
             self.processes[pg_name] = pg_cfg
             self.processes_info[pg_name] = {}
 
     def get_default_config_help(self):
-        config_help = super(ProcessResourcesCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'info_keys': 'List of process metrics to collect. ' +
-                         'Valid list of metrics can be found ' +
-                         '[here](https://pythonhosted.org/psutil/)',
-            'unit': 'The unit in which memory data is collected.',
-            'process': ("A subcategory of settings inside of which each "
-                        "collected process has it's configuration"),
-        })
+        config_help = super(ProcessResourcesCollector, self).get_default_config_help()
+        config_help.update(
+            {
+                "info_keys": "List of process metrics to collect. "
+                + "Valid list of metrics can be found "
+                + "[here](https://pythonhosted.org/psutil/)",
+                "unit": "The unit in which memory data is collected.",
+                "process": (
+                    "A subcategory of settings inside of which each "
+                    "collected process has it's configuration"
+                ),
+            }
+        )
         return config_help
 
     def get_default_config(self):
         """
         Default settings are:
             info_keys: ['num_ctx_switches', 'cpu_percent', 'cpu_times',
                         'io_counters', 'num_threads', 'num_fds',
                         'memory_percent', 'memory_info_ex', ]
             path: 'process'
             unit: 'B'
         """
         config = super(ProcessResourcesCollector, self).get_default_config()
-        config.update({
-            'info_keys': ['num_ctx_switches', 'cpu_percent', 'cpu_times',
-                          'io_counters', 'num_threads', 'num_fds',
-                          'memory_percent', 'memory_info_ex', ],
-            'path': 'process',
-            'unit': 'B',
-            'process': {},
-        })
+        config.update(
+            {
+                "info_keys": [
+                    "num_ctx_switches",
+                    "cpu_percent",
+                    "cpu_times",
+                    "io_counters",
+                    "num_threads",
+                    "num_fds",
+                    "memory_percent",
+                    "memory_info_ex",
+                ],
+                "path": "process",
+                "unit": "B",
+                "process": {},
+            }
+        )
         return config
 
     def save_process_info(self, pg_name, process_info):
-        for key, value in process_info.iteritems():
+        for key, value in iter(process_info.items()):
             if key in self.processes_info[pg_name]:
                 self.processes_info[pg_name][key] += value
             else:
                 self.processes_info[pg_name][key] = value
 
     def collect_process_info(self, process):
         try:
-            pid = get_value(process, 'pid')
-            name = get_value(process, 'name')
-            cmdline = get_value(process, 'cmdline')
+            pid = get_value(process, "pid")
+            name = get_value(process, "name")
+            cmdline = get_value(process, "cmdline")
+
             try:
-                exe = get_value(process, 'exe')
+                exe = get_value(process, "exe")
             except psutil.AccessDenied:
                 exe = ""
+
             for pg_name, cfg in self.processes.items():
                 if match_process(pid, name, cmdline, exe, cfg):
-                    pi = process_info(process, self.config['info_keys'])
-                    if cfg['count_workers']:
-                        pi.update({'workers_count': 1})
-                    uptime = time.time() - get_value(process, 'create_time')
-                    pi.update({'uptime': uptime})
+                    pi = process_info(process, self.config["info_keys"])
+
+                    if cfg["count_workers"]:
+                        pi.update({"workers_count": 1})
+
+                    uptime = time.time() - get_value(process, "create_time")
+                    pi.update({"uptime": uptime})
                     self.save_process_info(pg_name, pi)
-        except psutil.NoSuchProcess, e:
+        except psutil.NoSuchProcess as e:
             self.log.info("Process exited while trying to get info: %s", e)
 
     def collect(self):
         """
         Collects resources usage of each process defined under the
         `process` subsection of the config file
         """
         if not psutil:
-            self.log.error('Unable to import psutil')
-            self.log.error('No process resource metrics retrieved')
+            self.log.error("Unable to import psutil")
+            self.log.error("No process resource metrics retrieved")
             return None
 
         for process in psutil.process_iter():
             self.collect_process_info(process)
 
         # publish results
-        for pg_name, counters in self.processes_info.iteritems():
+        for pg_name, counters in iter(self.processes_info.items()):
             if counters:
                 metrics = (
                     ("%s.%s" % (pg_name, key), value)
-                    for key, value in counters.iteritems())
+                    for key, value in iter(counters.items())
+                )
             else:
-                if self.processes[pg_name]['count_workers']:
-                    metrics = (('%s.workers_count' % pg_name, 0), )
+                if self.processes[pg_name]["count_workers"]:
+                    metrics = (("%s.workers_count" % pg_name, 0),)
                 else:
                     metrics = ()
 
             [self.publish(*metric) for metric in metrics]
+
             # reinitialize process info
             self.processes_info[pg_name] = {}
```

### Comparing `diamond-next-4.0.515/src/collectors/openstackswiftrecon/openstackswiftrecon.py` & `diamond-next-5.0.0/src/collectors/openstackswiftrecon/openstackswiftrecon.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,74 +7,88 @@
 #### Dependencies
 
  * Running Swift services must have a recon enabled
 
 """
 
 import os
+
+import diamond.collector
+
 try:
     import json
 except ImportError:
     import simplejson as json
 
-import diamond.collector
-
 
 class OpenstackSwiftReconCollector(diamond.collector.Collector):
-
     def get_default_config_help(self):
-        config_help = super(OpenstackSwiftReconCollector,
-                            self).get_default_config_help()
-        config_help.update({
-            'recon_account_cache': 'path to swift recon account cache '
-            '(default /var/cache/swift/account.recon)',
-            'recon_container_cache': 'path to swift recon container cache '
-            '(default /var/cache/swift/container.recon)',
-            'recon_object_cache': 'path to swift recon object cache '
-            '(default /var/cache/swift/object.recon)'
-        })
+        config_help = super(
+            OpenstackSwiftReconCollector, self
+        ).get_default_config_help()
+        config_help.update(
+            {
+                "recon_account_cache": "path to swift recon account cache "
+                "(default /var/cache/swift/account.recon)",
+                "recon_container_cache": "path to swift recon container cache "
+                "(default /var/cache/swift/container.recon)",
+                "recon_object_cache": "path to swift recon object cache "
+                "(default /var/cache/swift/object.recon)",
+            }
+        )
+
         return config_help
 
     def get_default_config(self):
         """
         Returns the default collector settings
         """
         config = super(OpenstackSwiftReconCollector, self).get_default_config()
-        config.update({
-            'path': 'swiftrecon',
-            'recon_account_cache': '/var/cache/swift/account.recon',
-            'recon_container_cache': '/var/cache/swift/container.recon',
-            'recon_object_cache': '/var/cache/swift/object.recon',
-            'interval': 300,
-        })
+        config.update(
+            {
+                "path": "swiftrecon",
+                "recon_account_cache": "/var/cache/swift/account.recon",
+                "recon_container_cache": "/var/cache/swift/container.recon",
+                "recon_object_cache": "/var/cache/swift/object.recon",
+                "interval": 300,
+            }
+        )
+
         return config
 
     def _process_cache(self, d, path=()):
         """Recusively walk a nested recon cache dict to obtain path/values"""
-        for k, v in d.iteritems():
+        for k, v in iter(d.items()):
             if not isinstance(v, dict):
                 self.metrics.append((path + (k,), v))
             else:
                 self._process_cache(v, path + (k,))
 
     def collect(self):
         self.metrics = []
-        recon_cache = {'account': self.config['recon_account_cache'],
-                       'container': self.config['recon_container_cache'],
-                       'object': self.config['recon_object_cache']}
+        recon_cache = {
+            "account": self.config["recon_account_cache"],
+            "container": self.config["recon_container_cache"],
+            "object": self.config["recon_object_cache"],
+        }
+
         for recon_type in recon_cache:
             if not os.access(recon_cache[recon_type], os.R_OK):
                 continue
+
             try:
                 f = open(recon_cache[recon_type])
+
                 try:
                     rmetrics = json.loads(f.readlines()[0].strip())
                     self.metrics = []
                     self._process_cache(rmetrics)
+
                     for k, v in self.metrics:
-                        metric_name = '%s.%s' % (recon_type, ".".join(k))
+                        metric_name = "%s.%s" % (recon_type, ".".join(k))
+
                         if isinstance(v, (int, float)):
                             self.publish(metric_name, v)
                 except (ValueError, IndexError):
                     continue
             finally:
                 f.close()
```

### Comparing `diamond-next-4.0.515/conf/vagrant/diamond.conf` & `diamond-next-5.0.0/conf/diamond.conf.example`

 * *Files 4% similar despite different names*

```diff
@@ -3,65 +3,67 @@
 ################################################################################
 
 ################################################################################
 ### Options for the server
 [server]
 
 # Handlers for published metrics.
-#handlers = diamond.handler.graphite.GraphiteHandler, diamond.handler.archive.ArchiveHandler
-handlers = diamond.handler.archive.ArchiveHandler
+handlers = diamond.handler.graphite.GraphiteHandler, diamond.handler.archive.ArchiveHandler
 
 # User diamond will run as
 # Leave empty to use the current user
 user =
 
 # Group diamond will run as
 # Leave empty to use the current group
 group =
 
-# Pid file
+# Pid file, on systemd if not using default PID file
+# make sure you also change systemd's diamond unit.
 pid_file = /var/run/diamond.pid
 
 # Directory to load collector modules from
-collectors_path = /vagrant/src/collectors/
+collectors_path = /usr/share/diamond/collectors/
 
 # Directory to load collector configs from
 collectors_config_path = /etc/diamond/collectors/
 
 # Number of seconds between each collector load
 # collectors_load_delay = 1.0
 
 # Directory to load handler configs from
 handlers_config_path = /etc/diamond/handlers/
 
 # Directory to load handler modules from
-handlers_path = /vagrant/src/diamond/handler/
+handlers_path = /usr/share/diamond/handlers/
 
 # Maximum number of metrics waiting to be processed by handlers.
 # When metric queue is full, new metrics are dropped.
 metric_queue_size = 16384
 
+# Abort the diamond process if the handler process exits
+abort_on_handlers_process_exit = False
 
 ################################################################################
 ### Options for handlers
 [handlers]
 
 # daemon logging handler(s)
 keys = rotated_file
 
-### Defaults options for all Handlers
-[[default]]
+### Default options for all Handlers
+[[handlersDefault]]
 
 [[ArchiveHandler]]
 
 # File to write archive log files
 log_file = /var/log/diamond/archive.log
 
 # Number of days to keep archive log files
-days = 1
+days = 7
 
 [[GraphiteHandler]]
 ### Options for GraphiteHandler
 
 # Graphite server host
 host = 127.0.0.1
 
@@ -137,16 +139,16 @@
 batch = 100
 
 
 ################################################################################
 ### Options for collectors
 [collectors]
 
-[[default]]
-### Defaults options for all Collectors
+[[collectorsDefault]]
+### Default options for all Collectors
 
 # Uncomment and set to hardcode a hostname for the collector path
 # Keep in mind, periods are seperators in graphite
 # hostname = my_custom_hostname
 
 # If you prefer to just use a different way of calculating the hostname
 # Uncomment and set this to one of these values:
@@ -179,15 +181,15 @@
 # If the host supports virtual machines, collectors may report per
 # VM metrics. Following OpenStack nomenclature, the prefix for
 # reporting per VM metrics is "instances", and metric foo for VM
 # bar will be reported as: instances.bar.foo...
 # instance_prefix = instances
 
 # Default Poll Interval (seconds)
-interval = 15
+# interval = 300
 
 ################################################################################
 # Default enabled collectors
 ################################################################################
 
 [[CPUCollector]]
 enabled = True
@@ -223,29 +225,29 @@
 [formatters]
 
 keys = default
 
 [logger_root]
 
 # to increase verbosity, set DEBUG
-level = DEBUG
+level = INFO
 handlers = rotated_file
 propagate = 1
 
 [handler_rotated_file]
 
 class = handlers.TimedRotatingFileHandler
 level = DEBUG
 formatter = default
 # rotate at midnight, each day and keep 7 days
 args = ('/var/log/diamond/diamond.log', 'midnight', 1, 7)
 
 [formatter_default]
 
-format = [%(asctime)s] [%(threadName)s] %(message)s
+format = [%(asctime)s] [%(levelname)s] [%(threadName)s:%(processName)s] %(message)s
 datefmt =
 
 ################################################################################
 ### Options for config merging
 # [configs]
 # path = "/etc/diamond/configs/"
 # extension = ".conf"
```

### Comparing `diamond-next-4.0.515/conf/diamond.conf.example.windows` & `diamond-next-5.0.0/conf/diamond.conf.example.windows`

 * *Files 2% similar despite different names*

```diff
@@ -38,16 +38,17 @@
 ################################################################################
 ### Options for handlers
 [handlers]
 
 # daemon logging handler(s)
 keys = rotated_file
 
-### Defaults options for all Handlers
-[[default]]
+### Default options for all Handlers
+[[handlersDefault]]
+
 enabled = False
 
 [[ArchiveHandler]]
 
 # File to write archive log files
 log_file = C:\\Program Files\\diamond\\archive.log
 
@@ -123,16 +124,16 @@
 batch = 100
 
 
 ################################################################################
 ### Options for collectors
 [collectors]
 
-[[default]]
-### Defaults options for all Collectors
+[[collectorsDefault]]
+### Default options for all Collectors
 
 # Uncomment and set to hardcode a hostname for the collector path
 # Keep in mind, periods are seperators in graphite
 # hostname = my_custom_hostname
 
 # If you prefer to just use a different way of calculating the hostname
 # Uncomment and set this to one of these values:
```

### Comparing `diamond-next-4.0.515/conf/diamond.conf.example` & `diamond-next-5.0.0/conf/vagrant/diamond.conf`

 * *Files 4% similar despite different names*

```diff
@@ -3,64 +3,65 @@
 ################################################################################
 
 ################################################################################
 ### Options for the server
 [server]
 
 # Handlers for published metrics.
-handlers = diamond.handler.graphite.GraphiteHandler, diamond.handler.archive.ArchiveHandler
+#handlers = diamond.handler.graphite.GraphiteHandler, diamond.handler.archive.ArchiveHandler
+handlers = diamond.handler.archive.ArchiveHandler
 
 # User diamond will run as
 # Leave empty to use the current user
 user =
 
 # Group diamond will run as
 # Leave empty to use the current group
 group =
 
 # Pid file
 pid_file = /var/run/diamond.pid
 
 # Directory to load collector modules from
-collectors_path = /usr/share/diamond/collectors/
+collectors_path = /vagrant/src/collectors/
 
 # Directory to load collector configs from
 collectors_config_path = /etc/diamond/collectors/
 
 # Number of seconds between each collector load
 # collectors_load_delay = 1.0
 
 # Directory to load handler configs from
 handlers_config_path = /etc/diamond/handlers/
 
 # Directory to load handler modules from
-handlers_path = /usr/share/diamond/handlers/
+handlers_path = /vagrant/src/diamond/handler/
 
 # Maximum number of metrics waiting to be processed by handlers.
 # When metric queue is full, new metrics are dropped.
 metric_queue_size = 16384
 
 
 ################################################################################
 ### Options for handlers
 [handlers]
 
 # daemon logging handler(s)
 keys = rotated_file
 
-### Defaults options for all Handlers
-[[default]]
+### Default options for all Handlers
+[[handlersDefault]]
 
 [[ArchiveHandler]]
 
 # File to write archive log files
 log_file = /var/log/diamond/archive.log
 
 # Number of days to keep archive log files
-days = 7
+days = 1
 
 [[GraphiteHandler]]
 ### Options for GraphiteHandler
 
 # Graphite server host
 host = 127.0.0.1
 
@@ -136,16 +137,16 @@
 batch = 100
 
 
 ################################################################################
 ### Options for collectors
 [collectors]
 
-[[default]]
-### Defaults options for all Collectors
+[[collectorsDefault]]
+### Default options for all Collectors
 
 # Uncomment and set to hardcode a hostname for the collector path
 # Keep in mind, periods are seperators in graphite
 # hostname = my_custom_hostname
 
 # If you prefer to just use a different way of calculating the hostname
 # Uncomment and set this to one of these values:
@@ -178,15 +179,15 @@
 # If the host supports virtual machines, collectors may report per
 # VM metrics. Following OpenStack nomenclature, the prefix for
 # reporting per VM metrics is "instances", and metric foo for VM
 # bar will be reported as: instances.bar.foo...
 # instance_prefix = instances
 
 # Default Poll Interval (seconds)
-# interval = 300
+interval = 15
 
 ################################################################################
 # Default enabled collectors
 ################################################################################
 
 [[CPUCollector]]
 enabled = True
@@ -222,15 +223,15 @@
 [formatters]
 
 keys = default
 
 [logger_root]
 
 # to increase verbosity, set DEBUG
-level = INFO
+level = DEBUG
 handlers = rotated_file
 propagate = 1
 
 [handler_rotated_file]
 
 class = handlers.TimedRotatingFileHandler
 level = DEBUG
```

### Comparing `diamond-next-4.0.515/debian/diamond.upstart` & `diamond-next-5.0.0/debian/diamond.upstart`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/debian/prerm` & `diamond-next-5.0.0/debian/prerm`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/debian/postinst` & `diamond-next-5.0.0/debian/postinst`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/debian/postrm` & `diamond-next-5.0.0/debian/postrm`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/debian/diamond.init` & `diamond-next-5.0.0/debian/diamond.init`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -18,20 +18,20 @@
 NAME=diamond
 DAEMON=/usr/bin/diamond
 DAEMON_ARGS="-p /var/run/diamond.pid"
 PIDFILE=/var/run/diamond.pid
 SCRIPTNAME=/etc/init.d/diamond
 CONF=/etc/diamond/diamond.conf
 
-# Exit if the package is not installed
-[ -x $DAEMON ] || exit 0
-
 # Read configuration variable file if it is present
 [ -r /etc/default/$NAME ] && . /etc/default/$NAME
 
+# Exit if the package is not installed
+[ -x $DAEMON ] || exit 0
+
 # Checking for a config file in place
 if [  ! -e $CONF ]; then
    echo "/etc/diamond/diamond.conf not found. Please see /etc/diamond/diamond.conf.example" 
    exit 2
 fi
 
 # Load the VERBOSE setting and other rcS variables
```

### Comparing `diamond-next-4.0.515/debian/preinst` & `diamond-next-5.0.0/debian/preinst`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/debian/changelog` & `diamond-next-5.0.0/debian/changelog`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/debian/copyright` & `diamond-next-5.0.0/debian/copyright`

 * *Files identical despite different names*

### Comparing `diamond-next-4.0.515/debian/control` & `diamond-next-5.0.0/debian/control`

 * *Files 23% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Source: diamond
 Section: misc
 Priority: extra
 Maintainer: Rob Smith <kormoc@gmail.com>
 Homepage: https://github.com/BrightcoveOS/Diamond
 Vcs-Git: git://github.com/BrightcoveOS/Diamond.git
 Vcs-Browser: https://github.com/BrightcoveOS/Diamond
-Build-Depends: debhelper (>= 7), python (>= 2.4), python-support, python-mock, python-configobj, cdbs
+Build-Depends: debhelper (>= 7), python3 (>= 3.8), python-mock, python3-configobj, cdbs
 Standards-Version: 3.9.1
 
 Package: diamond
 Architecture: all
-Depends: ${misc:Depends}, ${python:Depends}, python (>= 2.4), python-configobj
+Depends: ${misc:Depends}, ${python:Depends}, python3 (>= 3.8), python3-configobj
 Description: System statistics collector for Graphite.
  Diamond is a daemon and toolset for gathering system statistics
  and publishing them to Graphite.
```

### Comparing `diamond-next-4.0.515/LICENSE` & `diamond-next-5.0.0/LICENSE`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 Copyright (C) 2010-2012 by Brightcove Inc.      http://www.brightcove.com/
 Copyright (C) 2011-2012 by Ivan Pouzyrevsky
 Copyright (C) 2011-2012 by Rob Smith            http://www.kormoc.com
 Copyright (C) 2012 Wijnand Modderman-Lenstra    https://maze.io/
 Copyright (C) 2012 Dennis Kaarsemaker           <dennis@kaarsemaker.net>
+Copyright (C) 2012-2019 The Diamond Team       https://github.com/python-diamond/Diamond
+Copyright (C) 2019 The Diamond (Next) Team       https://github.com/diamond-next/diamond-next
 
 License: MIT
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
 to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
```

